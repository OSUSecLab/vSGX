From 4f463a7eb533c5174f02007a94f287265b231eb0 Mon Sep 17 00:00:00 2001
From: NSKernel <zhao.3289@osu.edu>
Date: Thu, 11 Mar 2021 02:03:54 -0500
Subject: [PATCH 1/9] Adapted vSGX to SEV-ES

---
 arch/x86/kernel/process_64.c        |   31 +-
 arch/x86/kernel/traps.c             |  289 ++++++
 arch/x86/mm/fault.c                 |  126 ++-
 crypto/asymmetric_keys/public_key.c |   73 ++
 include/crypto/public_key.h         |    4 +
 include/emusgx/emusgx.h             |  115 +++
 include/emusgx/emusgx_arch.h        |  242 +++++
 include/emusgx/emusgx_cpuid.h       |   12 +
 include/emusgx/emusgx_debug.h       |    8 +
 include/emusgx/emusgx_fault.h       |    9 +
 include/emusgx/emusgx_internal.h    |  173 ++++
 include/emusgx/emusgx_mm.h          |   12 +
 include/emusgx/emusgx_sender.h      |  279 +++++
 include/linux/sched.h               |   17 +
 init/init_task.c                    |    3 +
 kernel/Makefile                     |    2 +
 kernel/emusgx/Kconfig               |    3 +
 kernel/emusgx/Makefile              |    3 +
 kernel/emusgx/aex.c                 |  159 +++
 kernel/emusgx/cpusvn.c              |   19 +
 kernel/emusgx/cr.c                  |  228 +++++
 kernel/emusgx/crypto.c              |  575 +++++++++++
 kernel/emusgx/dispatcher.c          |  866 ++++++++++++++++
 kernel/emusgx/emusgx_fetch.c        |   94 ++
 kernel/emusgx/encls_cross_vm.c      | 1454 +++++++++++++++++++++++++++
 kernel/emusgx/enclu.c               | 1003 ++++++++++++++++++
 kernel/emusgx/entrance.c            |  491 +++++++++
 kernel/emusgx/fault.c               |   41 +
 kernel/emusgx/irq.c                 |  130 +++
 kernel/emusgx/local_dispatcher.c    |  118 +++
 kernel/emusgx/management.c          |  337 +++++++
 kernel/emusgx/sender.c              |  306 ++++++
 kernel/emusgx/switchless_sync.c     |  395 ++++++++
 kernel/fork.c                       |    9 +
 34 files changed, 7623 insertions(+), 3 deletions(-)
 create mode 100644 include/emusgx/emusgx.h
 create mode 100644 include/emusgx/emusgx_arch.h
 create mode 100644 include/emusgx/emusgx_cpuid.h
 create mode 100644 include/emusgx/emusgx_debug.h
 create mode 100644 include/emusgx/emusgx_fault.h
 create mode 100644 include/emusgx/emusgx_internal.h
 create mode 100644 include/emusgx/emusgx_mm.h
 create mode 100644 include/emusgx/emusgx_sender.h
 create mode 100644 kernel/emusgx/Kconfig
 create mode 100644 kernel/emusgx/Makefile
 create mode 100644 kernel/emusgx/aex.c
 create mode 100644 kernel/emusgx/cpusvn.c
 create mode 100644 kernel/emusgx/cr.c
 create mode 100644 kernel/emusgx/crypto.c
 create mode 100644 kernel/emusgx/dispatcher.c
 create mode 100644 kernel/emusgx/emusgx_fetch.c
 create mode 100644 kernel/emusgx/encls_cross_vm.c
 create mode 100644 kernel/emusgx/enclu.c
 create mode 100644 kernel/emusgx/entrance.c
 create mode 100644 kernel/emusgx/fault.c
 create mode 100644 kernel/emusgx/irq.c
 create mode 100644 kernel/emusgx/local_dispatcher.c
 create mode 100644 kernel/emusgx/management.c
 create mode 100644 kernel/emusgx/sender.c
 create mode 100644 kernel/emusgx/switchless_sync.c

diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index df342bede..fb4bac1b3 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -520,6 +520,22 @@ void compat_start_thread(struct pt_regs *regs, u32 new_ip, u32 new_sp)
 }
 #endif
 
+static __always_inline void vsgx_force_save_fsgs(struct task_struct *prev_p) {
+	// For use with enclave threads
+	rdmsrl(MSR_FS_BASE, prev_p->thread.fsbase);
+	rdmsrl(MSR_KERNEL_GS_BASE, prev_p->thread.gsbase);
+}
+
+static __always_inline void vsgx_force_load_fsgs(struct task_struct *next_p) {
+	// For use with enclave threads
+	// Better don't touch the loadsegment thing
+	// On Intel machines they may clear the seg base
+	//loadsegment(fs, 0x0B);
+	wrmsrl(MSR_FS_BASE, next_p->thread.fsbase);
+	//load_gs_index(0x0B);
+	wrmsrl(MSR_KERNEL_GS_BASE, next_p->thread.gsbase);
+}
+
 /*
  *	switch_to(x,y) should switch tasks from x to y.
  *
@@ -550,7 +566,13 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	 *
 	 * (e.g. xen_load_tls())
 	 */
-	save_fsgs(prev_p);
+	if (prev_p->is_enclave_thread) {
+		// We will force save the FS/GS for an enclave thread
+		vsgx_force_save_fsgs(prev_p);
+	}
+	else {
+		save_fsgs(prev_p);
+	}
 
 	/*
 	 * Load TLS before restoring any segments so that segment loads
@@ -587,7 +609,12 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	if (unlikely(next->ds | prev->ds))
 		loadsegment(ds, next->ds);
 
-	x86_fsgsbase_load(prev, next);
+	if (next_p->is_enclave_thread) {
+		vsgx_force_load_fsgs(next_p);
+	}
+	else {
+		x86_fsgsbase_load(prev, next);
+	}
 
 	/*
 	 * Switch the PDA and FPU contexts.
diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 170c94ec0..63c6c4c37 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -42,6 +42,8 @@
 
 #include <asm/stacktrace.h>
 #include <asm/processor.h>
+#include <asm/msr-index.h>
+#include <asm/segment.h>
 #include <asm/debugreg.h>
 #include <asm/realmode.h>
 #include <asm/text-patching.h>
@@ -243,10 +245,297 @@ static noinstr bool handle_bug(struct pt_regs *regs)
 	return handled;
 }
 
+#include <linux/kthread.h>
+
+#include <emusgx/emusgx.h>
+#include <emusgx/emusgx_internal.h>
+#include <emusgx/emusgx_sender.h>
+#include <emusgx/emusgx_mm.h>
+#include <emusgx/emusgx_cpuid.h>
+#include <emusgx/emusgx_debug.h>
+
+struct task_struct *emusgx_dispatcher_task = NULL;
+
+static void vsgx_enclave_debug_print(char *msg) {
+	pr_info("vSGX Enclave: %s\n", msg);
+}
+
 DEFINE_IDTENTRY_RAW(exc_invalid_op)
 {
 	irqentry_state_t state;
 
+	// Handle the SGX instructions
+	uint8_t *rip = (uint8_t *)(regs->ip);
+	uint8_t opcode_prefix;
+	uint8_t opcode_primary;
+	uint8_t opcode_secondary;
+	struct emusgx_regs reg_status;
+	struct emusgx_full_regs *full_regs;
+	struct emusgx_handle_buffer *handle_buffer;
+	int err;
+	char debug_print_buffer[100];
+
+
+	get_user(opcode_prefix, rip);
+	get_user(opcode_primary, (uint8_t *)(rip + 1));
+	get_user(opcode_secondary, (uint8_t *)(rip + 2));
+	if (opcode_prefix == 0x0F &&
+		opcode_primary == 0x01) {
+		if (opcode_secondary == 0xD7) {
+			reg_status.rax = (uint32_t)regs->ax;
+			reg_status.rbx = regs->bx;
+			reg_status.rcx = regs->cx;
+			reg_status.rdx = regs->dx;
+			reg_status.eflags = regs->flags;
+			// pr_info("SGX: trapped. not bad instruction, continued\n");
+			emusgx_handle_enclu(&reg_status, regs);
+
+			// If EEXIT, restore regs to previous state
+			if ((uint32_t)(regs->ax) == EMUSGX_EEXIT) {
+				// Not enclave anymore
+				current->is_enclave_thread = 0;
+				// Restore reg status
+				memcpy(regs, current->backup_regs_before_eenter, sizeof(struct pt_regs));
+				// So when EEXIT, it's just like nothing happened
+				kfree(current->backup_regs_before_eenter);
+
+				// Restore FS/GS
+				//loadsegment(fs, current->backup_fs);
+				wrmsrl(MSR_FS_BASE, current->backup_fsbase);
+				//load_gs_index(current->backup_gs);
+				wrmsrl(MSR_KERNEL_GS_BASE, current->backup_gsbase);
+
+				// Restore thread_struct's FS/GS base
+				// This must be done when removing is_enclave_thread flag
+				// since these fields are not updated since then but will 
+				// load when context switch
+				current->thread.fsbase = current->backup_fsbase;
+				current->thread.gsbase = current->backup_gsbase;
+			}
+			else {
+				regs->ax = reg_status.rax;
+				regs->bx = reg_status.rbx;
+				regs->cx = reg_status.rcx;
+				regs->dx = reg_status.rdx;
+				regs->flags = reg_status.eflags;
+			}
+			regs->ip += 3;
+			return;
+		}
+		if (opcode_secondary == 0xEB) { // esgxmgr: EmuSGX User Manager Handlers
+			if (regs->ax == EMUSGX_MGROPS_REG_SELF) {
+				current->manager_entry = emusgx_register_manager(current->pid);
+				if (current->manager_entry == NULL) {
+					regs->ax = -1;
+				}
+				else {
+					regs->ax = 0;
+				}
+				regs->ip += 3;
+				return;
+			}
+			else if (regs->ax == EMUSGX_MGROPS_INIT_SYS) {
+				// Only init once
+				regs->ax = 0;
+				if (atomic_xchg(&emusgx_cr_inited, 1) == 0) {
+					// Not inited, do the initialization
+
+					// Get dispatcher online
+					emusgx_dispatcher_task = kthread_run(emusgx_dispatcher, (void *)0, "emusgx_dispatcher_task");
+					if (IS_ERR(emusgx_dispatcher_task)) {
+						pr_err("EmuSGX: Failed to create enclave sender task\n");
+						regs->ax = -1;
+					}
+
+					// Initialize the receive page
+					// and IRQ
+					emusgx_init_shared_page();
+					err = request_irq(EMUSGX_IRQ, emusgx_irq_handler, 0, "emusgx_irq_response", NULL);
+					if (err < 0) {
+						pr_info("EmuSGX: Failed to register IRQ handler, err = %d\n", err);
+						regs->ax = -1;
+					}
+				}
+				regs->ip += 3;
+				return;
+			}
+			else if (regs->ax == EMUSGX_MGROPS_SET_EPC) {
+				// RBX: Start
+				// RCX: Size
+				if (regs->bx % 4096 != 0) {
+					regs->ax = -1;
+					return;
+				}
+				if (regs->cx % 4096 != 0 || regs->bx + regs->cx <= regs->bx) {
+					regs->ax = -1;
+					return;
+				}
+				emusgx_epc_start = regs->bx;
+				emusgx_epc_end = regs->bx + regs->cx;
+				emusgx_init_epcm(regs->cx);
+				regs->ip += 3;
+				regs->ax = 0;
+				return;
+			}
+
+			else if (regs->ax == EMUSGX_MGROPS_GET_EPC) {
+				regs->bx = emusgx_epc_start;
+				regs->cx = emusgx_epc_end;
+				regs->ip += 3;
+				return;
+			}
+
+			else if (regs->ax == EMUSGX_MGROPS_WAIT_FOR_ACTION) {
+				emusgx_debug_print("EmuSGX: action_semaphore@0x%016llX\n", (uint64_t)&(((struct emusgx_user_space_manager_entry *)(current->manager_entry))->action_semaphore));
+				if (down_interruptible(&(((struct emusgx_user_space_manager_entry *)(current->manager_entry))->action_semaphore))) {
+					regs->ax = -EINVAL;
+					regs->ip += 3;
+					return;
+				}
+				// Got the semaphore
+				// Currently the handle buffer can only go setup a thread
+				// Return the action and in-kernel handle buffer address
+				// regs->ax = ((struct emusgx_user_space_manager_entry *)(current->manager_entry))->handle_buffer->action;
+				regs->bx = (uint64_t)(((struct emusgx_user_space_manager_entry *)(current->manager_entry))->handle_buffer);
+				emusgx_debug_print("EmuSGX: Action handled\n");
+
+				// We have copied the handle buffer address, now the slot is available again
+				up(&(((struct emusgx_user_space_manager_entry *)(current->manager_entry))->action_avail_semaphore));
+				regs->ip += 3;
+				regs->ax = 0;
+				return;
+			}
+			/*
+			else if (regs->ax == EMUSGX_MGROPS_START_SENDER) {
+				regs->ax = emusgx_start_sender();
+				regs->ip += 3;
+				return;
+			}
+
+			else if (regs->ax == EMUSGX_MGROPS_STOP_SENDER) {
+				regs->ax = emusgx_stop_sender();
+				regs->ip += 3;
+				return;
+			}
+			*/
+			else if (regs->ax == EMUSGX_MGROPS_CHECK_SENDER) {
+				if (emusgx_check_sender()) {
+					pr_info("EmuSGX: WARNING: No sender is found\n");
+					regs->ax = 1;
+				} 
+				else {
+					regs->ax = 0;
+				}
+				regs->ip += 3;
+				return;
+			}
+
+			else if (regs->ax == EMUSGX_MGROPS_SETUP_THREAD) {
+				// RBX: handle buffer
+
+				handle_buffer = (struct emusgx_handle_buffer *)regs->bx;
+
+				emusgx_debug_print("EmuSGX: Now setting up thread\n");
+				// Setup FS/GS base
+				// o Save first
+				// o FS/GS selectors are set to 0x0B
+				// o Other attributes in the `hidden portion`
+				//   are ignored in 64-bit mode
+				//savesegment(fs, current->backup_fs);
+				//savesegment(gs, current->backup_gs);
+				rdmsrl(MSR_FS_BASE, current->backup_fsbase);
+				rdmsrl(MSR_KERNEL_GS_BASE, current->backup_gsbase);
+				
+				//loadsegment(fs, 0x0B);
+				wrmsrl(MSR_FS_BASE, handle_buffer->fsbase);
+				//load_gs_index(0x0B);
+				wrmsrl(MSR_KERNEL_GS_BASE, handle_buffer->gsbase);
+				emusgx_debug_print("EmuSGX: Done FS/GS\n");
+
+				// Setup SECS and set is_enclave_thread
+				current->secs = handle_buffer->secs;
+				current->secs_pa = handle_buffer->secs_pa;
+				current->tcs = handle_buffer->tcs;
+				current->gpr = handle_buffer->gpr;
+				current->is_enclave_thread = 1;
+				current->emusgx_pid = handle_buffer->pid;
+				current->backup_regs_before_eenter = kmalloc(sizeof(struct pt_regs), GFP_KERNEL);
+				// Backup regs, do not have to +3 because 
+				memcpy(current->backup_regs_before_eenter, regs, sizeof(struct pt_regs));
+				full_regs = &(handle_buffer->regs);
+				emusgx_debug_print("EmuSGX: Done preprocessing\n");
+
+				// Setup registers
+				// o RAX and RCX have been setup in entrance
+				full_regs = &(handle_buffer->regs);
+				regs->ip = handle_buffer->rip;
+				regs->r15 = full_regs->r15;
+				regs->r14 = full_regs->r14;
+				regs->r13 = full_regs->r13;
+				regs->r12 = full_regs->r12;
+				regs->bp = full_regs->bp;
+				regs->bx = full_regs->bx;
+				regs->r11 = full_regs->r11;
+				regs->r10 = full_regs->r10;
+				regs->r9 = full_regs->r9;
+				regs->r8 = full_regs->r8;
+				regs->ax = full_regs->ax;
+				regs->cx = full_regs->cx;
+				regs->dx = full_regs->dx;
+				regs->si = full_regs->si;
+				regs->di = full_regs->di;
+				regs->flags = full_regs->flags;
+				regs->sp = full_regs->sp;
+				emusgx_debug_print("EmuSGX: Done setting up regs. IP = 0x%016lX\n", regs->ip);
+				emusgx_debug_print("EmuSGX: R9 = %ld\n", regs->r9);
+
+				// Now the handle buffer is useless
+				// Free it
+				kfree (handle_buffer);
+
+				// We are now at the state of enclave entrance
+				return;
+			}
+			else if (regs->ax == EMUSGX_MGROPS_DBGPRINT) {
+				// Copy message to buffer then print it
+				__uaccess_begin();
+				strncpy(debug_print_buffer, (void __user *)regs->bx, 99);
+				__uaccess_end();
+				debug_print_buffer[99] = 0;
+				vsgx_enclave_debug_print(debug_print_buffer);
+				regs->ip += 3;
+				return;
+			}
+			else {
+				pr_info("EmuSGX: Unknown manager operation\n");
+				regs->ax = -1;
+				regs->ip += 3;
+				return;
+			}
+		}
+		if (opcode_secondary == 0xEC) { // esgxsl: EmuSGX Switchless Page Syncing
+			// DOES NOT EXPECT TO RETURN
+
+			// The switchless page syncing process
+			emusgx_switchless_sync_worker();
+			
+			regs->ax = -1;
+			regs->ip += 3;
+			return;
+		}
+		if (opcode_secondary == 0xED) { // esgxes: EmuSGX Dispatcher
+			// DOES NOT EXPECT TO RETURN
+
+			emusgx_local_dispatcher(current->manager_entry->manager_nr);
+
+			// Only possible to be here when SIGKILL is issued
+			regs->ax = -1; 
+			regs->ip += 3;
+			return;
+		}
+	}
+
+
 	/*
 	 * We use UD2 as a short encoding for 'CALL __WARN', as such
 	 * handle it before exception entry to avoid recursive WARN
diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c
index 9c1545c37..a6aa3cd0e 100644
--- a/arch/x86/mm/fault.c
+++ b/arch/x86/mm/fault.c
@@ -19,6 +19,9 @@
 #include <linux/efi.h>			/* efi_recover_from_page_fault()*/
 #include <linux/mm_types.h>
 
+#include <linux/mm.h>
+#include <linux/mman.h>
+
 #include <asm/cpufeature.h>		/* boot_cpu_has, ...		*/
 #include <asm/traps.h>			/* dotraplinkage, ...		*/
 #include <asm/fixmap.h>			/* VSYSCALL_ADDR		*/
@@ -34,6 +37,10 @@
 #define CREATE_TRACE_POINTS
 #include <asm/trace/exceptions.h>
 
+#include <emusgx/emusgx_mm.h>
+#include <emusgx/emusgx_internal.h>
+#include <emusgx/emusgx_debug.h>
+
 /*
  * Returns 0 if mmiotrace is disabled, or if the fault is not
  * handled by mmiotrace:
@@ -1215,6 +1222,93 @@ do_kern_addr_fault(struct pt_regs *regs, unsigned long hw_error_code,
 }
 NOKPROBE_SYMBOL(do_kern_addr_fault);
 
+static int emusgx_handle_fault(struct mm_struct *mm, unsigned long address, struct pt_regs *regs) {
+	struct vm_area_struct *vma;
+	uint64_t page_addr = (address >> PAGE_SHIFT) << PAGE_SHIFT;
+	long mmap_error;
+	void *page_data;
+	uint8_t get_page_ret_val;
+	unsigned long mmap_populate;
+	int i;
+	uint64_t waiting_nr;
+	struct semaphore *other_waiting_semaphore;
+
+	// Check if the address is in ELRANGE
+	if (vsgx_check_in_elrange((void __user *)address)) {
+		pr_info("vSGX: PF within ELRANGE. AEX\n");
+		return -1;
+	}
+
+	// Try get the page from another VM
+	emusgx_debug_print("EmuSGX: Requesting data for page 0x%016lX\n", address);
+	get_page_ret_val = emusgx_get_guest_page(page_addr, &page_data, &other_waiting_semaphore, &waiting_nr);
+	if (get_page_ret_val) {
+		// The page is already set up
+		// We are good to return
+		mmap_read_unlock(mm);
+		return 0;
+	}
+	if (page_data != NULL) {
+		// Create mmap for such page
+		// We have mm locked so we could go all the way into do_mmap
+		mmap_read_unlock(mm);
+		// We have to WRITE
+		// pr_info("EmuSGX: Getting write lock\n");
+		if (mmap_write_lock_killable(mm)) {
+			pr_info("EmuSGX: mmap failed to get lock\n");
+			return -1;
+		}
+		// pr_info("EmuSGX: Doing mmap @ 0x%016llX\n", page_addr);
+		mmap_error = do_mmap(NULL, page_addr, 4096, PROT_READ | PROT_WRITE | PROT_EXEC, 
+			MAP_FIXED_NOREPLACE | MAP_ANONYMOUS| MAP_PRIVATE, 0, &mmap_populate, NULL);
+		// pr_info("EmuSGX: mmap is done\n");
+		mmap_write_unlock(mm);
+		// Release the write semaphore and grab the read semaphore again
+		mmap_read_lock(mm);
+		if (mmap_error != page_addr) {
+			pr_info("EmuSGX: mmap failed at 0x%016llX. Error = 0x%016lX\n", page_addr, mmap_error);				// Just let it fall to bad_area
+			return -1;
+		}
+		else {
+			// Good mmap
+			vma = find_vma(mm, address);
+			if (likely(vma)) {
+				// Write page
+				emusgx_clear_dirty((void *)page_addr);
+				if (copy_to_user((void *)page_addr, page_data, 4096)) {
+					pr_info("EmuSGX: Failed to copy to user\n");
+				}
+				// Clear the dirty bit
+				// page_data needs to be manually freed
+				// Create new slot for switchless syncing
+				mmap_read_unlock(mm);
+				// Here the create new slot will need to down_write mem_sem
+				// So we have to leave the semaphore open
+				if (emusgx_switchless_new_slot((void *)page_addr, page_data)) {
+					pr_info("EmuSGX: Failed to create slot\n");
+					// This is deadly
+					// Will fall to bad area
+					// So we will not be handling the do_unmmap
+					kfree(page_data);
+					mmap_read_lock(mm);
+					return -1;
+				}
+				kfree(page_data);
+
+				for (i = 0; i < waiting_nr; i++) {
+					// Wakeup other threads that are waiting for our thread to populate the data
+					up(other_waiting_semaphore);
+				}
+				// When return success, we do not need to re-grab the semaphore
+				return 0;
+			}
+			pr_info("EmuSGX: Why can't I find VMA?\n");
+			return -1;
+		}
+	}
+	return -1;
+}
+
 /* Handle faults in the user portion of the address space */
 static inline
 void do_user_addr_fault(struct pt_regs *regs,
@@ -1339,16 +1433,46 @@ void do_user_addr_fault(struct pt_regs *regs,
 
 	vma = find_vma(mm, address);
 	if (unlikely(!vma)) {
+		if (current->is_enclave_thread) {
+			if (!emusgx_handle_fault(mm, address, regs)) {
+				return;
+			}
+			pr_err("vSGX: Failed to handle page fault at 0x%016llX\n", (uint64_t)address);
+			// AEX
+			vsgx_aex_on_current_thread(regs, 14, hw_error_code, address );
+			return;
+		}
 		bad_area(regs, hw_error_code, address);
 		return;
 	}
 	if (likely(vma->vm_start <= address))
 		goto good_area;
 	if (unlikely(!(vma->vm_flags & VM_GROWSDOWN))) {
+		if (current->is_enclave_thread) {
+			if (!emusgx_handle_fault(mm, address, regs)) {
+				return;
+			}
+			pr_err("vSGX: Failed to handle page fault at 0x%016llX\n", (uint64_t)address);
+			// AEX
+			vsgx_aex_on_current_thread(regs, 14, hw_error_code, address);
+			return;
+		}
 		bad_area(regs, hw_error_code, address);
 		return;
 	}
-	if (unlikely(expand_stack(vma, address))) {
+	if (current->is_enclave_thread) {
+		if (!emusgx_handle_fault(mm, address, regs)) {
+			return;
+		}
+		pr_err("vSGX: Failed to handle page fault at 0x%016llX\n", (uint64_t)address);
+		// An enclave thread must not expand its stack anyway
+		// We do not allow doing that since the real payload
+		// is running on a trusted stack instead of this one
+		// AEX
+		vsgx_aex_on_current_thread(regs, 14, hw_error_code, address );
+		return;
+	}
+	else if (unlikely(expand_stack(vma, address))) {
 		bad_area(regs, hw_error_code, address);
 		return;
 	}
diff --git a/crypto/asymmetric_keys/public_key.c b/crypto/asymmetric_keys/public_key.c
index 788a4ba1e..9c4585651 100644
--- a/crypto/asymmetric_keys/public_key.c
+++ b/crypto/asymmetric_keys/public_key.c
@@ -14,11 +14,13 @@
 #include <linux/slab.h>
 #include <linux/seq_file.h>
 #include <linux/scatterlist.h>
+#include <linux/mpi.h>
 #include <keys/asymmetric-subtype.h>
 #include <crypto/public_key.h>
 #include <crypto/akcipher.h>
 #include <crypto/sm2.h>
 #include <crypto/sm3_base.h>
+#include <crypto/internal/akcipher.h>
 
 MODULE_DESCRIPTION("In-software asymmetric public-key subtype");
 MODULE_AUTHOR("Red Hat, Inc.");
@@ -387,6 +389,77 @@ int public_key_verify_signature(const struct public_key *pkey,
 }
 EXPORT_SYMBOL_GPL(public_key_verify_signature);
 
+int emusgx_rsa_public_key_verify_signature(uint8_t *n, uint64_t nsize, 
+					   uint8_t *e, uint64_t esize,
+					   const struct public_key_signature *sig)
+{
+	struct crypto_wait cwait;
+	struct crypto_akcipher *tfm;
+	struct akcipher_request *req;
+	struct scatterlist src_sg[2];
+	char alg_name[CRYPTO_MAX_ALG_NAME];
+	int ret;
+	struct rsa_mpi_key {
+		MPI n;
+		MPI e;
+		MPI d;
+	} *mpi_key;
+
+	pr_devel("==>%s()\n", __func__);
+
+	BUG_ON(!n);
+	BUG_ON(!e);
+	BUG_ON(!sig);
+	BUG_ON(!sig->s);
+
+	snprintf(alg_name, CRYPTO_MAX_ALG_NAME, "pkcs1pad(rsa,sha1)");
+
+	tfm = crypto_alloc_akcipher(alg_name, 0, 0);
+	if (IS_ERR(tfm))
+		return PTR_ERR(tfm);
+
+	ret = -ENOMEM;
+	req = akcipher_request_alloc(tfm, GFP_KERNEL);
+	if (!req)
+		goto error_free_tfm;
+
+	// set key
+	mpi_key = akcipher_tfm_ctx(tfm);
+	
+	mpi_key->e = mpi_read_raw_data(e, esize);
+	if (!mpi_key->e)
+		goto error_free_req;
+
+	mpi_key->n = mpi_read_raw_data(n, nsize);
+	if (!mpi_key->n)
+		goto error_free_req;
+	
+	if (nsize != 384) {
+		ret = -EINVAL;
+		goto error_free_req;
+	}
+
+	sg_init_table(src_sg, 2);
+	sg_set_buf(&src_sg[0], sig->s, sig->s_size);
+	sg_set_buf(&src_sg[1], sig->digest, sig->digest_size);
+	akcipher_request_set_crypt(req, src_sg, NULL, sig->s_size,
+				   sig->digest_size);
+	crypto_init_wait(&cwait);
+	akcipher_request_set_callback(req, CRYPTO_TFM_REQ_MAY_BACKLOG |
+				      CRYPTO_TFM_REQ_MAY_SLEEP,
+				      crypto_req_done, &cwait);
+	ret = crypto_wait_req(crypto_akcipher_verify(req), &cwait);
+
+error_free_req:
+	akcipher_request_free(req);
+error_free_tfm:
+	crypto_free_akcipher(tfm);
+	pr_devel("<==%s() = %d\n", __func__, ret);
+	if (WARN_ON_ONCE(ret > 0))
+		ret = -EINVAL;
+	return ret;
+}
+
 static int public_key_verify_signature_2(const struct key *key,
 					 const struct public_key_signature *sig)
 {
diff --git a/include/crypto/public_key.h b/include/crypto/public_key.h
index 948c5203c..e21647b0d 100644
--- a/include/crypto/public_key.h
+++ b/include/crypto/public_key.h
@@ -84,4 +84,8 @@ extern int verify_signature(const struct key *,
 int public_key_verify_signature(const struct public_key *pkey,
 				const struct public_key_signature *sig);
 
+int emusgx_rsa_public_key_verify_signature(uint8_t *n, uint64_t nsize, 
+					   uint8_t *e, uint64_t esize,
+					   const struct public_key_signature *sig);
+
 #endif /* _LINUX_PUBLIC_KEY_H */
diff --git a/include/emusgx/emusgx.h b/include/emusgx/emusgx.h
new file mode 100644
index 000000000..328368f07
--- /dev/null
+++ b/include/emusgx/emusgx.h
@@ -0,0 +1,115 @@
+#ifndef EMUSGX_H
+#define EMUSGX_H
+
+#include <linux/kernel.h>
+#include <linux/ptrace.h>
+#include <emusgx/emusgx_arch.h>
+#include <emusgx/emusgx_internal.h>
+
+#define EMUSGX_ECREATE		0x00
+#define EMUSGX_EADD		0x01
+#define EMUSGX_EINIT		0x02
+#define EMUSGX_EREMOVE		0x03
+#define EMUSGX_EDBGRD		0x04
+#define EMUSGX_EDBGWR		0x05
+#define EMUSGX_EEXTEND		0x06
+#define EMUSGX_ELDB		0x07
+#define EMUSGX_ELDU		0x08
+#define EMUSGX_EBLOCK		0x09
+#define EMUSGX_EPA		0x0A
+#define EMUSGX_EWB		0x0B
+#define EMUSGX_ETRACK		0x0C
+#define EMUSGX_EAUG		0x0D
+#define EMUSGX_EMODPR		0x0E
+#define EMUSGX_EMODT		0x0F
+
+#define EMUSGX_EREPORT		0x00
+#define EMUSGX_EGETKEY		0x01
+#define EMUSGX_EENTER		0x02
+#define EMUSGX_ERESUME		0x03
+#define EMUSGX_EEXIT		0x04
+#define EMUSGX_EACCEPT		0x05
+#define EMUSGX_EMODPE		0x06
+#define EMUSGX_EACCEPTCOPY	0x07
+
+#define EMUSGX_SUCCESS			0
+#define EMUSGX_INVALID_SIG_STRUCT	1
+#define EMUSGX_INVALID_ATTRIBUTE	2
+#define EMUSGX_BLKSTATE			3
+#define EMUSGX_INVALID_MEASUREMENT	4
+#define EMUSGX_NOTBLOCKABLE		5
+#define EMUSGX_PG_INVLD			6
+#define EMUSGX_LOCKFAIL			7
+#define EMUSGX_INVALID_SIGNATURE	8
+#define EMUSGX_MAC_COMPARE_FAIL		9
+#define EMUSGX_PAGE_NOT_BLOCKED		10
+#define EMUSGX_NOT_TRACKED		11
+#define EMUSGX_VA_SLOT_OCCUPIED		12
+#define EMUSGX_CHILD_PRESENT		13
+#define EMUSGX_ENCLAVE_ACT		14
+#define EMUSGX_ENTRYEPOCH_LOCKED	15
+#define EMUSGX_INVALID_EINITTOKEN	16
+#define EMUSGX_PREV_TRK_INCMPL		17
+#define EMUSGX_PG_IS_SECS		18
+#define EMUSGX_PAGE_ATTRIBUTES_MISMATCH	19
+#define EMUSGX_PAGE_NOT_MODIFIABLE	20
+#define EMUSGX_INVALID_CPUSVN		32
+#define EMUSGX_INVALID_ISVSVN		64
+#define EMUSGX_UNMASKED_EVENT		128
+#define EMUSGX_INVALID_KEYNAME		256
+
+#define EMUSGX_PF_RBX			508
+#define EMUSGX_GP			509
+#define EMUSGX_PF_RCX			510
+#define EMUSGX_PF_RDX			511
+
+struct emusgx_regs {
+	uint64_t rax;
+	uint64_t rbx;
+	uint64_t rcx;
+	uint64_t rdx;
+	union {
+		unsigned long eflags;
+		struct {
+			uint8_t CF 		: 1;
+			uint8_t Reserved1	: 1; // always 1
+			uint8_t PF		: 1;
+			uint8_t Reserved2	: 1; // always 0
+			uint8_t AF		: 1;
+			uint8_t Reserved3	: 1; // always 0
+			uint8_t ZF		: 1;
+			uint8_t SF		: 1;
+			uint8_t TFIFDF		: 3; // don't care
+			uint8_t OF		: 1;
+			uint32_t DONOTCARE	: 20;
+			uint32_t ZEROS		: 32;
+		} __attribute__((__packed__)) flags;
+	};
+	
+};
+
+struct emusgx_epcm {
+	uint8_t valid;
+	uint8_t R;
+	uint8_t W;
+	uint8_t X;
+	uint8_t page_type;
+	uint64_t enclave_secs;
+	void *enclave_address;
+	uint8_t blocked;
+	uint8_t pending;
+	uint8_t modified;
+	uint8_t secs_inited;
+	struct emusgx_user_space_manager_entry *manager_entry;
+};
+
+extern uint64_t emusgx_csr_owner_epoch[2];
+extern uint64_t emusgx_cr_seal_fuses[2];
+extern uint64_t emusgx_cr_cpusvn[2];
+extern uint64_t emusgx_cr_report_keyid[4];
+extern uint32_t emusgx_csr_intelpubkeyhash[8];
+extern uint8_t  emusgx_cr_base_pk[16];
+
+void emusgx_handle_enclu(struct emusgx_regs *reg_status, struct pt_regs *ptrace_regs);
+
+#endif // EMUSGX_H
\ No newline at end of file
diff --git a/include/emusgx/emusgx_arch.h b/include/emusgx/emusgx_arch.h
new file mode 100644
index 000000000..f4748bb02
--- /dev/null
+++ b/include/emusgx/emusgx_arch.h
@@ -0,0 +1,242 @@
+#ifndef EMUSGX_ARCH_H
+#define EMUSGX_ARCH_H
+
+#include <linux/kernel.h>
+
+#define SGX_SECS_RESERVED1_SIZE 24
+#define SGX_SECS_RESERVED2_SIZE 32
+#define SGX_SECS_RESERVED3_SIZE 96
+#define SGX_SECS_RESERVED4_SIZE 3460
+
+extern const uint8_t HARDCODED_PKCS1_5_PADDING[352];
+
+struct sgx_secs {
+	uint64_t size;
+	uint64_t base;
+	uint32_t ssaframesize;
+	uint32_t miscselect;
+	uint8_t reserved1[SGX_SECS_RESERVED1_SIZE];
+	union {
+		uint64_t attributes;
+		struct {
+			uint8_t reserved1	: 1;
+			uint8_t debug		: 1;
+			uint8_t mod64bit	: 1;
+			uint8_t reserved2	: 1;
+			uint8_t provisionkey	: 1;
+			uint8_t einittokenkey	: 1;
+			uint64_t reserved3	: 58;
+		}  __attribute__((__packed__)) attribute;
+	};
+	uint64_t xfrm;
+	uint32_t mrenclave[8];
+	uint8_t reserved2[SGX_SECS_RESERVED2_SIZE];
+	uint32_t mrsigner[8];
+	uint8_t	reserved3[SGX_SECS_RESERVED3_SIZE];
+	uint16_t isvprodid;
+	uint16_t isvsvn;
+	uint64_t eid;
+	uint8_t	padding[352];
+	uint64_t mrenclave_update_counter;
+	uint64_t manager_entry;
+	uint8_t reserved4[SGX_SECS_RESERVED4_SIZE];
+} __attribute__((__packed__));
+
+#define SGX_PT_SECS		0x00
+#define SGX_PT_TCS		0x01
+#define SGX_PT_REG		0x02
+#define SGX_PT_VA		0x03
+#define SGX_PT_TRIM		0x04
+
+struct sgx_secinfo {
+	struct {
+		uint8_t R 		: 1;
+		uint8_t W		: 1;
+		uint8_t X		: 1;
+		uint8_t pending		: 1;
+		uint8_t modified	: 1;
+		uint8_t reserved	: 3;
+		uint8_t page_type	: 8;
+		uint64_t reserved2	: 48;
+	} flags __attribute__((__packed__));
+	uint64_t reserved[7];
+} __attribute__((__packed__));
+
+#define SGX_LAUNCH_KEY 		0
+#define SGX_PROVISION_KEY	1
+#define SGX_PROVISION_SEAL_KEY	2
+#define SGX_REPORT_KEY		3
+#define SGX_SEAL_KEY		4
+
+struct sgx_keyrequest {
+	uint16_t keyname;
+	union {
+		uint16_t keypolicy;
+		struct {
+			uint8_t mrenclave	: 1;
+			uint8_t mrsigner	: 1;
+			uint64_t reserved	: 14;
+		} __attribute__((__packed__)) policy;
+	};
+	uint16_t isvsvn;
+	uint16_t reserved;
+	uint64_t cpusvn[2];
+	uint64_t attributemask[2];
+	uint64_t keyid[4];
+	uint32_t miscmask;
+	uint8_t reserved2[436];
+} __attribute__((__packed__));
+
+struct sgx_targetinfo {
+	uint32_t measurement[8];
+	uint64_t attributes[2];
+	uint32_t reserved1;
+	uint32_t miscselect;
+	uint8_t reserved2[456];
+} __attribute__((__packed__));
+
+struct sgx_report {
+	uint64_t cpusvn[2];
+	uint32_t miscselect;
+	uint8_t reserved1[28];
+	uint64_t attributes[2];
+	uint32_t mrenclave[8]; 
+	uint64_t reserved2[4];
+	uint32_t mrsigner[8];
+	uint8_t reserved3[96];
+	uint16_t isvprodid;
+	uint16_t isvsvn;
+	uint8_t reserved4[60];
+	uint8_t reportdata[64]; // 64 byte buffer
+	uint64_t keyid[4];
+	uint64_t mac[2];
+} __attribute__((__packed__));
+
+struct sgx_tcs {
+	uint64_t state;
+	struct {
+		uint8_t dbgoptin : 1;
+		uint64_t reserved : 63;
+	} __attribute__((__packed__)) flags;
+	uint64_t ossa;
+	uint32_t cssa;
+	uint32_t nssa;
+	uint64_t oentry;
+	uint64_t aep;
+	uint64_t ofsbasgx;
+	uint64_t ogsbasgx;
+	uint32_t fslimit;
+	uint32_t gslimit;
+	uint64_t pid;
+	uint8_t reserved4[4016]; // We use first 8 bytes of reserved field to store PID in the guest VM
+} __attribute__((__packed__));
+
+struct sgx_pageinfo {
+	uint64_t linaddr;
+	uint64_t srcpage;
+	union {
+		uint64_t secinfo;
+		uint64_t pcmd;
+	};
+	uint64_t secs;
+} __attribute__((__packed__));
+
+struct sgx_sigstruct {
+	uint64_t header[2];
+	uint32_t vendor;
+	uint32_t date;
+	uint8_t header2[16];
+	uint32_t swdefined;
+	uint8_t reserved1[84];
+	uint8_t modulus[384];
+	uint32_t exponent;
+	uint8_t signature[384];
+	uint32_t miscselect;
+	uint32_t miscmask;
+	uint8_t reserved2[20];
+	uint64_t attributes[2];
+	uint64_t attributemask[2];
+	uint32_t enclavehash[8];
+	uint8_t reserved3[32];
+	uint16_t isvprodid;
+	uint16_t isvsvn;
+	uint8_t reserved4[12];
+	uint8_t q1[384];
+	uint8_t q2[384];
+} __attribute__((__packed__));
+
+struct sgx_einittoken{
+	uint32_t valid;
+	uint8_t reserved1[44];
+	uint64_t attributes[2];
+	uint64_t attributemask[2];
+	uint32_t mrenclave[8];
+	uint8_t reserved2[32];
+	uint32_t mrsigner[8];
+	uint8_t reserved3[32];
+	uint64_t cpusvnle[2];
+	uint16_t isvprodidle;
+	uint16_t isvsvnle;
+	uint8_t reserved4[24];
+	uint32_t maskedmiscselectle;
+	uint64_t maskedattributesle[2];
+	uint64_t keyid[4];
+	uint64_t mac[2];
+} __attribute__((__packed__));
+
+struct sgx_pcmd {
+	struct sgx_secinfo secinfo;
+	uint64_t enclaveid;
+	uint8_t reserved[40];
+	uint64_t mac[2];
+} __attribute__((__packed__));
+
+// From OpenSGX implementation
+struct emusgx_mac_header { //128 bytes...
+	uint64_t eid;
+	uint64_t linaddr;
+	struct sgx_secinfo secinfo; //64 bytes
+	uint8_t padding[48]; // padding bytes to make it 128 byte ...
+} __attribute__((__packed__));
+
+struct sgx_exitinfo {
+    uint32_t    vector		: 8;
+    uint32_t    exit_type	: 3;
+    uint32_t    reserved	: 20;
+    uint32_t    valid		: 1;
+} __attribute__((__packed__));
+
+struct sgx_exinfo {
+	uint64_t maddr;
+	uint32_t errcd;
+	uint32_t reserved;
+} __attribute__((__packed__));
+
+struct sgx_gprsgx {
+	uint64_t rax;
+	uint64_t rcx;
+	uint64_t rdx;
+	uint64_t rbx;
+	uint64_t rsp;
+	uint64_t rbp;
+	uint64_t rsi;
+	uint64_t rdi;
+	uint64_t r8;
+	uint64_t r9;
+	uint64_t r10;
+	uint64_t r11;
+	uint64_t r12;
+	uint64_t r13;
+	uint64_t r14;
+	uint64_t r15;
+	uint64_t rflags;
+	uint64_t rip;
+	uint64_t ursp;
+	uint64_t urbp;
+	struct sgx_exitinfo exitinfo;
+	uint32_t reserved;
+	uint64_t fsbase;
+	uint64_t gsbase;
+} __attribute__((__packed__));
+
+#endif
\ No newline at end of file
diff --git a/include/emusgx/emusgx_cpuid.h b/include/emusgx/emusgx_cpuid.h
new file mode 100644
index 000000000..ba650cf99
--- /dev/null
+++ b/include/emusgx/emusgx_cpuid.h
@@ -0,0 +1,12 @@
+#ifndef EMUSGX_CPUID_H
+#define EMUSGX_CPUID_H
+
+#define KVM_CPUID_EMUSGX_ENCLAVE_SEND_PAGE 	0x40000002
+#define KVM_CPUID_EMUSGX_ENCLAVE_ACK_PAGE  	0x40000003
+#define KVM_CPUID_EMUSGX_ENCLAVE_SHARE_PAGE	0x40000004
+#define KVM_CPUID_EMUSGX_RUN_SENDER_KTHREADS	0x40000009
+#define KVM_CPUID_EMUSGX_STOP_SENDER_KTHREADS	0x4000000A
+#define KVM_CPUID_EMUSGX_CHECK_SENDER_KTHREADS	0x4000000B
+#define KVM_CPUID_EMUSGX_ENCLAVE_RETRIVE_PAGE	0x4000000D
+
+#endif
\ No newline at end of file
diff --git a/include/emusgx/emusgx_debug.h b/include/emusgx/emusgx_debug.h
new file mode 100644
index 000000000..3dfc7c903
--- /dev/null
+++ b/include/emusgx/emusgx_debug.h
@@ -0,0 +1,8 @@
+#ifndef EMUSGX_DEBUG_H
+#define EMUSGX_DEBUG_H
+
+// #define emusgx_debug_print(x, ...) pr_info(x, ##__VA_ARGS__)
+
+#define emusgx_debug_print(x, ...)
+
+#endif
\ No newline at end of file
diff --git a/include/emusgx/emusgx_fault.h b/include/emusgx/emusgx_fault.h
new file mode 100644
index 000000000..184b7c473
--- /dev/null
+++ b/include/emusgx/emusgx_fault.h
@@ -0,0 +1,9 @@
+#ifndef EMUSGX_FAULT_H
+#define EMUSGX_FAULT_H
+
+#include <linux/ptrace.h>
+
+void emusgx_gp(int code, struct pt_regs *ptrace_regs);
+void emusgx_pf(void *addr, struct pt_regs *ptrace_regs);
+
+#endif
\ No newline at end of file
diff --git a/include/emusgx/emusgx_internal.h b/include/emusgx/emusgx_internal.h
new file mode 100644
index 000000000..33aa9d573
--- /dev/null
+++ b/include/emusgx/emusgx_internal.h
@@ -0,0 +1,173 @@
+#ifndef EMUSGX_INTERNAL_H
+#define EMUSGX_INTERNAL_H
+
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/interrupt.h>
+#include <linux/spinlock.h>
+#include <linux/semaphore.h>
+
+#include <emusgx/emusgx_sender.h>
+
+#define EMUSGX_SWITCHLESS_SLOT_COUNT	10
+#define EMUSGX_MAXIMUM_ENCLAVES 	10
+
+#define _enclave_local
+
+struct emusgx_tmp_key_dependencies {
+	uint16_t keyname;
+	uint16_t isvprodid;
+	uint16_t isvsvn;
+	uint64_t ownerepoch[2];
+	uint64_t attributes[2];
+	uint64_t attributesmask[2];
+	uint32_t mrenclave[8];
+	uint32_t mrsigner[8];
+	uint64_t keyid[4];
+	uint64_t seal_key_fuses[2];
+	uint64_t cpusvn[2];
+	uint8_t	padding[352];
+	uint32_t miscselect;
+	uint32_t miscmask;
+};
+
+#define EMUSGX_MGRSTAT_EMPTY		0
+#define EMUSGX_MGRSTAT_FREE		1
+#define EMUSGX_MGRSTAT_OCCUPIED		2
+
+#define EMUSGX_MGRACT_NEW_THREAD	1
+
+struct emusgx_handle_buffer {
+	uint8_t action;
+	struct emusgx_full_regs regs;
+	uint64_t fsbase;
+	uint64_t gsbase;
+	uint64_t pid;
+	struct sgx_secs *secs;
+	uint64_t secs_pa;
+	uint64_t rip;
+	struct sgx_tcs *tcs;
+	struct sgx_gprsgx *gpr;
+} __attribute__((__packed__));
+
+struct emusgx_local_dispatch_queue_node {
+	void *package;
+	struct emusgx_local_dispatch_queue_node *next;
+};
+
+struct emusgx_user_space_manager_entry {
+	uint8_t status;
+	uint64_t manager_nr;
+	pid_t pid;
+	struct emusgx_handle_buffer *handle_buffer;
+	struct semaphore action_semaphore;
+	struct semaphore action_avail_semaphore;
+	struct semaphore local_dispatcher_semaphore;
+	spinlock_t local_dispatch_queue_lock;
+	struct emusgx_local_dispatch_queue_node *local_dispatch_queue;
+	struct emusgx_local_dispatch_queue_node *local_dispatch_queue_tail;
+	void *secs_pa;
+};
+
+#define EMUSGX_MGROPS_REG_SELF		0
+#define EMUSGX_MGROPS_SET_EPC		1
+#define EMUSGX_MGROPS_GET_EPC		2
+#define EMUSGX_MGROPS_SETUP_THREAD	3
+#define EMUSGX_MGROPS_WAIT_FOR_ACTION	4
+#define EMUSGX_MGROPS_START_SENDER	5
+#define EMUSGX_MGROPS_STOP_SENDER	6
+#define EMUSGX_MGROPS_CHECK_SENDER	7
+#define EMUSGX_MGROPS_INIT_SYS		8
+#define EMUSGX_MGROPS_DBGPRINT		9
+
+extern uint64_t emusgx_epc_start;
+extern uint64_t emusgx_epc_end;
+extern atomic_t emusgx_cr_inited;
+uint8_t emusgx_check_within_epc(void *addr);
+
+// Returns a 128-bit secret key
+// The user is resiponsible for freeing the key
+uint64_t *emusgx_derive_key(const struct emusgx_tmp_key_dependencies *key_dependencies);
+
+// Returns a 128-bit MAC
+// The user is resiponsible for freeing the MAC
+uint64_t *emusgx_cmac(const uint64_t *key, const void *data, const size_t size);
+
+// SHA256
+void emusgx_sha256init(uint32_t *original);
+void emusgx_sha256update(uint32_t *original, uint64_t *data);
+void emusgx_sha256final(uint32_t *original, uint64_t counter);
+// The user is resiponsible for freeing the hash 
+uint32_t *emusgx_sha256(uint64_t *data, size_t len);
+
+// EID
+uint64_t vsgx_get_new_eid(void);
+
+// ELRANGE
+int _enclave_local vsgx_check_in_elrange(void __user *addr);
+
+// EPCM
+uint8_t emusgx_init_epcm(uint64_t epc_size);
+struct emusgx_epcm *emusgx_get_epcm(void *epc_page);
+uint64_t *emusgx_get_cpusvn(void);
+int emusgx_compare_cpusvn(const uint64_t *cpusvn1, const uint64_t *cpusvn2);
+
+// VMA
+int vsgx_confirm_mapping(void __user *vaddr, void *paddr);
+void _enclave_local vsgx_map_epc_to_vaddr(void __user *vaddr, uint64_t epc_addr);
+void _enclave_local vsgx_unmap_epc_from_vaddr(void __user *vaddr);
+uint64_t vsgx_vaddr_to_paddr(void __user *vaddr, int enclave_nr);
+void _enclave_local vsgx_check_and_unmap_page(void __user *linaddr);
+long _enclave_local vsgx_map_empty_page_to_user_space(void __user *linaddr);
+
+uint8_t emusgx_is_secs_inited(void *secs_pa);
+void emusgx_mark_secs_inited(struct sgx_secs *secs, uint8_t inited);
+
+uint8_t emusgx_verify_sigstruct(struct sgx_sigstruct *sig);
+
+uint8_t emusgx_secs_has_associated_page(struct sgx_secs *secs_pa);
+
+int emusgx_aes_128_gcm_dec(uint8_t *key, uint64_t *counter, void *aad, size_t aad_size, 
+				void *cipher_text, size_t cipher_size, void *plain_text, uint64_t *mac);
+
+int emusgx_aes_128_gcm_enc(uint8_t *key, uint64_t *counter, void *aad, size_t aad_size, 
+				void *plain_text, size_t plain_size, void *cipher_text, uint64_t *mac);
+
+extern uint8_t *emusgx_internal_cr_cross_vm_key;
+
+struct emusgx_user_space_manager_entry *emusgx_register_manager(pid_t current_pid);
+struct emusgx_user_space_manager_entry *emusgx_get_free_manager(void);
+struct emusgx_user_space_manager_entry *emusgx_get_manager(int manager_nr);
+void emusgx_free_current_manager(void);
+int vsgx_need_new_manager(void *package);
+int emusgx_find_enclave_with_package(void *package);
+void emusgx_queue_local_dispatch_request(void *package, uint64_t manager_nr);
+
+struct emusgx_page_fault_queue_node _enclave_local *emusgx_register_pf_request(void *addr, uint8_t *is_main);
+
+int emusgx_send_data(void *addr, uint64_t size);
+
+void _enclave_local emusgx_switchless_write_page(struct emusgx_page_package *package);
+int _enclave_local emusgx_switchless_new_slot(void *addr, void *page_data);
+int emusgx_switchless_get_slot(void *addr, uint64_t manager_nr);
+int emusgx_switchless_get_and_hold_slot(void *addr, uint64_t manager_nr);
+void _enclave_local emusgx_sync_all_pages(void);
+void _enclave_local emusgx_switchless_sync_worker(void);
+
+uint64_t emusgx_enter_enclave(struct sgx_tcs *tcs, void *aep, uint64_t pid, struct emusgx_full_regs *regs, void **pf_addr);
+uint8_t vsgx_resume_enclave(struct sgx_tcs __user *tcs, uint64_t tcs_pa, uint64_t aep, uint64_t pid, void **pf_addr);
+
+void emusgx_do_remote_for_encls(uint8_t instr, void *package);
+
+irqreturn_t emusgx_irq_handler(int irq, void *dev_id);
+
+int emusgx_dispatcher(void *dummy);
+void _enclave_local emusgx_local_dispatcher(int manager_nr);
+
+/*int emusgx_start_sender(void);
+int emusgx_stop_sender(void);*/
+int emusgx_check_sender(void);
+
+void _enclave_local vsgx_aex_on_current_thread(struct pt_regs *ptrace_regs, uint8_t exception_code, uint32_t error_code, uint64_t cr2);
+
+#endif
\ No newline at end of file
diff --git a/include/emusgx/emusgx_mm.h b/include/emusgx/emusgx_mm.h
new file mode 100644
index 000000000..4048589bc
--- /dev/null
+++ b/include/emusgx/emusgx_mm.h
@@ -0,0 +1,12 @@
+#ifndef EMUSGX_MM_H
+#define EMUSGX_MM_H
+
+#include <linux/kernel.h>
+
+extern void *emusgx_receive_page;
+void emusgx_init_shared_page(void);
+uint8_t emusgx_get_guest_page(uint64_t addr, void **page_data_addr, struct semaphore **other_waiting_semaphore, uint64_t *waiting_nr);
+
+void emusgx_clear_dirty(void *address);
+
+#endif
\ No newline at end of file
diff --git a/include/emusgx/emusgx_sender.h b/include/emusgx/emusgx_sender.h
new file mode 100644
index 000000000..cadbd98af
--- /dev/null
+++ b/include/emusgx/emusgx_sender.h
@@ -0,0 +1,279 @@
+#ifndef EMUSGX_SENDER_H
+#define EMUSGX_SENDER_H
+
+#include <linux/semaphore.h>
+
+#define EMUSGX_PAYLOAD_SIZE (4096 - 6 * sizeof(uint64_t))
+
+#define EMUSGX_MAX_SESSION_NUMBER_D64	512
+#define EMUSGX_MAX_SESSION_NUMBER	(EMUSGX_MAX_SESSION_NUMBER_D64 * 64)
+
+#define EMUSGX_S_ECREATE	0x00
+#define EMUSGX_S_EADD		0x01
+#define EMUSGX_S_EINIT		0x02
+#define EMUSGX_S_EREMOVE	0x03
+#define EMUSGX_S_EDBGRD		0x04
+#define EMUSGX_S_EDBGWR		0x05
+#define EMUSGX_S_EEXTEND	0x06
+#define EMUSGX_S_ELDB		0x07
+#define EMUSGX_S_ELDU		0x08
+#define EMUSGX_S_EBLOCK		0x09
+#define EMUSGX_S_EPA		0x0A
+#define EMUSGX_S_EWB		0x0B
+#define EMUSGX_S_ETRACK		0x0C
+#define EMUSGX_S_EAUG		0x0D
+#define EMUSGX_S_EMODPR		0x0E
+#define EMUSGX_S_EMODT		0x0F
+
+#define EMUSGX_S_EREPORT	0x10
+#define EMUSGX_S_EGETKEY	0x11
+#define EMUSGX_S_EENTER		0x12
+#define EMUSGX_S_ERESUME	0x13
+#define EMUSGX_S_EEXIT		0x14
+#define EMUSGX_S_EACCEPT	0x15
+#define EMUSGX_S_EMODPE		0x16
+#define EMUSGX_S_EACCEPTCOPY	0x17
+
+#define EMUSGX_S_PAGEREQ	0x18
+#define EMUSGX_S_SWITCHLESS	0x19
+#define EMUSGX_S_FAULT		0x20
+
+#define EMUSGX_S_AEX		0x21
+
+#include <linux/kernel.h>
+#include <linux/spinlock.h>
+#include <linux/semaphore.h>
+#include <linux/atomic.h>
+
+#include <emusgx/emusgx_arch.h>
+
+// order starts from 0 and ends at total_pages - 1
+struct emusgx_cross_vm_package {
+	// Encrypted
+	uint64_t session_number;
+	uint64_t order;
+	uint64_t total_pages;
+	uint64_t total_size;
+
+	uint8_t payload[EMUSGX_PAYLOAD_SIZE];
+
+	// Non-encrypted MAC
+
+	uint64_t mac[2];
+} __attribute__((__packed__));
+
+struct emusgx_full_regs {
+/*
+ * C ABI says these regs are callee-preserved. They aren't saved on kernel entry
+ * unless syscall needs a complete, fully filled "struct pt_regs".
+ */
+	unsigned long r15;
+	unsigned long r14;
+	unsigned long r13;
+	unsigned long r12;
+	unsigned long bp;
+	unsigned long bx;
+/* These regs are callee-clobbered. Always saved on kernel entry. */
+	unsigned long r11;
+	unsigned long r10;
+	unsigned long r9;
+	unsigned long r8;
+	unsigned long ax;
+	unsigned long cx;
+	unsigned long dx;
+	unsigned long si;
+	unsigned long di;
+/* Return frame for iretq */
+	unsigned long ip;
+	//unsigned long cs;
+	unsigned long flags;
+	unsigned long sp;
+	//unsigned long ss;
+} __attribute__((__packed__));
+
+struct emusgx_eenter_package {
+	uint8_t instr;
+	uint64_t tcs;
+	uint64_t tcs_pa;
+	uint64_t aep;
+	uint64_t pid;
+	struct emusgx_full_regs regs;
+} __attribute__((__packed__));
+
+#define EMUSGX_DISPATCH_SLOT_FREE	0
+#define EMUSGX_DISPATCH_SLOT_INUSE	1
+
+struct emusgx_dispatch_slot {
+	uint8_t status;
+	uint64_t session_number;
+	uint64_t total_pages;
+	uint64_t current_order;
+	uint64_t total_size;
+	void *data;
+};
+
+// Used for IRQ data dispatch
+struct emusgx_request_queue_node {
+	void *page;
+	struct emusgx_request_queue_node *next;
+};
+
+struct emusgx_page_package {
+	uint8_t instr;
+	uint64_t addr;
+	uint64_t id; // Use this as ID
+	uint8_t page[4096];
+	uint8_t mask[512]; // 4096-bit bitmap for each byte
+} __attribute__((__packed__));
+
+// slot field shall be allocated before registration
+// data field in the slot field shall also be pre-allocated
+struct emusgx_page_fault_queue_node {
+	struct emusgx_dispatch_slot *slot;
+	struct semaphore *node_semaphore;
+	void *addr;
+	atomic_t waiting_nr;
+	uint64_t manager_nr;
+	struct emusgx_page_fault_queue_node *next;
+	struct semaphore other_waiting_semaphore;
+};
+
+struct emusgx_aex_queue_node {
+	struct emusgx_dispatch_slot *slot;
+	struct semaphore semaphore;
+	uint64_t tcs_pa;
+	struct emusgx_aex_queue_node *next;
+};
+
+extern spinlock_t emusgx_dispatcher_queue_lock;
+extern struct semaphore emusgx_dispatcher_sem;
+extern struct emusgx_request_queue_node *emusgx_request_queue;
+extern struct emusgx_request_queue_node *emusgx_request_queue_tail;
+
+extern char *emusgx_static_aad;
+
+struct emusgx_raw_response {
+	uint8_t instr;
+	uint64_t response;
+	uint64_t linaddr;
+	uint8_t write_back; // Must be 0
+} __attribute__((__packed__));
+
+struct emusgx_raw_response_with_page {
+	uint8_t instr;
+	uint64_t response;
+	uint64_t linaddr;
+	uint8_t write_back; // Must be 1
+	uint8_t page[4096]; 
+	struct sgx_pcmd pcmd;
+} __attribute__((__packed__));
+
+struct emusgx_eadd_package {
+	uint8_t instr;
+	uint64_t secs;
+	uint64_t linaddr;
+	uint64_t epc_page;
+	uint8_t srcpage[4096];
+	struct sgx_secinfo secinfo;
+} __attribute__((__packed__));
+
+struct emusgx_eaug_package {
+	uint8_t instr;
+	uint64_t secs;
+	uint64_t linaddr;
+	uint64_t epc_addr;
+} __attribute__((__packed__));
+
+struct emusgx_eblock_package {
+	uint8_t instr;
+	uint64_t epc_page;
+} __attribute__((__packed__));
+
+struct emusgx_ecreate_package {
+	uint8_t instr;
+	uint8_t srcpage[4096];
+	uint64_t epc_page;
+} __attribute__((__packed__));
+
+struct emusgx_eextend_package {
+	uint8_t instr;
+	uint64_t addr;
+} __attribute__((__packed__));
+
+struct emusgx_einit_package {
+	uint8_t instr;
+	struct sgx_sigstruct sigstruct;
+	uint64_t secs;
+	struct sgx_einittoken einittoken;
+} __attribute__((__packed__));
+
+struct emusgx_eldb_eldu_package {
+	uint8_t instr;
+	uint8_t srcpage[4096];
+	uint64_t secs;
+	uint64_t vaslot;
+	uint64_t epc_page;
+	struct sgx_pcmd pcmd;
+	uint64_t linaddr;
+	uint8_t block;
+} __attribute__((__packed__));
+
+struct emusgx_emodpr_package {
+	uint8_t instr;
+	uint64_t epc_page;
+	uint8_t R;
+	uint8_t W;
+	uint8_t X;
+} __attribute__((__packed__));
+
+struct emusgx_emodt_package {
+	uint8_t instr;
+	uint64_t epc_page;
+	uint8_t R;
+	uint8_t W;
+	uint8_t page_type;
+} __attribute__((__packed__));
+
+struct emusgx_epa_package {
+	uint8_t instr;
+	uint64_t epc_page;
+} __attribute__((__packed__));
+
+struct emusgx_eremove_package {
+	uint8_t instr;
+	uint64_t epc_page;
+} __attribute__((__packed__));
+
+struct emusgx_ewb_package {
+	uint8_t instr;
+	uint64_t epc_page;
+	uint64_t vaslot;
+	uint64_t linaddr;
+} __attribute__((__packed__));
+
+struct emusgx_eexit_package {
+	uint8_t instr;
+	uint64_t pid;
+	struct emusgx_full_regs regs;
+} __attribute__((__packed__));
+
+struct emusgx_eresume_package {
+	uint8_t instr;
+	uint64_t tcs_pa;
+	uint64_t tcs;
+	uint64_t aep;
+	uint64_t pid;
+} __attribute__((__packed__));
+
+struct emusgx_aex_package {
+	uint8_t instr;
+	uint64_t pid;
+	struct emusgx_full_regs regs; // Synthetic state
+	uint8_t exception_code;
+	uint32_t error_code;
+	uint64_t fault_addr;
+} __attribute__((__packed__));
+
+#define EMUSGX_IRQ 				10
+
+#endif
\ No newline at end of file
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 76cd21fa5..fe9a8de05 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -637,6 +637,9 @@ struct wake_q_node {
 	struct wake_q_node *next;
 };
 
+#include <emusgx/emusgx_arch.h>
+#include <emusgx/emusgx_internal.h>
+
 struct task_struct {
 #ifdef CONFIG_THREAD_INFO_IN_TASK
 	/*
@@ -660,6 +663,20 @@ struct task_struct {
 	unsigned int			flags;
 	unsigned int			ptrace;
 
+	unsigned char			is_enclave_thread;
+	struct sgx_secs *		secs;
+	uint64_t			secs_pa;
+	struct sgx_tcs *		tcs;
+	struct sgx_gprsgx *		gpr;
+	struct emusgx_user_space_manager_entry *manager_entry;
+	unsigned long			emusgx_pid;
+	struct pt_regs *		backup_regs_before_eenter;
+
+	unsigned short			backup_fs;
+	unsigned short			backup_gs;
+	unsigned long			backup_fsbase;
+	unsigned long			backup_gsbase;
+
 #ifdef CONFIG_SMP
 	int				on_cpu;
 	struct __call_single_node	wake_entry;
diff --git a/init/init_task.c b/init/init_task.c
index 16d14c2eb..b375a55e6 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -75,6 +75,9 @@ struct task_struct init_task
 	.stack		= init_stack,
 	.usage		= REFCOUNT_INIT(2),
 	.flags		= PF_KTHREAD,
+	.is_enclave_thread	= 0,
+	.manager_entry	= NULL,
+	.secs		= NULL,
 	.prio		= MAX_PRIO - 20,
 	.static_prio	= MAX_PRIO - 20,
 	.normal_prio	= MAX_PRIO - 20,
diff --git a/kernel/Makefile b/kernel/Makefile
index 88b60a6e5..79021eaa4 100644
--- a/kernel/Makefile
+++ b/kernel/Makefile
@@ -48,6 +48,8 @@ obj-y += livepatch/
 obj-y += dma/
 obj-y += entry/
 
+obj-y += emusgx/
+
 obj-$(CONFIG_KCMP) += kcmp.o
 obj-$(CONFIG_FREEZER) += freezer.o
 obj-$(CONFIG_PROFILING) += profile.o
diff --git a/kernel/emusgx/Kconfig b/kernel/emusgx/Kconfig
new file mode 100644
index 000000000..e499fe601
--- /dev/null
+++ b/kernel/emusgx/Kconfig
@@ -0,0 +1,3 @@
+config EMUSGX
+	bool "Enable the Emusgx feature"
+	default n
\ No newline at end of file
diff --git a/kernel/emusgx/Makefile b/kernel/emusgx/Makefile
new file mode 100644
index 000000000..56985e542
--- /dev/null
+++ b/kernel/emusgx/Makefile
@@ -0,0 +1,3 @@
+obj-y += enclu.o fault.o cpusvn.o crypto.o encls_cross_vm.o \
+	irq.o management.o cr.o dispatcher.o emusgx_fetch.o encls_cross_vm.o \
+	entrance.o sender.o switchless_sync.o local_dispatcher.o aex.o
\ No newline at end of file
diff --git a/kernel/emusgx/aex.c b/kernel/emusgx/aex.c
new file mode 100644
index 000000000..01ea67063
--- /dev/null
+++ b/kernel/emusgx/aex.c
@@ -0,0 +1,159 @@
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/uaccess.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/ptrace.h>
+
+#include <emusgx/emusgx.h>
+#include <emusgx/emusgx_arch.h>
+#include <emusgx/emusgx_internal.h>
+#include <emusgx/emusgx_sender.h>
+#include <emusgx/emusgx_fault.h>
+#include <emusgx/emusgx_debug.h>
+
+#define VSGX_GP		0x0D
+#define VSGX_PF		0x0E
+
+
+/*
+ *  AEX Procedure
+ *
+ *  The AEX procedure of vSGX is elegant while still dirty. Unlike
+ *  SGX, we do not have to save the state. Instead, we sleep our thread,
+ *  just like what we've done with the EENTER on the guest VM side.
+ *
+ *  We send a package of synthetic state regs, error code, exception vector,
+ *  and on the guest VM side when the sleeping thread wakes up, it's already
+ *  in the UD trap handler so we do something dirty: we call the exception
+ *  handler inside the UD handler.
+ *
+ *  After that handler returns, the synthetic state is restored and we are done.
+ */
+void _enclave_local vsgx_aex_on_current_thread(struct pt_regs *ptrace_regs, uint8_t exception_code, uint32_t error_code, uint64_t cr2) {
+	struct emusgx_aex_package *package;
+	struct sgx_exinfo *exinfo;
+
+	// Make sure all pages are written back to the guest VM
+	emusgx_debug_print("EmuSGX: AEX... Now syncing all pages\n");
+	emusgx_sync_all_pages();
+	emusgx_debug_print("EmuSGX: Pages are synced\n");
+
+	package = kmalloc(sizeof(struct emusgx_eexit_package), GFP_KERNEL);
+
+	package->instr = EMUSGX_S_AEX;
+
+	// Save RIP for later use
+	// Don't have to
+
+	// Save all registers, When saving EFLAGS, the TF bit is set to 0 and
+	// the RF bit is set to what would have been saved on stack in the non-SGX case
+	// Don't have to
+
+	// Save FS and GS BASE into SSA using CR_GPR_PA
+	// Don't have to
+
+	// XSAVE
+	// Don't have to
+
+	// Apply synthetic state to GPRs, RFLAGS, extended features, etc.
+	// Send along with the exit package
+	package->regs.r15 = 0;
+	package->regs.r14 = 0;
+	package->regs.r13 = 0;
+	package->regs.r12 = 0;
+	package->regs.r11 = 0;
+	package->regs.r10 = 0;
+	package->regs.r9 = 0;
+	package->regs.r8 = 0;
+	package->regs.ax = 3;
+	package->regs.bx = (uint64_t)(current->tcs); // TCS pointer
+	__uaccess_begin();
+	package->regs.cx = current->tcs->aep; // Current AEP
+	package->regs.dx = 0;
+	package->regs.si = 0;
+	package->regs.di = 0;
+	// RSP/RBP are set on the other end
+	package->regs.sp = current->gpr->ursp;
+	package->regs.bp = current->gpr->urbp;
+	__uaccess_end();
+
+	// CF0, PF2, AF4, ZF6, SF7, OF11, RF16 are cleared
+	package->regs.flags = ptrace_regs->flags & 0xFFFFFFFFFFFEF72A;
+
+	// MMX and other registers are ignored
+
+	// Restore the FS and GS
+	// They are in the guest VM
+
+	// Examine exception code and update enclave internal states
+	// Please note that we only support #PF exception's AEX
+	// All other cases the enclave is crashed
+	if (exception_code != VSGX_PF) {
+		// Unexpected
+		pr_info("vSGX: Unexpected error code 0x%02X lead to AEX\n", exception_code);
+		emusgx_gp(0, ptrace_regs);
+	}
+
+	// IF (exception_code = (#DE OR #DB OR #BP OR #BR OR #UD OR #MF OR #AC OR #XM ))
+	//   NOT HANDLED
+	if (exception_code == VSGX_PF || exception_code == VSGX_GP) {
+		// Check SECS.MISCSELECT
+		if (current->secs->miscselect & 0x1) {
+			current->gpr->exitinfo.vector = exception_code;
+			current->gpr->exitinfo.exit_type = 3;
+			exinfo = (void *)(((uint64_t)current->gpr) - sizeof(struct sgx_exinfo));
+			if (exception_code == VSGX_PF) {
+				exinfo->maddr = cr2;
+				exinfo->errcd = error_code;
+				exinfo->reserved = 0;
+			}
+			else {
+				exinfo->maddr = 0;
+				exinfo->errcd = error_code;
+				exinfo->reserved = 0;
+			}
+			current->gpr->exitinfo.valid = 1;
+		}
+	}
+	// ELSE
+	//   NOT HANDLED
+
+	// Execution will resume at the AEP
+	package->regs.ip = current->tcs->aep; // Current AEP
+
+	// Set EAX to the ERESUME leaf index
+	// Done in synthetic state
+
+	// Put the TCS LA into RBX for later use by ERESUME
+	// Done in synthetic state
+
+	// Put the AEP into RCX for later use by ERESUME
+	// Done in synthetic state
+
+	// Update the SSA frame
+	__uaccess_begin();
+	current->tcs->cssa += 1;
+
+	// Set TCS to inactive
+	current->tcs->state = 0;
+	__uaccess_end();
+
+	// Restore XCR0
+	// Not needed
+	
+	// Clear low 12 bits of CR2
+	package->fault_addr = cr2 & 0xFFFFFFFFFFFFF000;
+
+	package->pid = current->emusgx_pid;
+	//package->tcs_pa = vsgx_vaddr_to_paddr(current->tcs, current->manager_entry->manager_nr);
+	package->exception_code = exception_code;
+	package->error_code = error_code;
+
+	// Send package
+	if (emusgx_send_data(package, sizeof(struct emusgx_eexit_package))) {
+		pr_info("EmuSGX: EEXIT failed to send data\n");
+	}
+
+	kfree(package);
+}
diff --git a/kernel/emusgx/cpusvn.c b/kernel/emusgx/cpusvn.c
new file mode 100644
index 000000000..a9daaac3c
--- /dev/null
+++ b/kernel/emusgx/cpusvn.c
@@ -0,0 +1,19 @@
+#include <linux/kernel.h>
+#include <emusgx/emusgx.h>
+
+// Our CPUSVN must be simulated
+uint64_t emusgx_cpusvn[2] = {0, 0};
+
+uint64_t *emusgx_get_cpusvn(void) {
+	return emusgx_cpusvn;
+}
+
+int emusgx_compare_cpusvn(const uint64_t *cpusvn1, const uint64_t *cpusvn2) {
+	// if 1 > 2
+	// return 1
+	// if 1 == 2
+	// return 0;
+	// if 1 < 2
+	// return -1
+	return 0;
+}
\ No newline at end of file
diff --git a/kernel/emusgx/cr.c b/kernel/emusgx/cr.c
new file mode 100644
index 000000000..ef05f4298
--- /dev/null
+++ b/kernel/emusgx/cr.c
@@ -0,0 +1,228 @@
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/slab.h>
+#include <linux/sched.h>
+#include <linux/spinlock.h>
+#include <linux/semaphore.h>
+
+#include <asm/ldt.h>
+#include <asm/uaccess.h>
+
+#include <emusgx/emusgx.h>
+#include <emusgx/emusgx_sender.h>
+#include <emusgx/emusgx_internal.h>
+#include <emusgx/emusgx_debug.h>
+
+static DEFINE_SPINLOCK(emusgx_user_manager_lock);
+
+uint64_t emusgx_csr_owner_epoch[2];
+uint64_t emusgx_cr_seal_fuses[2];
+uint64_t emusgx_cr_cpusvn[2];
+uint64_t emusgx_cr_report_keyid[4];
+uint32_t emusgx_csr_intelpubkeyhash[8];
+uint8_t  emusgx_cr_base_pk[16] = { 0 };
+
+uint64_t emusgx_epc_start = 0;
+uint64_t emusgx_epc_end = 0;
+
+atomic_t emusgx_cr_inited = (atomic_t)ATOMIC_INIT(0);
+
+// TODO: The 16 bytes key. In production environment
+// shall be set before deploy
+uint8_t *emusgx_internal_cr_cross_vm_key = "EmuSGX Cross VM";
+
+struct emusgx_user_space_manager_entry emusgx_user_space_manager[EMUSGX_MAXIMUM_ENCLAVES] = { [0 ... EMUSGX_MAXIMUM_ENCLAVES - 1] = { .status = EMUSGX_MGRSTAT_EMPTY, .local_dispatch_queue = NULL, .local_dispatch_queue_tail = NULL, .local_dispatch_queue_lock = __SPIN_LOCK_UNLOCKED(emusgx_local_dispatch_queue_lock) } };
+
+struct emusgx_user_space_manager_entry *emusgx_register_manager(pid_t current_pid) {
+	int i;
+	spin_lock(&emusgx_user_manager_lock);
+	for (i = 0; i < EMUSGX_MAXIMUM_ENCLAVES; i++) {
+		if (emusgx_user_space_manager[i].status == EMUSGX_MGRSTAT_EMPTY) {
+			emusgx_user_space_manager[i].status = EMUSGX_MGRSTAT_FREE;
+			break;
+		}
+	}
+	spin_unlock(&emusgx_user_manager_lock);
+
+	if (i == EMUSGX_MAXIMUM_ENCLAVES) {
+		return NULL;
+	}
+
+	emusgx_user_space_manager[i].manager_nr = i;
+	emusgx_debug_print("vSGX: Manager number is %d\n", i);
+	emusgx_user_space_manager[i].pid = current_pid;
+
+	emusgx_user_space_manager[i].action_semaphore = (struct semaphore)__SEMAPHORE_INITIALIZER(emusgx_user_space_manager[i].action_semaphore, 0);
+	emusgx_user_space_manager[i].action_avail_semaphore = (struct semaphore)__SEMAPHORE_INITIALIZER(emusgx_user_space_manager[i].action_avail_semaphore, 1);
+	emusgx_user_space_manager[i].local_dispatcher_semaphore = (struct semaphore)__SEMAPHORE_INITIALIZER(emusgx_user_space_manager[i].local_dispatcher_semaphore, 0);
+	return &(emusgx_user_space_manager[i]);
+}
+
+struct emusgx_user_space_manager_entry *emusgx_get_free_manager() {
+	int i;
+	struct emusgx_user_space_manager_entry *ret_val = NULL;
+	spin_lock(&emusgx_user_manager_lock);
+	for (i = 0; i < EMUSGX_MAXIMUM_ENCLAVES; i++) {
+		if (emusgx_user_space_manager[i].status == EMUSGX_MGRSTAT_FREE) {
+			emusgx_user_space_manager[i].status = EMUSGX_MGRSTAT_OCCUPIED;
+			ret_val = &(emusgx_user_space_manager[i]);
+			spin_unlock(&emusgx_user_manager_lock);
+			pr_info("vSGX: Manager %d is successfully registered\n", i);
+			return ret_val;
+		}
+	}
+	spin_unlock(&emusgx_user_manager_lock);
+
+	return NULL;
+}
+
+void emusgx_free_current_manager() {
+	spin_lock(&emusgx_user_manager_lock);
+	current->manager_entry->status = EMUSGX_MGRSTAT_FREE;
+	spin_unlock(&emusgx_user_manager_lock);
+}
+
+static int emusgx_find_enclave_with_page(void *epc_page) {
+	int manager_nr;
+
+	if (emusgx_get_epcm(epc_page) == NULL) {
+		// Not in range. Just let manager 0 handle it
+		return 0;
+	}
+	if (emusgx_get_epcm(epc_page)->valid == 0) {
+		// Not assigned to any manager. Just let manager 0 handle it
+		return 0;
+	}
+
+	if (emusgx_get_epcm(epc_page)->manager_entry == NULL) {
+		// Doing jobs on a verison array page
+		// Let manager 0 handle it
+		return 0;
+	}
+
+	manager_nr = (int)(emusgx_get_epcm(epc_page)->manager_entry->manager_nr);
+
+	return manager_nr;
+	
+}
+
+int vsgx_need_new_manager(void *package) {
+	uint8_t instr = *((uint8_t *)package);
+	struct emusgx_eldb_eldu_package *load_package = package;
+
+	if (instr == EMUSGX_S_ECREATE) {
+		return 1;
+	}
+	if (instr == EMUSGX_S_ELDB || instr == EMUSGX_S_ELDB) {
+		// Check if loading a SECS
+		if (load_package->pcmd.secinfo.flags.page_type == SGX_PT_SECS) {
+			return 1;
+		}
+	}
+
+	return 0;
+}
+
+int emusgx_find_enclave_with_package(void *package) {
+	// Find 
+	uint8_t instr = *((uint8_t *)package);
+	struct emusgx_page_package *switchless_package;
+	int manager_nr;
+
+	switch (instr) {
+	case EMUSGX_S_SWITCHLESS:
+		// Check who this page was assigned
+		switchless_package = package;
+		manager_nr = (int)switchless_package->id;
+		if (manager_nr < 0 || manager_nr >= EMUSGX_MAXIMUM_ENCLAVES) {
+			pr_info("vSGX: Switchless received a page whose manager_nr is invalid\n");
+			return -1;
+		}
+		if (emusgx_user_space_manager[manager_nr].status == EMUSGX_MGRSTAT_EMPTY) {
+			pr_info("vSGX: Switchless received a page whose manager does not exist yet\n");
+			return -1;
+		}
+
+		return manager_nr;
+	case EMUSGX_S_EENTER:
+		// Depends on TCS's SECS
+		// We have to use physical address
+		// We should also check if the TCS's vaddr actually maps to paddr
+		if (vsgx_confirm_mapping((void __user *)(((struct emusgx_eenter_package *)package)->tcs), (void *)(((struct emusgx_eenter_package *)package)->tcs_pa))) {
+			// Mapping inconsistent
+			// Reject
+			pr_info("vSGX: EENTER failed to find enclave due to inconsistent mapping of TCS to TCS's PA\n");
+			// Find a random enclave and let it complain it
+			return 0;
+		}
+		return emusgx_find_enclave_with_page((void *)(((struct emusgx_eenter_package *)package)->tcs_pa));
+	case EMUSGX_S_EADD:
+		return emusgx_find_enclave_with_page((void *)(((struct emusgx_eadd_package *)package)->secs));
+	case EMUSGX_S_EINIT:
+		return emusgx_find_enclave_with_page((void *)(((struct emusgx_einit_package *)package)->secs));
+	case EMUSGX_S_EREMOVE:
+		return emusgx_find_enclave_with_page((void *)(((struct emusgx_eremove_package *)package)->epc_page));
+	case EMUSGX_S_EEXTEND:
+		return emusgx_find_enclave_with_page((void *)(((struct emusgx_eextend_package *)package)->addr));
+	case EMUSGX_S_ELDB:
+	case EMUSGX_S_ELDU:
+		return emusgx_find_enclave_with_page((void *)(((struct emusgx_eldb_eldu_package *)package)->secs));
+	case EMUSGX_S_EBLOCK:
+		return emusgx_find_enclave_with_page((void *)(((struct emusgx_eblock_package *)package)->epc_page));
+	case EMUSGX_S_EPA:
+		// EPA adds an EPC page to the system rather than a specific process
+		// Just let a random manager 0  handle it
+		return 0;
+	case EMUSGX_S_EWB:
+		return emusgx_find_enclave_with_page((void *)(((struct emusgx_ewb_package *)package)->epc_page));
+	case EMUSGX_S_EAUG:
+		return emusgx_find_enclave_with_page((void *)(((struct emusgx_eaug_package *)package)->secs));
+	case EMUSGX_S_EMODPR:
+		return emusgx_find_enclave_with_page((void *)(((struct emusgx_emodpr_package *)package)->epc_page));
+	case EMUSGX_S_EMODT:
+		return emusgx_find_enclave_with_page((void *)(((struct emusgx_emodt_package *)package)->epc_page));
+	default:
+		// False package
+		pr_info("vSGX: Uknown package type %d\n", instr);
+		return -1;
+	}
+	
+}
+
+struct emusgx_user_space_manager_entry *emusgx_get_manager(int manager_nr) {
+	return &(emusgx_user_space_manager[manager_nr]);
+}
+
+void emusgx_queue_local_dispatch_request(void *package, uint64_t manager_nr) {
+	// This will queue the package to the corresponding manager's
+	// local dispatcher
+
+	struct emusgx_user_space_manager_entry *manager_entry = &(emusgx_user_space_manager[manager_nr]);
+	struct emusgx_local_dispatch_queue_node *new_node = kmalloc(sizeof(struct emusgx_local_dispatch_queue_node), GFP_KERNEL);
+
+	if (new_node == NULL) {
+		// Unexpected
+		pr_info("vSGX: Failed to create new dispatch request node\n");
+		// Free the package since no later processing will be done
+		kfree(package);
+		return;
+	}
+
+	new_node->package = package;
+	new_node->next = NULL;
+
+	spin_lock(&(manager_entry->local_dispatch_queue_lock));
+
+	// Be quick
+	if (manager_entry->local_dispatch_queue == NULL) {
+		manager_entry->local_dispatch_queue = new_node;
+	}
+	else {
+		manager_entry->local_dispatch_queue_tail->next = new_node;
+	}
+	manager_entry->local_dispatch_queue_tail = new_node;
+
+	spin_unlock(&(manager_entry->local_dispatch_queue_lock));
+
+	up(&(manager_entry->local_dispatcher_semaphore));
+}
\ No newline at end of file
diff --git a/kernel/emusgx/crypto.c b/kernel/emusgx/crypto.c
new file mode 100644
index 000000000..a145b8cef
--- /dev/null
+++ b/kernel/emusgx/crypto.c
@@ -0,0 +1,575 @@
+#include <linux/kernel.h>
+#include <linux/scatterlist.h>
+#include <linux/string.h>
+
+#include <crypto/hash.h>
+#include <crypto/sha.h>
+#include <crypto/aead.h>
+#include <crypto/public_key.h>
+
+#include <emusgx/emusgx.h>
+#include <emusgx/emusgx_internal.h>
+
+const uint8_t HARDCODED_PKCS1_5_PADDING[352] = {
+	0x00, 0x01, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+	0xFF, 0xFF, 0xFF, 0xFF, 0x00, 0x30, 0x31, 0x30,
+	0x0D, 0x06, 0x09, 0x60, 0x86, 0x48, 0x01, 0x65,
+	0x03, 0x04, 0x02, 0x01, 0x05, 0x00, 0x04, 0x20
+};
+
+// Returns a 128-bit secret key
+// The user is responsible for freeing the key
+uint64_t *emusgx_derive_key(const struct emusgx_tmp_key_dependencies *key_dependencies) {
+	uint64_t *key;
+	uint32_t *hash;
+	uint64_t *data;
+	uint64_t data_size;
+
+	// Use processor key emusgx_cr_base_pk to derive
+	data_size = sizeof(struct emusgx_tmp_key_dependencies) + 16;
+	data = kmalloc(data_size, GFP_KERNEL);
+	if (data == NULL) {
+		return NULL;
+	}
+	memcpy(data, key_dependencies, sizeof(struct emusgx_tmp_key_dependencies));
+	memcpy(data + sizeof(struct emusgx_tmp_key_dependencies), emusgx_cr_base_pk, 16);
+
+	// Get hash
+	hash = emusgx_sha256(data, data_size);
+	if (hash == NULL) {
+		return NULL;
+	}
+
+	key = kmalloc(16, GFP_KERNEL);
+	if (key == NULL) {
+		kfree(hash);
+		return NULL;
+	}
+	memcpy(key, hash, 16);
+	kfree(hash);
+
+	return key;
+}
+
+// Returns a 128-bit MAC
+// The user is responsible for freeing the MAC
+uint64_t *emusgx_cmac(const uint64_t *key, const void *data, const size_t size) {
+	uint64_t *cmac = NULL;
+
+	struct scatterlist plain_list[2];
+	struct scatterlist cipher_list[3];
+
+	struct crypto_aead *tfm = NULL;
+        struct aead_request *req;
+	char *algo = "gcm(aes)";
+
+	char *iv;
+	uint64_t ivsize;
+
+	void *cipher_text_page;
+	void *plain_text_page;
+	void *mac_page;
+
+	size_t plain_size = size;
+
+	if (plain_size > 4096) {
+		pr_info("EmuSGX: Cannot do GCM(AES) for data more than a page\n");
+		return NULL;
+	}
+
+	// Everything must be reallocated into pages
+	cipher_text_page = (void *)get_zeroed_page(GFP_KERNEL);
+	plain_text_page = (void *)get_zeroed_page(GFP_KERNEL);
+	mac_page = (void *)get_zeroed_page(GFP_KERNEL);
+
+	memcpy(plain_text_page, data, plain_size);
+
+	// Allocate a cipher handle for AEAD
+	tfm = crypto_alloc_aead(algo, 0, 0);
+	if (IS_ERR(tfm)) {
+		printk(KERN_INFO "EmuSGX: Failed to create cipher handle\n");
+		goto out1;
+	}
+
+	// Allocate AEAD request
+	req = aead_request_alloc(tfm, GFP_KERNEL);
+	if (req == NULL) {
+		printk(KERN_INFO "EmuSGX: Failed to create aead request\n");
+		goto out2;
+	}
+
+	// Synchronized
+	crypto_aead_clear_flags(tfm, ~0);
+
+	// Set key
+	if (crypto_aead_setkey(tfm, (const char *)key, 16) != 0) {
+		printk(KERN_INFO "EmuSGX: key could not be set\n");
+		goto out3;
+	}
+
+	// Set tag size (CMAC)
+	if (crypto_aead_setauthsize(tfm, 16)) {
+		pr_info("EmuSGX: Failed to set tag size\n");
+		goto out3;
+	}
+
+	// Get iv size
+	ivsize = crypto_aead_ivsize(tfm);
+	if (!(ivsize)){
+		pr_info("IV size could not be authenticated\n");
+		goto out3;
+	}
+
+	// Set initialization vector
+	iv = kmalloc(ivsize, GFP_KERNEL);
+	// iv shall be 12 bytes in gcm(aes), but our counter is 8 bytes
+	// so we leave other bits 0
+	memset(iv, 0, ivsize);
+
+	sg_init_table(plain_list, 1);
+	sg_set_buf(&plain_list[0], plain_text_page, plain_size);
+
+	sg_init_table(cipher_list, 2);
+	sg_set_buf(&cipher_list[0], cipher_text_page, plain_size);
+	sg_set_buf(&cipher_list[1], mac_page, 16);
+
+	// For unknown reason, the encrypt size does not need to add MAC
+	aead_request_set_crypt(req, plain_list, cipher_list, plain_size, iv);
+	aead_request_set_ad(req, 0);
+
+	
+	if (crypto_aead_encrypt(req)) {
+		pr_info("EmuSGX: Decrypt failed due to some weird reason\n");
+	}
+	else {
+		cmac = kmalloc(16, GFP_KERNEL);
+		memcpy(cmac, mac_page, 16);
+	}
+
+	// Freeup things
+	kfree(iv);
+out3:
+	aead_request_free(req);
+out2:
+	crypto_free_aead(tfm);
+out1:
+	free_page((uint64_t)plain_text_page);
+	free_page((uint64_t)cipher_text_page);
+	free_page((uint64_t)mac_page);
+
+	return cmac;
+}
+
+// Initialize a 256-bit SHA hash
+void emusgx_sha256init(uint32_t *original) {
+	struct sha256_state sctx;
+	sha256_init(&sctx);
+	__uaccess_begin();
+	original[0] = sctx.state[0];
+	original[1] = sctx.state[1];
+	original[2] = sctx.state[2];
+	original[3] = sctx.state[3];
+	original[4] = sctx.state[4];
+	original[5] = sctx.state[5];
+	original[6] = sctx.state[6];
+	original[7] = sctx.state[7];
+	__uaccess_end();
+}
+
+// Update a 256-bit SHA hash
+void emusgx_sha256update(uint32_t *original, uint64_t *data) {
+	struct sha256_state sctx;
+	__uaccess_begin();
+	sctx.state[0] = original[0];
+	sctx.state[1] = original[1];
+	sctx.state[2] = original[2];
+	sctx.state[3] = original[3];
+	sctx.state[4] = original[4];
+	sctx.state[5] = original[5];
+	sctx.state[6] = original[6];
+	sctx.state[7] = original[7];
+	__uaccess_end();
+	sctx.count = 0; // We do 64 bytes each time so we don't need the count option
+	sha256_update(&sctx, (uint8_t *)data, 64);
+	__uaccess_begin();
+	original[0] = sctx.state[0];
+	original[1] = sctx.state[1];
+	original[2] = sctx.state[2];
+	original[3] = sctx.state[3];
+	original[4] = sctx.state[4];
+	original[5] = sctx.state[5];
+	original[6] = sctx.state[6];
+	original[7] = sctx.state[7];
+	__uaccess_end();
+}
+
+// Finalize a 256-bit SHA hash
+void emusgx_sha256final(uint32_t *original, uint64_t counter) {
+	struct sha256_state sctx;
+	uint8_t final_hash[32];
+	__uaccess_begin();
+	sctx.state[0] = original[0];
+	sctx.state[1] = original[1];
+	sctx.state[2] = original[2];
+	sctx.state[3] = original[3];
+	sctx.state[4] = original[4];
+	sctx.state[5] = original[5];
+	sctx.state[6] = original[6];
+	sctx.state[7] = original[7];
+	__uaccess_end();
+	sctx.count = counter * 64; // We do 64 bytes each time
+	sha256_final(&sctx, final_hash);
+	__uaccess_begin();
+	memcpy((void *)original, (void *)final_hash, 32);
+	__uaccess_end();
+}
+
+uint32_t *emusgx_sha256(uint64_t *data, size_t len) {
+	uint32_t *ret_hash;
+	struct sha256_state sctx;
+
+	sha256_init(&sctx);
+	sha256_update(&sctx, (uint8_t *)data, len);
+	ret_hash = kmalloc(sizeof(uint32_t) * 8, GFP_KERNEL);
+	sha256_final(&sctx, (uint8_t *)ret_hash);
+
+	return ret_hash;
+}
+
+uint8_t emusgx_verify_sigstruct(struct sgx_sigstruct *sig) {
+	struct sgx_sigstruct hash_sig;
+	int ret_val, i;
+
+	struct public_key_signature pkey_sig;
+
+
+	// SHA1
+	struct shash_desc sha1_desc;
+	uint8_t hashval[20];
+
+	memcpy(&hash_sig, sig, sizeof(struct sgx_sigstruct));
+
+	// Set unhashed fields to zero
+	memset(&(hash_sig.modulus), 0, sizeof(hash_sig.modulus));
+	memset(&(hash_sig.exponent), 0, sizeof(hash_sig.exponent));
+	memset(&(hash_sig.signature), 0, sizeof(hash_sig.signature));
+	memset(&(hash_sig.q1), 0, sizeof(hash_sig.q1));
+	memset(&(hash_sig.q2), 0, sizeof(hash_sig.q2));
+
+	// RFC 3447 signature uses SHA1
+	sha1_desc.tfm = crypto_alloc_shash("sha1", 0, 0);
+
+	if (crypto_shash_digest(&sha1_desc, (uint8_t *)(&hash_sig), sizeof(struct sgx_sigstruct), hashval) != 0) {
+		pr_info("EmuSGX: Failed to calculate SHA1\n");
+		crypto_free_shash(sha1_desc.tfm);
+		return -1;
+	}
+
+	crypto_free_shash(sha1_desc.tfm);
+
+	pkey_sig.s = sig->signature;
+	pkey_sig.s_size = 384;
+	pkey_sig.digest = hashval;
+	pkey_sig.digest_size = 20;
+	pkey_sig.pkey_algo = "rsa";
+	pkey_sig.hash_algo = "sha1";
+
+	ret_val = emusgx_rsa_public_key_verify_signature(sig->modulus, 384, (uint8_t *)(&sig->exponent), 4, &pkey_sig);
+	
+	if (ret_val != 0) {
+		if (ret_val == -EKEYREJECTED) {
+			pr_info("EmuSGX: Signature rejected\n");
+			pr_info("EmuSGX: Digest is\n");
+			for (i = 0; i < 4; i++) {
+				pr_info("EmuSGX: %02X %02X %02X %02X %02X\n", hashval[5 * i], hashval[5 * i + 1], hashval[5 * i + 2], hashval[5 * i + 3], hashval[5 * i + 4]);
+			}
+			return -1;
+		}
+		else {
+			pr_info("EmuSGX: Unexpected error happened during signature verification\n");
+			return -2;
+		}
+	}
+
+	return 0;
+}
+int emusgx_aes_128_gcm_dec(uint8_t *key, uint64_t *counter, void *aad, size_t aad_size, 
+				void *cipher_text, size_t cipher_size, void *plain_text, uint64_t *mac) {
+	struct scatterlist plain_list[2];
+	struct scatterlist cipher_list[3];
+
+	struct crypto_aead *tfm = NULL;
+        struct aead_request *req;
+	char *algo = "gcm(aes)";
+	int ret;
+
+	char *iv;
+	uint64_t ivsize;
+
+	void *aad_page;
+	void *cipher_text_page;
+	void *plain_text_page;
+	void *mac_page;
+
+	if (cipher_size > 4096) {
+		pr_info("EmuSGX: Cannot do GCM(AES) for data more than a page\n");
+		return -1;
+	}
+
+	// Everything must be reallocated into pages
+	aad_page = (void *)get_zeroed_page(GFP_KERNEL);
+	cipher_text_page = (void *)get_zeroed_page(GFP_KERNEL);
+	plain_text_page = (void *)get_zeroed_page(GFP_KERNEL);
+	mac_page = (void *)get_zeroed_page(GFP_KERNEL);
+
+	memcpy(aad_page, aad, aad_size);
+	memcpy(cipher_text_page, cipher_text, cipher_size);
+	memcpy(mac_page, mac, 16);
+
+	// Allocate a cipher handle for AEAD
+	tfm = crypto_alloc_aead(algo, 0, 0);
+	if (IS_ERR(tfm)) {
+		printk(KERN_INFO "EmuSGX: Failed to create cipher handle\n");
+		ret = -1;
+		goto out1;
+	}
+
+	// Allocate AEAD request
+	req = aead_request_alloc(tfm, GFP_KERNEL);
+	if (req == NULL) {
+		printk(KERN_INFO "EmuSGX: Failed to create aead request\n");
+		ret = -1;
+		goto out2;
+	}
+
+	// Synchronized
+	crypto_aead_clear_flags(tfm, ~0);
+
+	// Set key
+	if ((ret = crypto_aead_setkey(tfm, key, 16) != 0)) {
+		printk(KERN_INFO "EmuSGX: Return value for setkey is %d\n", ret);
+		printk(KERN_INFO "EmuSGX: key could not be set\n");
+		ret = -1;
+		goto out3;
+	}
+
+	// Set tag size (CMAC)
+	if (crypto_aead_setauthsize(tfm, 16)) {
+		pr_info("EmuSGX: Failed to set tag size\n");
+		ret = -1;
+		goto out3;
+	}
+
+	// Get iv size
+	ivsize = crypto_aead_ivsize(tfm);
+	if (!(ivsize)){
+		pr_info("IV size could not be authenticated\n");
+		ret = -1;
+		goto out3;
+	}
+
+	// Set initialization vector
+	iv = kmalloc(ivsize, GFP_KERNEL);
+	// iv shall be 12 bytes in gcm(aes), but our counter is 8 bytes
+	// so we leave other bits 0
+	memset(iv, 0, ivsize);
+	memcpy(iv, counter, 8);
+
+	sg_init_table(plain_list, 2);
+	sg_set_buf(&plain_list[0], aad_page, aad_size);
+	sg_set_buf(&plain_list[1], plain_text_page, cipher_size);
+
+	sg_init_table(cipher_list, 3);
+	sg_set_buf(&cipher_list[0], aad_page, aad_size);
+	sg_set_buf(&cipher_list[1], cipher_text_page, cipher_size);
+	sg_set_buf(&cipher_list[2], mac_page, 16);
+
+	// For unknown reason, the decrypt size needs to add MAC
+	aead_request_set_crypt(req, cipher_list, plain_list, cipher_size + 16, iv);
+	aead_request_set_ad(req, aad_size);
+
+	ret = crypto_aead_decrypt(req);
+	if (ret != 0) {
+		if (ret == -EBADMSG) {
+			pr_info("EmuSGX: Decrypt failed due to MAC mismatch\n");
+		}
+		else {
+			pr_info("EmuSGX: Decrypt failed due to some wired reason\n");
+		}
+	}
+	else {
+		memcpy(plain_text, plain_text_page, cipher_size);
+	}
+
+	// Freeup things
+	kfree(iv);
+out3:
+	aead_request_free(req);
+out2:
+	crypto_free_aead(tfm);
+out1:
+	free_page((uint64_t)aad_page);
+	free_page((uint64_t)plain_text_page);
+	free_page((uint64_t)cipher_text_page);
+	free_page((uint64_t)mac_page);
+
+	return ret;
+}
+
+int emusgx_aes_128_gcm_enc(uint8_t *key, uint64_t *counter, void *aad, size_t aad_size, 
+				void *plain_text, size_t plain_size, void *cipher_text, uint64_t *mac) {
+	struct scatterlist plain_list[2];
+	struct scatterlist cipher_list[3];
+
+	struct crypto_aead *tfm = NULL;
+        struct aead_request *req;
+	char *algo = "gcm(aes)";
+	int ret;
+
+	char *iv;
+	uint64_t ivsize;
+
+	void *aad_page;
+	void *cipher_text_page;
+	void *plain_text_page;
+	void *mac_page;
+
+	if (plain_size > 4096) {
+		pr_info("EmuSGX: Cannot do GCM(AES) for data more than a page\n");
+		return -1;
+	}
+
+	// Everything must be reallocated into pages
+	aad_page = (void *)get_zeroed_page(GFP_KERNEL);
+	cipher_text_page = (void *)get_zeroed_page(GFP_KERNEL);
+	plain_text_page = (void *)get_zeroed_page(GFP_KERNEL);
+	mac_page = (void *)get_zeroed_page(GFP_KERNEL);
+
+	memcpy(aad_page, aad, aad_size);
+	memcpy(plain_text_page, plain_text, plain_size);
+
+	// Allocate a cipher handle for AEAD
+	tfm = crypto_alloc_aead(algo, 0, 0);
+	if (IS_ERR(tfm)) {
+		printk(KERN_INFO "EmuSGX: Failed to create cipher handle\n");
+		ret = -1;
+		goto out1;
+	}
+
+	// Allocate AEAD request
+	req = aead_request_alloc(tfm, GFP_KERNEL);
+	if (req == NULL) {
+		printk(KERN_INFO "EmuSGX: Failed to create aead request\n");
+		ret = -1;
+		goto out2;
+	}
+
+	// Synchronized
+	crypto_aead_clear_flags(tfm, ~0);
+
+	// Set key
+	if ((ret = crypto_aead_setkey(tfm, key, 16) != 0)) {
+		printk(KERN_INFO "EmuSGX: Return value for setkey is %d\n", ret);
+		printk(KERN_INFO "EmuSGX: key could not be set\n");
+		ret = -1;
+		goto out3;
+	}
+
+	// Set tag size (CMAC)
+	if (crypto_aead_setauthsize(tfm, 16)) {
+		pr_info("EmuSGX: Failed to set tag size\n");
+		ret = -1;
+		goto out3;
+	}
+
+	// Get iv size
+	ivsize = crypto_aead_ivsize(tfm);
+	if (!(ivsize)){
+		pr_info("IV size could not be authenticated\n");
+		ret = -1;
+		goto out3;
+	}
+
+	// Set initialization vector
+	iv = kmalloc(ivsize, GFP_KERNEL);
+	// iv shall be 12 bytes in gcm(aes), but our counter is 8 bytes
+	// so we leave other bits 0
+	memset(iv, 0, ivsize);
+	memcpy(iv, counter, 8);
+
+	sg_init_table(plain_list, 2);
+	sg_set_buf(&plain_list[0], aad_page, aad_size);
+	sg_set_buf(&plain_list[1], plain_text_page, plain_size);
+
+	sg_init_table(cipher_list, 3);
+	sg_set_buf(&cipher_list[0], aad_page, aad_size);
+	sg_set_buf(&cipher_list[1], cipher_text_page, plain_size);
+	sg_set_buf(&cipher_list[2], mac_page, 16);
+
+	// For unknown reason, the encrypt size does not need to add MAC
+	aead_request_set_crypt(req, plain_list, cipher_list, plain_size, iv);
+	aead_request_set_ad(req, aad_size);
+
+	ret = crypto_aead_encrypt(req);
+	if (ret != 0) {
+		pr_info("EmuSGX: Decrypt failed due to some weird reason\n");
+	}
+	else {
+		memcpy(cipher_text, cipher_text_page, plain_size);
+		memcpy(mac, mac_page, 16);
+	}
+
+	// Freeup things
+	kfree(iv);
+out3:
+	aead_request_free(req);
+out2:
+	crypto_free_aead(tfm);
+out1:
+	free_page((uint64_t)aad_page);
+	free_page((uint64_t)plain_text_page);
+	free_page((uint64_t)cipher_text_page);
+	free_page((uint64_t)mac_page);
+
+	return ret;
+}
diff --git a/kernel/emusgx/dispatcher.c b/kernel/emusgx/dispatcher.c
new file mode 100644
index 000000000..bdb464b21
--- /dev/null
+++ b/kernel/emusgx/dispatcher.c
@@ -0,0 +1,866 @@
+#include <linux/kernel.h>
+#include <linux/interrupt.h>
+#include <linux/spinlock.h>
+#include <linux/semaphore.h>
+#include <linux/slab.h>
+#include <linux/ktime.h>
+
+#include <linux/uaccess.h>
+
+#include <emusgx/emusgx_internal.h>
+#include <emusgx/emusgx_sender.h>
+#include <emusgx/emusgx_debug.h>
+
+struct emusgx_request_queue_node *emusgx_request_queue = NULL;
+struct emusgx_request_queue_node *emusgx_request_queue_tail = NULL;
+DEFINE_SPINLOCK(emusgx_dispatcher_queue_lock);
+struct semaphore emusgx_dispatcher_sem = __SEMAPHORE_INITIALIZER(emusgx_dispatcher_sem, 0);
+
+struct emusgx_page_fault_queue_node *emusgx_pf_queue = NULL;
+struct emusgx_page_fault_queue_node *emusgx_pf_queue_tail = NULL;
+DEFINE_SPINLOCK(emusgx_pf_queue_lock);
+
+struct emusgx_aex_queue_node *emusgx_aex_queue = NULL;
+struct emusgx_aex_queue_node *emusgx_aex_queue_tail = NULL;
+DEFINE_SPINLOCK(emusgx_aex_queue_lock);
+
+struct emusgx_dispatch_slot emusgx_switchless_sync_slot[EMUSGX_MAXIMUM_ENCLAVES][EMUSGX_SWITCHLESS_SLOT_COUNT] = { [0 ... EMUSGX_MAXIMUM_ENCLAVES - 1] = { [0 ... EMUSGX_SWITCHLESS_SLOT_COUNT - 1] = {.status = EMUSGX_DISPATCH_SLOT_FREE} } };
+
+struct emusgx_dispatch_slot emusgx_instruction_emulation_slot = {.status = EMUSGX_DISPATCH_SLOT_FREE};
+
+static uint64_t emusgx_used_session_number[EMUSGX_MAX_SESSION_NUMBER_D64] = { [0 ... EMUSGX_MAX_SESSION_NUMBER_D64 - 1] = (uint64_t)0 }; // Maximum session number = 4095, session number ranged from [0, 4095]
+
+static uint64_t stamps_waked[20];
+uint64_t stamps_dispatched[20];
+static uint32_t stamp_i = 0;
+
+/*
+void emusgx_print_page(void *page) {
+	uint64_t *ptr = page;
+	int i;
+	for (i = 0; i < 512; i++) {
+		pr_info("EmuSGX: %016llX\n", ptr[i]);
+	}
+}
+*/
+void emusgx_remove_pf_node(struct emusgx_page_fault_queue_node *node) {
+	struct emusgx_page_fault_queue_node *current_node;
+	struct emusgx_page_fault_queue_node *prev_node = NULL;
+
+	// Sleepable
+	spin_lock(&emusgx_pf_queue_lock);
+
+	current_node = emusgx_pf_queue;
+	while (current_node != NULL) {
+		if (current_node == node) {
+			if (prev_node == NULL) {
+				// First node
+				emusgx_pf_queue = current_node->next;
+				if (emusgx_pf_queue == NULL) {
+					emusgx_pf_queue_tail = NULL;
+				}
+			}
+			else {
+				prev_node->next = current_node->next;
+				if (prev_node->next == NULL) {
+					// Last node
+					emusgx_pf_queue_tail = prev_node;
+				}
+			}
+			spin_unlock(&emusgx_pf_queue_lock);
+			return;
+		}
+		prev_node = current_node;
+		current_node = current_node->next;
+	}
+
+	spin_unlock(&emusgx_pf_queue_lock);
+}
+
+struct emusgx_page_fault_queue_node _enclave_local *emusgx_register_pf_request(void *addr, uint8_t *is_main) {
+	struct emusgx_page_fault_queue_node *current_node;
+	uint64_t manager_nr = current->manager_entry->manager_nr;
+
+	// Sleepable
+	spin_lock(&emusgx_pf_queue_lock);
+
+	current_node = emusgx_pf_queue;
+	while (current_node != NULL) {
+		if (current_node->addr == addr && current_node->manager_nr == manager_nr) {
+			break;
+		}
+		current_node = current_node->next;
+	}
+
+	if (current_node != NULL) {
+		atomic_inc(&current_node->waiting_nr);
+		*is_main = 0;
+	}
+	else {
+		current_node = kmalloc(sizeof(struct emusgx_page_fault_queue_node), GFP_KERNEL);
+		current_node->slot = kmalloc(sizeof(struct emusgx_dispatch_slot), GFP_KERNEL);
+		current_node->slot->status = EMUSGX_DISPATCH_SLOT_FREE;
+		current_node->node_semaphore = kmalloc(sizeof(struct semaphore), GFP_KERNEL);
+		*(current_node->node_semaphore) = (struct semaphore)__SEMAPHORE_INITIALIZER(*(current_node->node_semaphore), 0);
+		current_node->addr = addr;
+		current_node->waiting_nr = (atomic_t)ATOMIC_INIT(0);
+		current_node->manager_nr = manager_nr;
+		current_node->slot->data = kmalloc(sizeof(struct emusgx_page_package), GFP_KERNEL);
+		current_node->other_waiting_semaphore = (struct semaphore)__SEMAPHORE_INITIALIZER(current_node->other_waiting_semaphore, 0);
+		current_node->next = NULL;
+		if (emusgx_pf_queue_tail == NULL) {
+			emusgx_pf_queue = emusgx_pf_queue_tail = current_node;
+		}
+		else {
+			emusgx_pf_queue_tail->next = current_node;
+			emusgx_pf_queue_tail = current_node;
+		}
+		*is_main = 1;
+	}
+
+	spin_unlock(&emusgx_pf_queue_lock);
+
+	return current_node;
+}
+
+struct emusgx_page_fault_queue_node *emusgx_find_pf_node_with_session_number(uint64_t session_number) {
+	struct emusgx_page_fault_queue_node *current_node;
+
+	// Sleepable
+	spin_lock(&emusgx_pf_queue_lock);
+
+	current_node = emusgx_pf_queue;
+	while (current_node != NULL) {
+		if (current_node->slot->session_number == session_number) {
+			spin_unlock(&emusgx_pf_queue_lock);
+			return current_node;
+		}
+		current_node = current_node->next;
+	}
+
+	spin_unlock(&emusgx_pf_queue_lock);
+
+	return NULL;
+}
+
+struct emusgx_page_fault_queue_node *emusgx_find_pf_node(struct semaphore *sem) {
+	struct emusgx_page_fault_queue_node *current_node;
+
+	// Sleepable
+	spin_lock(&emusgx_pf_queue_lock);
+
+	current_node = emusgx_pf_queue;
+	while (current_node != NULL) {
+		if (current_node->node_semaphore == sem) {
+			spin_unlock(&emusgx_pf_queue_lock);
+			return current_node;
+		}
+		current_node = current_node->next;
+	}
+
+	spin_unlock(&emusgx_pf_queue_lock);
+
+	return NULL;
+}
+
+uint8_t emusgx_register_aex_request(uint64_t tcs_pa) {
+	struct emusgx_aex_queue_node *current_node;
+
+	// Sleepable
+	spin_lock(&emusgx_aex_queue_lock);
+
+	// We don't have to check previously saved AEX requests since in Intel's
+	// reference there's no documentation suggesting that you cannot reenter
+	// a thread in AEX
+	// Intel manages AEX using SSA stack and CSSA as stack pointer so ERESUME
+	// executes in a FILO manner
+	// To do this we register and dequeue using the same FILO manner
+
+	
+	current_node = kmalloc(sizeof(struct emusgx_aex_queue_node), GFP_KERNEL);
+	current_node->slot = kmalloc(sizeof(struct emusgx_dispatch_slot), GFP_KERNEL);
+	current_node->slot->status = EMUSGX_DISPATCH_SLOT_FREE;
+	current_node->semaphore = (struct semaphore)__SEMAPHORE_INITIALIZER(current_node->semaphore, 0);
+	current_node->tcs_pa = tcs_pa;
+	current_node->next = NULL;
+	if (emusgx_aex_queue == NULL) {
+		emusgx_aex_queue = emusgx_aex_queue_tail = current_node;
+	}
+	else {
+		// Push to the front
+		current_node->next = emusgx_aex_queue;
+		emusgx_aex_queue = current_node;
+	}
+	
+
+	spin_unlock(&emusgx_aex_queue_lock);
+
+	return 0;
+}
+
+uint8_t emusgx_wait_for_aex_request(uint64_t tcs_pa, struct pt_regs *regs) {
+	struct emusgx_aex_queue_node *current_node;
+	struct emusgx_aex_queue_node *previous_node = NULL;
+	struct emusgx_eresume_package *package;
+	struct emusgx_raw_response eresume_response;
+	void *pf_addr;
+	//uint8_t instr;
+	uint8_t resume_ret;
+
+	// Sleepable
+	spin_lock(&emusgx_aex_queue_lock);
+
+	// Search for node
+	// Search and pop from the front
+	current_node = emusgx_aex_queue;
+	while (current_node != NULL) {
+		if (current_node->tcs_pa == tcs_pa) {
+			break;
+		}
+		previous_node = current_node;
+		current_node = current_node->next;
+	}
+
+	// If not found, unexpected
+	if (current_node == NULL) {
+		spin_unlock(&emusgx_aex_queue_lock);
+		pr_info("EmuSGX: Unexpected. You have not been registered. TCS_PA = %lld\n", tcs_pa);
+		return -1;
+	}
+
+	spin_unlock(&emusgx_aex_queue_lock);
+
+	// Now wait for the response
+	if (down_interruptible(&current_node->semaphore)) {
+		pr_info("EmuSGX: Some one tries to kill me. TCS_PA = %lld\n", tcs_pa);
+		kfree(current_node->slot->data);
+		kfree(current_node->slot);
+		kfree(current_node);
+		return -1;
+	}
+
+	// Must be ERESUME
+	// Dequeue and handle
+
+	spin_lock(&emusgx_aex_queue_lock);
+
+	// Clear up everything
+
+	if (current_node == emusgx_aex_queue) {
+		// First node
+		if (current_node->next == NULL) {
+			// The only node
+			emusgx_aex_queue = NULL;
+			emusgx_aex_queue_tail = NULL;
+		}
+		else {
+			emusgx_aex_queue = current_node->next;
+		}
+	}
+	else {
+		previous_node->next = current_node->next;
+		if (previous_node->next == NULL) {
+			// Last node
+			emusgx_aex_queue_tail = previous_node;
+		}
+	}
+
+	package = current_node->slot->data;
+
+	// Call ERESUME handler
+	resume_ret = vsgx_resume_enclave((void *)package->tcs, package->tcs_pa, package->aep, package->pid, &pf_addr);
+	eresume_response.instr = EMUSGX_S_ERESUME;
+	eresume_response.response = resume_ret;
+	eresume_response.write_back = 0;
+	if (resume_ret == 1) {
+		// PF on TCS
+		eresume_response.linaddr = (uint64_t)package->tcs;
+	}
+	else if (resume_ret == 2) {
+		// PF on GPR
+		eresume_response.linaddr = (uint64_t)pf_addr;
+	}
+
+	// Send package
+	if (emusgx_send_data(&eresume_response, sizeof(struct emusgx_raw_response))) {
+		pr_info("vSGX: Failed to response ERESUME\n");
+	}
+
+	kfree(current_node->slot->data);
+	kfree(current_node->slot);
+	kfree(current_node);
+
+	spin_unlock(&emusgx_aex_queue_lock);
+
+	return 0;
+}
+
+struct emusgx_aex_queue_node *emusgx_find_aex_node(uint64_t tcs_pa) {
+	struct emusgx_aex_queue_node *current_node;
+
+	// Sleepable
+	spin_lock(&emusgx_aex_queue_lock);
+
+	current_node = emusgx_aex_queue;
+	while (current_node != NULL) {
+		if (current_node->tcs_pa == tcs_pa) {
+			spin_unlock(&emusgx_aex_queue_lock);
+			return current_node;
+		}
+		current_node = current_node->next;
+	}
+
+	spin_unlock(&emusgx_aex_queue_lock);
+
+	return NULL;
+}
+
+struct emusgx_aex_queue_node *emusgx_find_aex_node_with_session_number(uint64_t session_number) {
+	struct emusgx_aex_queue_node *current_node;
+
+	// Sleepable
+	spin_lock(&emusgx_aex_queue_lock);
+
+	current_node = emusgx_aex_queue;
+	while (current_node != NULL) {
+		if (current_node->slot->session_number == session_number) {
+			spin_unlock(&emusgx_aex_queue_lock);
+			return current_node;
+		}
+		current_node = current_node->next;
+	}
+
+	spin_unlock(&emusgx_aex_queue_lock);
+
+	return NULL;
+}
+
+struct emusgx_dispatch_slot *emusgx_get_slot_with_session_number(uint64_t session_number) {
+	struct emusgx_page_fault_queue_node *pf_node;
+	struct emusgx_aex_queue_node *aex_node;
+	int i, j;
+
+	// First, the encls slot
+	if (emusgx_instruction_emulation_slot.status == EMUSGX_DISPATCH_SLOT_INUSE) {
+		if (emusgx_instruction_emulation_slot.session_number == session_number) {
+			return &(emusgx_instruction_emulation_slot);
+		}
+	}
+
+	// Next, page fault list
+	pf_node = emusgx_find_pf_node_with_session_number(session_number);
+	if (pf_node != NULL) {
+		// Found node
+		// The slot in the queue will always be INUSE
+		return pf_node->slot;
+	}
+
+	// Next, AEX list
+	aex_node = emusgx_find_aex_node_with_session_number(session_number);
+	if (aex_node != NULL) {
+		// Found node
+		// The slot in the queue will always be INUSE
+		return aex_node->slot;
+	}
+
+	// Finally we check switchless sync list
+	for (j = 0; j < EMUSGX_MAXIMUM_ENCLAVES; j++) {
+		for (i = 0; i < EMUSGX_SWITCHLESS_SLOT_COUNT; i++) {
+			if (emusgx_switchless_sync_slot[j][i].session_number == session_number) {
+				// Must be INUSE state
+				if (emusgx_switchless_sync_slot[j][i].status == EMUSGX_DISPATCH_SLOT_INUSE)
+					return &(emusgx_switchless_sync_slot[j][i]);
+			}
+		}
+	}
+
+	return NULL;
+}
+
+// return 0 for usable
+// 1 for unusable
+uint8_t emusgx_check_session_number_usable(uint64_t session_number) {
+	uint64_t group = session_number / 64;
+	uint64_t bit = session_number % 64;
+	// Check if the session number usable bit is 0
+	if (session_number >= EMUSGX_MAX_SESSION_NUMBER) {
+		pr_info("EmuSGX: Current session number is out of bound\n");
+		return 1;
+	}
+
+	return (emusgx_used_session_number[group] >> bit) & 1;
+}
+
+// return 0 for success
+// 1 for unusable
+uint8_t emusgx_register_session_number(uint64_t session_number) {
+	uint64_t group = session_number / 64;
+	uint64_t bit = session_number % 64;
+	int i;
+
+	// No session number collision should happen due to very high session number capacity
+	// So no lock is needed
+	// First check session number for safety
+	if (emusgx_check_session_number_usable(session_number)) {
+		return 1;
+	}
+
+	// Register the session number
+	emusgx_used_session_number[group] |=  ((uint64_t)1 << bit);
+
+	// Perform session number wrapping if the session number is comming to the upper limit
+	// Our overhead limit is 20 but it could be changed
+	if (EMUSGX_MAX_SESSION_NUMBER - session_number == 20) {
+		for (i = 0; i < EMUSGX_MAX_SESSION_NUMBER_D64; i++) {
+			emusgx_used_session_number[i] = 0;
+		}
+	}
+
+	return 0;
+}
+
+int emusgx_dispatcher(void *dummy) {
+	int decrypt_ret;
+	int slot_index;
+	uint8_t instr;
+	struct emusgx_request_queue_node *current_node;
+	struct emusgx_page_fault_queue_node *pf_node;
+	struct emusgx_aex_queue_node *aex_node;
+	struct emusgx_dispatch_slot *slot;
+	struct emusgx_user_space_manager_entry *manager_entry;
+	unsigned long flags;
+	uint64_t manager_nr;
+	struct emusgx_raw_response response;
+
+
+	uint64_t iv = 0;
+	
+	struct emusgx_cross_vm_package *cipher_package;
+	struct emusgx_cross_vm_package *plain_package;
+
+	// This process is KILLABLE not INTERRUPTABLE
+	// No one shall ever send any signal except for SIGKILL to this process
+	while (true) {
+		// Down dispatcher
+		emusgx_debug_print("EmuSGX: Dispatcher waiting for package\n");
+		if (down_killable(&(emusgx_dispatcher_sem))) {
+			// Some one wants to kill me
+			// Return
+			return -1;
+		}
+		if (stamp_i < 20)
+			stamps_waked[stamp_i] = ktime_get_real_ns();
+
+		// Get data
+		// The lock is held with IRQ save
+		emusgx_debug_print("EmuSGX: Dispatcher received package\n");
+		spin_lock_irqsave(&emusgx_dispatcher_queue_lock, flags);
+		emusgx_debug_print("EmuSGX: Dispatcher got the lock\n");
+
+		// Be QUICK
+		// Take out the first node and leave
+		current_node = emusgx_request_queue;
+
+		if (current_node == NULL) {
+			// Shit happens
+			pr_err("EmuSGX: Unexpected error in dispatch queue\n");
+			spin_unlock_irqrestore(&emusgx_dispatcher_queue_lock, flags);
+			continue;
+		}
+
+		emusgx_request_queue = current_node->next;
+		if (emusgx_request_queue == NULL) {
+			// Fist node
+			emusgx_request_queue_tail = NULL;
+		}
+
+		spin_unlock_irqrestore(&emusgx_dispatcher_queue_lock, flags);
+		emusgx_debug_print("EmuSGX: Dispatcher released the lock\n");
+
+		// Decrypt the data
+		cipher_package = current_node->page;
+		plain_package = kmalloc(sizeof(struct emusgx_cross_vm_package), GFP_KERNEL);
+
+		decrypt_ret = emusgx_aes_128_gcm_dec(emusgx_internal_cr_cross_vm_key, &iv, emusgx_static_aad, 16, cipher_package, 4096 - 16, plain_package, cipher_package->mac);
+		
+		if (decrypt_ret != 0) {
+			pr_info("EmuSGX: Unexpected decryption issue\n");
+			pr_info("EmuSGX: Signature: First: 0x%016llX. Last: 0x%016llX\n", *((uint64_t *)cipher_package), *((uint64_t *)((void *)cipher_package + 4096 - 8)));
+			// emusgx_print_page(cipher_package);
+			kfree(plain_package);
+			kfree(cipher_package);
+			kfree(current_node);
+			// May be false package
+			// Leave it and continue
+			continue;
+		}
+
+		kfree(cipher_package);
+		kfree(current_node);
+
+		// Now we have the package, to prevent replay attack,
+		// we check the session number
+		if (emusgx_check_session_number_usable(plain_package->session_number)) {
+			// Only drop the package if the package's order 
+			// is 0 because only the first package will create a 
+			// new slot and register itself
+			if (plain_package->order == 0) {
+				// Replay attack is found
+				// Drop the package
+				pr_info("EmuSGX: Current session number %lld is unuseable\n", plain_package->session_number);
+				pr_info("EmuSGX: Group: 0x%016llX\n", emusgx_used_session_number[plain_package->session_number / 64]);
+				kfree(plain_package);
+				// Leave it and continue
+				continue;
+			}
+		}
+
+		emusgx_debug_print("EmuSGX: Package %lld/%lld of session %lld\n", plain_package->order, plain_package->total_pages, plain_package->session_number);
+
+		// Now we have the package, for the very first package,
+		// we find the corresponding slot to store it
+		if (plain_package->order == 0) {
+			// Register the session_number
+			if (emusgx_register_session_number(plain_package->session_number)) {
+				// Should not happen
+				pr_info("EmuSGX: Unexpected situation. Check if there's a bug\n");
+				kfree(plain_package);
+				continue;
+			}
+
+			// Generic package fields test
+			if (plain_package->total_pages <= 0) {
+				pr_info("EmuSGX: Invalid empty page package\n");
+				kfree(plain_package);
+				continue;
+			}
+			if (plain_package->total_pages != (plain_package->total_size / EMUSGX_PAYLOAD_SIZE + ((plain_package->total_size % EMUSGX_PAYLOAD_SIZE != 0) ? 1 : 0))) {
+				pr_info("EmuSGX: Package total pages does not match total size\n");
+				kfree(plain_package);
+				continue;
+			}
+
+			// Find its type
+			// The first uint8_t of every package is uint8_t instr
+			// which indicates the type of the package
+			instr = ((struct emusgx_page_package *)(plain_package->payload))->instr;
+			emusgx_debug_print("EmuSGX: instr = %d", instr);
+			if (instr == EMUSGX_S_SWITCHLESS) {
+				// Switchless 
+				// Find the corresponding slot
+				slot_index = emusgx_switchless_get_slot((void *)((struct emusgx_page_package *)(plain_package->payload))->addr, ((struct emusgx_page_package *)(plain_package->payload))->id);
+				if (slot_index == -1) {
+					// This page is swapped out of the sync list
+					// Nothing is done. It's cool
+					// Drop the package
+					kfree(plain_package);
+					continue;
+				}
+
+				manager_nr = ((struct emusgx_page_package *)(plain_package->payload))->id;
+
+				// Put the page into the slot
+				// Will there be a chance that a finished transmiting page gets lost?
+				// No because as the last page comes in, the dispatcher will dispatch that page 
+				// and set the slot free in this thread so there won't be a collision
+				// Thus no check is needed! But we do need to take care of the data field
+				if (emusgx_switchless_sync_slot[manager_nr][slot_index].status == EMUSGX_DISPATCH_SLOT_INUSE) {
+					// Should not happen. If happend, drop the current slot
+					pr_info("vSGX: Unexpected overwriting an inuse switchless slot @ mgr %lld, slot %d\n", manager_nr, slot_index);
+					
+					// Yet still, we overwrite the last change
+					// Because the data size should be sizeof(struct emusgx_page_package)
+					// Every time the data size should be the same
+					if (emusgx_switchless_sync_slot[manager_nr][slot_index].total_size != plain_package->total_size) {
+						// Should not happen
+						pr_info("EmuSGX: Data size mismatch in sync slot\n");
+						// Reallocate the correct size
+						kfree(emusgx_switchless_sync_slot[manager_nr][slot_index].data);
+						emusgx_switchless_sync_slot[manager_nr][slot_index].data = kmalloc(plain_package->total_size, GFP_KERNEL);
+						if (emusgx_switchless_sync_slot[manager_nr][slot_index].data == NULL) {
+							pr_info("EmuSGX: Failed to allocate data for sync slot\n");
+							emusgx_switchless_sync_slot[manager_nr][slot_index].status = EMUSGX_DISPATCH_SLOT_FREE;
+							// Drop the package
+							kfree(plain_package);
+							continue;
+						}
+					}
+					// Or the data should be the same size
+				}
+				else {
+					// If a free slot, just allocate the data field
+					emusgx_switchless_sync_slot[manager_nr][slot_index].data = kmalloc(plain_package->total_size, GFP_KERNEL);
+					if (emusgx_switchless_sync_slot[manager_nr][slot_index].data == NULL) {
+						pr_info("EmuSGX: Failed to allocate data for sync slot\n");
+						emusgx_switchless_sync_slot[manager_nr][slot_index].status = EMUSGX_DISPATCH_SLOT_FREE;
+						// Drop the package
+						kfree(plain_package);
+						continue;
+					}
+				}
+
+				emusgx_switchless_sync_slot[manager_nr][slot_index].status = EMUSGX_DISPATCH_SLOT_INUSE;
+
+				emusgx_switchless_sync_slot[manager_nr][slot_index].session_number = plain_package->session_number;
+				emusgx_switchless_sync_slot[manager_nr][slot_index].total_pages = plain_package->total_pages;
+				emusgx_switchless_sync_slot[manager_nr][slot_index].current_order = plain_package->order; // Here must be zero
+				emusgx_switchless_sync_slot[manager_nr][slot_index].total_size = plain_package->total_size;
+
+				// Data is allocated
+				slot = &(emusgx_switchless_sync_slot[manager_nr][slot_index]);
+			}
+			else if (instr == EMUSGX_S_PAGEREQ) {
+				// First we find the page request
+				pf_node = emusgx_find_pf_node((void *)(((struct emusgx_page_package *)(&(plain_package->payload[0])))->id));
+				if (pf_node == NULL) {
+					// No one is waiting for this page
+					// This is a false package
+					pr_info("EmuSGX: False page fault package is received\n");
+					kfree(plain_package);
+					continue;
+				}
+
+				if (pf_node->slot->status != EMUSGX_DISPATCH_SLOT_FREE) {
+					// Someone is transmitting the data already
+					// This should not happen
+					// Drop the package
+					pr_info("EmuSGX: PF already handled\n");
+					kfree(plain_package);
+					continue;
+				}
+
+				pf_node->slot->status = EMUSGX_DISPATCH_SLOT_INUSE;
+				pf_node->slot->session_number = plain_package->session_number;
+				pf_node->slot->total_pages = plain_package->total_pages;
+				pf_node->slot->current_order = plain_package->order;
+				pf_node->slot->total_size = plain_package->total_size;
+
+				slot = pf_node->slot;
+			}
+			else if (instr == EMUSGX_S_ERESUME) {
+				// EEXIT handling
+				// First we find corresponding waiting thread
+
+				aex_node = emusgx_find_aex_node(((struct emusgx_eresume_package *)(plain_package->payload))->tcs_pa);
+				if (aex_node == NULL) {
+					// No one is waiting for this eexit
+					// This is a false package
+					pr_info("EmuSGX: False ERESUME package is received\n");
+					kfree(plain_package);
+					continue;
+				}
+
+				if (aex_node->slot->status != EMUSGX_DISPATCH_SLOT_FREE) {
+					// Someone is transmitting the data already
+					// This should not happen
+					// Drop the package
+					pr_info("EmuSGX: EEXIT already handled\n");
+					kfree(plain_package);
+					continue;
+				}
+
+				aex_node->slot->status = EMUSGX_DISPATCH_SLOT_INUSE;
+				aex_node->slot->session_number = plain_package->session_number;
+				aex_node->slot->total_pages = plain_package->total_pages;
+				aex_node->slot->current_order = plain_package->order;
+				aex_node->slot->total_size = plain_package->total_size;
+				aex_node->slot->data = kmalloc(sizeof(struct emusgx_eresume_package), GFP_KERNEL);
+
+				slot = aex_node->slot;
+			}
+			else {
+				// enclu and encls
+				if (emusgx_instruction_emulation_slot.status != EMUSGX_DISPATCH_SLOT_FREE) {
+					// Not free, this should not happen
+					// Drop the package
+					pr_info("EmuSGX: Instruction emulation slot is in use\n");
+					kfree(plain_package);
+					continue;
+				}
+				emusgx_instruction_emulation_slot.status = EMUSGX_DISPATCH_SLOT_INUSE;
+				emusgx_instruction_emulation_slot.session_number = plain_package->session_number;
+				emusgx_instruction_emulation_slot.total_pages = plain_package->total_pages;
+				emusgx_instruction_emulation_slot.current_order = plain_package->order;
+				emusgx_instruction_emulation_slot.total_size = plain_package->total_size;
+
+				// Allocate the package data
+				emusgx_instruction_emulation_slot.data = kmalloc(plain_package->total_size, GFP_KERNEL);
+				if (emusgx_instruction_emulation_slot.data == NULL) {
+					pr_info("EmuSGX: Failed to allocate data for instruction emulation\n");
+					emusgx_instruction_emulation_slot.status = EMUSGX_DISPATCH_SLOT_FREE;
+					kfree(plain_package);
+					continue;
+				}
+
+				slot = &emusgx_instruction_emulation_slot;
+			}
+		}
+		else {
+			// Order 1 or above
+			// First get the slot
+			slot = emusgx_get_slot_with_session_number(plain_package->session_number);
+
+			if (slot == NULL) {
+				// No slot is found
+				pr_info("EmuSGX: No slot is found for the session\n");
+				kfree(plain_package);
+				continue;
+			}
+
+			// Validate the slot
+			if (slot->total_pages != plain_package->total_pages) {
+				pr_info("EmuSGX: Package total pages mismatch\n");
+				// Drop the package;
+				kfree(plain_package);
+				continue;
+			}
+			if (slot->current_order != plain_package->order - 1) { // Must be the next package
+				pr_info("EmuSGX: Package order mismatch\n");
+				// Drop the package;
+				kfree(plain_package);
+				continue;
+			}
+			if (slot->total_size != plain_package->total_size) {
+				pr_info("EmuSGX: Package size mismatch\n");
+				// Drop the package;
+				kfree(plain_package);
+				continue;
+			}
+			if (slot->total_pages <= plain_package->order) {
+				pr_info("EmuSGX: Package order overflow\n");
+				// Drop the package;
+				kfree(plain_package);
+				continue;
+			}
+
+			// Update the order
+			slot->current_order += 1;
+		}
+
+		// Now we copy the data to the node
+		if ((plain_package->order == plain_package->total_pages - 1) && (plain_package->total_size % EMUSGX_PAYLOAD_SIZE != 0)) {
+			// The final bytes
+			memcpy(slot->data + EMUSGX_PAYLOAD_SIZE * plain_package->order, &(plain_package->payload[0]), plain_package->total_size % EMUSGX_PAYLOAD_SIZE);
+		}
+		else {
+			// Copy the whole payload
+			memcpy(slot->data + EMUSGX_PAYLOAD_SIZE * plain_package->order, &(plain_package->payload[0]), EMUSGX_PAYLOAD_SIZE);
+		}
+
+
+		// plain_package is not used anymore
+		// Free it
+		kfree(plain_package);
+		
+
+		// Finalization: dispatch signal to the handler thread or just do it on our own
+		if (slot->current_order == slot->total_pages - 1) {
+			if (stamp_i < 20)
+				stamps_dispatched[stamp_i] = ktime_get_real_ns();
+			stamp_i+= 1;
+			// Check the instr and dispatch
+			instr = *((uint8_t *)(slot->data));
+			// PAGEREQ can be handled directly here because it will wake up the waiting
+			// user thread
+			if (instr == EMUSGX_S_PAGEREQ) {
+				// We just wake up the semaphore and that's done
+				// But wait! Multiple threads could be waiting on this page
+				pf_node = emusgx_find_pf_node((void *)(((struct emusgx_page_package *)(slot->data))->id));
+				if (pf_node == NULL) {
+					// Should not happen
+					pr_info("EmuSGX: Unexpected page fault node not found\n");
+				}
+
+				// We then need to remove this node from the queue
+				emusgx_remove_pf_node(pf_node);
+
+				// We will just wake up once
+				// The thread who firstly registered the request will wakeup others
+				// when the page is ready
+				up(pf_node->node_semaphore);
+			}
+			else if (instr == EMUSGX_S_ERESUME) {
+				// EEXIT
+				// Wake up the waiting handler
+				aex_node = emusgx_find_aex_node(((struct emusgx_eresume_package *)(slot->data))->tcs_pa);
+				if (aex_node == NULL) {
+					// Should not happen
+					pr_info("EmuSGX: Unexpected AEX node not found\n");
+				}
+
+				// Wake up the semaphore
+				up(&aex_node->semaphore);
+
+				// The node will be freed by the woken-up thread
+				// in emusgx_wait_for_eexit_request
+			}
+			else {
+				if (vsgx_need_new_manager(slot->data)) {
+					// Find a new manager
+					manager_entry = emusgx_get_free_manager();
+					if (manager_entry == NULL) {
+						// We send a GP back
+						pr_info("vSGX: dispatcher: No more free manager\n");
+						response.instr = instr;
+						response.write_back = 0;
+
+						if (instr == EMUSGX_S_ECREATE) {
+							response.response = 2; // GP for ECREATE
+						}
+						else {
+							// ELDB/ELDU
+							response.response = 5; // GP for ELDB/ELDU
+						}
+						if (emusgx_send_data(&response, sizeof(struct emusgx_raw_response))) {
+							pr_info("vSGX: Failed to response\n");
+						}
+						// Free the slot and continue
+						kfree(slot->data);
+						slot->status = EMUSGX_DISPATCH_SLOT_FREE;
+						continue;
+					}
+					manager_nr = manager_entry->manager_nr;
+					// Note the local dispatcher does not need to have the manager
+					// entry because it can get the entry with current->manager_entry
+					// The current->manager_entry is set by the manager during its
+					// registration
+
+					emusgx_debug_print("vSGX: New manager at %lld\n", manager_nr);
+
+					// We also do not have to free the manager if ECREATE fails here 
+					// because the ENCLS handler will do this for us
+				}
+				else {
+					manager_nr = emusgx_find_enclave_with_package(slot->data);
+				}
+				if (manager_nr == -1) {
+					pr_info("vSGX: Failed to retrive package's manager. This should not happen.\n");
+					// Only SWITCHLESS or a false package can go here.
+					// Note that a hack is done by dispatching the non-contained
+					// package for ENCLS and ENCLU to the first manager.
+					// This is because that Intel has specified faults for this situation
+					// thus we can randomly pick a manager to report such fault.
+					slot->status = EMUSGX_DISPATCH_SLOT_FREE;
+					continue;
+				}
+				
+				// Dispatch to loacl dispatchers
+				// Freeing the data is the responsibility of the local dispatcher
+				emusgx_queue_local_dispatch_request(slot->data, manager_nr);
+
+				slot->status = EMUSGX_DISPATCH_SLOT_FREE;
+			}
+		}
+		if (stamp_i == 20) {
+			for (stamp_i = 0; stamp_i < 20; stamp_i++) {
+				pr_info("EmuSGX: Waked = %lld\n", stamps_waked[stamp_i]);
+			}
+			for (stamp_i = 0; stamp_i < 20; stamp_i++) {
+				pr_info("EmuSGX: Dispatched = %lld\n", stamps_dispatched[stamp_i]);
+			}
+			stamp_i = 100;
+		}
+	
+	}
+	return -1;
+}
\ No newline at end of file
diff --git a/kernel/emusgx/emusgx_fetch.c b/kernel/emusgx/emusgx_fetch.c
new file mode 100644
index 000000000..de952514a
--- /dev/null
+++ b/kernel/emusgx/emusgx_fetch.c
@@ -0,0 +1,94 @@
+#include <linux/kernel.h>
+#include <linux/mm.h>
+#include <linux/slab.h>
+#include <linux/string.h>
+
+#include <emusgx/emusgx_internal.h>
+#include <emusgx/emusgx_sender.h>
+
+void *emusgx_receive_page = NULL;
+
+struct emusgx_page_request_package {
+	uint8_t instr;
+	uint64_t addr;
+	uint64_t manager_nr;
+	uint64_t semaphore_addr;
+} __attribute__((__packed__));
+
+uint8_t emusgx_get_guest_page(uint64_t addr, void **page_data_addr, struct semaphore **other_waiting_semaphore, uint64_t *waiting_nr) {
+	struct emusgx_page_request_package request;
+	struct emusgx_page_fault_queue_node *pf_request_node;
+	uint8_t is_main;
+
+	*page_data_addr = NULL;
+
+	// Here addr is already page aligned
+
+	request.instr = EMUSGX_S_PAGEREQ;
+	request.addr = addr;
+	request.manager_nr = current->manager_entry->manager_nr;
+
+	// Register a page request
+	pf_request_node = emusgx_register_pf_request((void *)addr, &is_main);
+
+	if (pf_request_node == NULL) {
+		// Terrible thing happened
+		pr_info("EmuSGX: Failed to register pf request\n");
+		return 0;
+	}
+
+	if (!is_main) {
+		// Wait on the other_waiting_semaphore
+		if (down_killable(&(pf_request_node->other_waiting_semaphore))) {
+			return 0;
+		}
+		if (atomic_dec_and_test(&pf_request_node->waiting_nr)) {
+			// The node is good to be freed
+			// First we free the slot and data
+			kfree(pf_request_node->slot->data);
+			kfree(pf_request_node->slot);
+			// Then we free the semaphore
+			kfree(pf_request_node->node_semaphore);
+			// Finally we free the node itself
+			kfree(pf_request_node);
+		}
+		// The page is set up
+		return 1;
+	}
+
+	request.semaphore_addr = (uint64_t)(pf_request_node->node_semaphore);
+
+	// The main requester
+	// Send request
+	if (emusgx_send_data(&request, sizeof(struct emusgx_page_request_package))) {
+		pr_info("EmuSGX: Failed to send pf request\n");
+		return 0;
+	}
+
+	// Wait for the request package to return
+	if (down_killable(pf_request_node->node_semaphore)) {
+		return 0;
+	}
+
+	// Check if the guest failed to fetch the page
+	if (((struct emusgx_page_package *)(pf_request_node->slot->data))->addr == (uint64_t)NULL) {
+		pr_info("EmuSGX: Guest VM failed to fetch the page\n");
+		return 0;
+	}
+
+	// The page is here
+	*page_data_addr = kmalloc(4096, GFP_KERNEL);
+	if (*page_data_addr == NULL) {
+		// Terrible thing happened
+		pr_info("EmuSGX: Failed to allocate page data buffer for pf request\n");
+		return 0;
+	}
+
+	// mask is unused since we are copying the whole page
+	memcpy(*page_data_addr, &(((struct emusgx_page_package *)(pf_request_node->slot->data))->page[0]), 4096);
+
+	*other_waiting_semaphore = &(pf_request_node->other_waiting_semaphore);
+	*waiting_nr = atomic_read(&pf_request_node->waiting_nr);
+
+	return 0;
+}
diff --git a/kernel/emusgx/encls_cross_vm.c b/kernel/emusgx/encls_cross_vm.c
new file mode 100644
index 000000000..b77be51bd
--- /dev/null
+++ b/kernel/emusgx/encls_cross_vm.c
@@ -0,0 +1,1454 @@
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/slab.h> 
+#include <linux/uaccess.h>
+#include <linux/string.h>
+
+#include <emusgx/emusgx.h>
+#include <emusgx/emusgx_fault.h>
+#include <emusgx/emusgx_internal.h>
+#include <emusgx/emusgx_sender.h>
+#include <emusgx/emusgx_debug.h>
+
+// 0 Good
+// 1 PF SECS
+// 2 PF EPC Page
+// 3 GP 0
+uint8_t _enclave_local emusgx_validate_and_do_remote_for_eadd(struct sgx_secs *secs_pa, void *epc_page, void __user *linaddr, void *srcpage, struct sgx_secinfo *secinfo) {
+	struct emusgx_epcm *epc_epcm;
+	struct emusgx_epcm *secs_epcm;
+	struct sgx_secs *secs;
+	struct sgx_tcs __user *tmp_tcs = linaddr;
+	int i;
+	uint64_t tmp_enclave_offset;
+	uint64_t tmp_update_field[8];
+
+	// Check within EPC
+	if (emusgx_check_within_epc(epc_page)) {
+		return 2;
+	}
+
+	if (emusgx_check_within_epc(secs_pa)) {
+		return 1;
+	}
+
+	epc_epcm = emusgx_get_epcm(epc_page);
+	secs_epcm = emusgx_get_epcm(secs_pa);
+
+	if (epc_epcm->valid != 0) {
+		return 2;
+	}
+
+	if (secs_epcm->valid == 0 || secs_epcm->page_type != SGX_PT_SECS) {
+		return 1;
+	}
+
+	// Switch the SECS from PA to real data in kernel
+	secs = secs_epcm->enclave_address;
+
+	// Map the page
+	vsgx_check_and_unmap_page(linaddr);
+	vsgx_map_empty_page_to_user_space(linaddr);
+
+	// Copy 4KBytes from source page to EPC page
+	if (copy_to_user(linaddr, srcpage, 4096)) {
+		pr_info("EmuSGX: Failed to copy data to linaddr in user space for eadd\n");
+		return 3;
+	}
+
+	if (secinfo->flags.page_type == SGX_PT_TCS) {
+		// Reserved bits must be zero
+		__uaccess_begin();
+		if (tmp_tcs->flags.reserved != 0) {
+			__uaccess_end();
+			vsgx_check_and_unmap_page(linaddr);
+			return 3;
+		}
+		for (i = 0; i < 4016; i++) {
+			if (tmp_tcs->reserved4[i] != 0) {
+				__uaccess_end();
+				vsgx_check_and_unmap_page(linaddr);
+				return 3;
+			}
+		}
+
+		if (
+			(secs->attribute.mod64bit == 0) && 
+			(
+				(((tmp_tcs->fslimit) & 0x0FFF) != 0x0FFF) ||
+				(((tmp_tcs->gslimit) & 0x0FFF) != 0x0FFF)
+			)
+		) {
+			__uaccess_end();
+			vsgx_check_and_unmap_page(linaddr);
+			return 3;
+		}
+		__uaccess_end();
+	}
+	else if (secinfo->flags.page_type == SGX_PT_REG) {
+		if (secinfo->flags.W == 1 && secinfo->flags.R == 0) {
+			vsgx_check_and_unmap_page(linaddr);
+			return 3;
+		}
+	}
+
+	// Check the enclave offset is within the enclave linear address space
+	if ((uint64_t)linaddr < secs->base || (uint64_t)linaddr >= secs->base + secs->size) {
+		vsgx_check_and_unmap_page(linaddr);
+		return 3;
+	}
+
+	// Check concurrency of measurement resource
+	// Ignored
+
+	// Check if the enclave to which the page will be added is already in initialized state
+	// Moved here because we use real SECS data thereon
+	if (emusgx_is_secs_inited(secs_pa)) {
+		vsgx_check_and_unmap_page(linaddr);
+		return 3;
+	}
+
+	// For TCS pages, force EPCM.rwx bits to 0 and no debug access
+	if (secinfo->flags.page_type == SGX_PT_TCS) {
+		secinfo->flags.R = 0;
+		secinfo->flags.W = 0;
+		secinfo->flags.X = 0;
+		__uaccess_begin();
+		// Flags.DBGOPTIN
+		tmp_tcs->flags.dbgoptin = 0;
+		// CSSA
+		tmp_tcs->cssa = 0;
+		// AEP
+		tmp_tcs->aep = 0;
+		// STATE
+		tmp_tcs->state = 0;
+		__uaccess_end();
+	}
+
+	// Add enclave offset and security attributes to MRENCLAVE
+	tmp_enclave_offset = (uint64_t)linaddr - (uint64_t)secs->base;
+	tmp_update_field[0] = 0x0000000044444145;
+	tmp_update_field[1] = tmp_enclave_offset;
+	memcpy(((uint8_t *)tmp_update_field) + 16, secinfo, 48);
+	emusgx_sha256update(secs->mrenclave, tmp_update_field);
+	secs->mrenclave_update_counter += 1; 
+
+	// Create mapping record
+	vsgx_map_epc_to_vaddr(linaddr, (uint64_t)epc_page);
+
+	// Add enclave offset and security attributes to EPCM
+	epc_epcm->R = secinfo->flags.R;
+	epc_epcm->W = secinfo->flags.W;
+	epc_epcm->X = secinfo->flags.X;
+	epc_epcm->page_type = secinfo->flags.page_type;
+	epc_epcm->enclave_address = linaddr;
+
+	// Associate the EPCPAGE with the SECS by storing the SECS identifier of TMP_SECS
+	epc_epcm->enclave_secs = (uint64_t)secs_pa; // Here the encalve_secs refers to physical address
+	epc_epcm->manager_entry = current->manager_entry;
+
+	// Set EPCM entry fields
+	epc_epcm->blocked = 0;
+	epc_epcm->pending = 0;
+	epc_epcm->modified = 0;
+	epc_epcm->valid = 1;
+
+	return 0;
+}
+
+// return 0 if good
+// return 1 if pf on secs
+// return 2 if pf on epc_addr
+// return 3 if gp(0)
+uint8_t emusgx_validate_and_do_remote_for_eaug(struct sgx_secs *secs_pa, void __user *linaddr, void *epc_addr) {
+	struct emusgx_epcm *secs_epcm;
+	struct emusgx_epcm *epc_epcm;
+	struct sgx_secs *secs;
+
+	secs_epcm = emusgx_get_epcm(secs_pa);
+	epc_epcm = emusgx_get_epcm(epc_addr);
+	
+	// Check within EPC
+	if (emusgx_check_within_epc(epc_addr)) {
+		return 2;
+	}
+
+	if (emusgx_check_within_epc(secs_pa)) {
+		return 1;
+	}
+
+	// Check the EPC page for concurrency
+	// Ignored
+
+	if (epc_epcm->valid != 0) {
+		pr_info("vSGX: EAUG: EPC already valid\n");
+		return 2;
+	}
+
+	// Check the SECS for concurrency
+	// Ignored
+
+	if (secs_epcm->valid == 0 || secs_epcm->page_type != SGX_PT_SECS) {
+		pr_info("vSGX: EAUG: SECS EPCM invalid\n");
+		return 1;
+	}
+
+	// Check if the enclave to which the page will be added is in the initialized state
+	if (!emusgx_is_secs_inited(secs_pa)) {
+		pr_info("vSGX: EAUG: SECS not inited\n");
+		return 3;
+	}
+	
+	// Switch SECS from PA to real data
+	secs = secs_epcm->enclave_address;
+
+	// Check the enclave offset is within the enclave linear address space
+	if ((uint64_t)linaddr < secs->base || (uint64_t)linaddr >= secs->base + secs->size) {
+		pr_info("vSGX: EAUG: linaddr out of range\n");
+		return 3;
+	}
+
+	// Map the page
+	vsgx_check_and_unmap_page(linaddr);
+	vsgx_map_empty_page_to_user_space(linaddr);
+
+	// Clear the content of EPC page
+	__uaccess_begin();
+	memset(linaddr, 0, 4096);
+	__uaccess_end();
+
+	// Create mapping record
+	vsgx_map_epc_to_vaddr(linaddr, (uint64_t)epc_addr);
+
+	// Set EPCM security attributes
+	epc_epcm->R = 1;
+	epc_epcm->W = 1;
+	epc_epcm->X = 0;
+	epc_epcm->page_type = SGX_PT_REG;
+	epc_epcm->enclave_address = linaddr;
+	epc_epcm->blocked = 0;
+	epc_epcm->pending = 1;
+	epc_epcm->modified = 0; 
+
+	// Associate the EPCPAGE with the SECS by storing the SECS identifier of SECS
+	epc_epcm->enclave_secs = (uint64_t)secs_pa;
+	epc_epcm->manager_entry = current->manager_entry;
+
+	// Set EPCM valid fields
+	epc_epcm->valid = 1;
+
+	return 0;
+}
+
+// return rax value
+uint64_t emusgx_do_remote_for_eblock(void *epc_page) {
+	struct emusgx_epcm *epc_epcm;
+	
+	// Check within EPC
+	if (emusgx_check_within_epc(epc_page)) {
+		return EMUSGX_PF_RCX;
+	}
+
+	// Check concurrency with other Intel SGX instructions
+	// Not possible for ETRACK since we will handle ENCLS in serial
+	// TODO: If other Intel SGX instructions reading or writing EPCM
+
+	epc_epcm = emusgx_get_epcm(epc_page);
+
+	if (epc_epcm->valid == 0) {
+		return EMUSGX_PG_INVLD;
+	}
+
+	if ((epc_epcm->page_type != SGX_PT_REG) &&
+		 (epc_epcm->page_type != SGX_PT_TCS) && 
+		 (epc_epcm->page_type != SGX_PT_TRIM)) {
+		if (epc_epcm->page_type == SGX_PT_SECS) {
+			return EMUSGX_PG_IS_SECS;
+		}
+		else {
+			return EMUSGX_NOTBLOCKABLE;
+		}
+	}
+
+	// Check if the page is already blocked and report blocked state
+	if (epc_epcm->blocked == 1) {
+		return EMUSGX_BLKSTATE;
+	}
+	else {
+		epc_epcm->blocked = 1;
+	}
+
+	return 0;
+}
+
+// return 0 if good
+// return 1 if pf on epc_page
+// return 2 if gp(0)
+uint8_t emusgx_validate_and_do_remote_for_ecreate(void *srcpage, void *epc_page) {
+	struct emusgx_epcm *epc_epcm;
+	struct sgx_secs *tmp_secs;
+	int i, popcnt;
+	uint64_t tmp_update_field[8];
+	struct emusgx_user_space_manager_entry *tmp_manager_entry;
+
+	// Check within EPC
+	if (emusgx_check_within_epc(epc_page)) {
+		return 1;
+	}
+
+	epc_epcm = emusgx_get_epcm(epc_page);
+	// Since linaddr must be 0, no mapping is created from SECS to the encalve mapping
+	tmp_secs = kmalloc(sizeof(struct sgx_secs), GFP_KERNEL);
+
+	if (epc_epcm->valid == 1) {
+		return 1;
+	}
+
+	// Copy 4KBytes from source page to EPC page
+	if (tmp_secs == NULL) {
+		pr_info("EmuSGX: Failed to allocate SECS\n");
+		return 2;
+	}
+	memcpy(tmp_secs, srcpage, 4096);
+
+	// Check lower 2 bits of XFRM are set
+	if (((tmp_secs->xfrm) & 0x03) != 0x03) {
+		return 2;
+	}
+
+	// if (XFRM is legal) 
+	// o XFRM[1:0] must be set to 0x3
+	// o If the processor does not support XSAVE, or if the system software 
+	//   has not enabled XSAVE, then XFRM[63:2] must be zero.
+	// o If the processor does support XSAVE, XFRM must contain a value that 
+	//   would be legal if loaded into XCR0.
+	// Ignored
+
+	// Make sure that the SECS does not have any unsupported MISCSELECT options
+	// MISC is for marking extended features
+	// We hardcode them into our implementation rather than offering CPUID
+	if (1 & tmp_secs->miscselect) {
+		// entry lock release
+		return 2;
+	}
+
+	// Compute size of MISC area
+	// We do not xsave so there's no need to do it
+
+	// Compute the size required to save state of the enclave on async exit
+	// No need
+
+	// Ensure that the declared area is larget enough to hold XSAVE and GPR stat
+	// No need
+
+	if (tmp_secs->attribute.mod64bit == 1) {
+		// if base addr not canonical
+		if ((((tmp_secs->base) & 0xFFFF800000000000) != 0) && 
+			(((tmp_secs->base) & 0xFFFF800000000000) != 0xFFFF800000000000)) {
+			// non canonical
+			return 2;
+		}
+	}
+
+	if (tmp_secs->attribute.mod64bit == 0) {
+		// if base addr not correct
+		/*
+		if (((tmp_secs->base) & 0xFFFFFFFF00000000) != 0) {
+			// non correct
+			__uaccess_end();
+			return 2;
+		}
+
+		if (((tmp_secs->size) & 0xFFFFFFFF00000000) != 0) {
+			// non correct
+			__uaccess_end();
+			return 2;
+		}
+		*/
+		// We do not support 32-bit mode
+		pr_info("EmuSGX: We do not support 32-bit enclaven\n");
+		return 2;
+	}
+
+	if (tmp_secs->attribute.mod64bit == 1) {
+		// if size is corrupted
+		if (((tmp_secs->size) & 0xFFFFFFE000000000) != 0) {
+			// non canonical
+			pr_info("vSGX: ECREATE: Non canonical size\n");
+			return 2;
+		}
+	}
+
+	// Enclave size must be at least 8192 bytes and must be power of 2 in bytes
+	if (tmp_secs->size < 8192) {
+		pr_info("vSGX: ECREATE: Enclave smaller than 8192\n");
+		return 2;
+	}
+	popcnt = 0;
+	for (i = 0; i < 64; i++) {
+		if (tmp_secs->size & (((uint64_t)1) << i)) {
+			popcnt += 1;
+		}
+	}
+	if (popcnt > 1) {
+		pr_info("vSGX: ECREATE: Enclave size not a power of 2\n");
+		return 2;
+	}
+
+	// Ensure base address of an enclave is aligned on size
+	if (tmp_secs->base & (tmp_secs->size - 1)) {
+		pr_info("vSGX: ECREATE: Enclave base not aligned on size\n");
+		return 2;
+	}
+
+	// Ensure the SECS does not have any unsupported attributes
+	// WE SUPPORT ALL
+
+	// Ensure reserved fields are zero
+	for (i = 0; i < 24; i++) {
+		if (tmp_secs->reserved1[i] != 0) {
+			pr_info("vSGX: ECREATE: Reserved fields not 0\n");
+			return 2;
+		}
+	}
+	for (i = 0; i < 32; i++) {
+		if (tmp_secs->reserved2[i] != 0) {
+			pr_info("vSGX: ECREATE: Reserved fields not 0\n");
+			return 2;
+		}
+	}
+	// reserved 3 seems to include configid
+	// we'd better leave it
+	//for (i = 0; i < 96; i++) {
+	//	if (tmp_secs->reserved3[i] != 0) {
+	//		return 2;
+	//	}
+	//}
+	// reserved 4 includes EID, other non-zero reserved field and must be zero fields
+	// not here to check
+
+	// Clear SECS to uninitialized
+	emusgx_mark_secs_inited(epc_page, 0);
+
+	emusgx_sha256init(tmp_secs->mrenclave);
+	tmp_secs->isvsvn = 0;
+	tmp_secs->isvprodid = 0;
+	tmp_secs->mrenclave_update_counter = 0;
+
+	// Add ECREATE string and SECS fields to MRENCLAVE
+	tmp_update_field[0] = 0x0045544145524345;
+	tmp_update_field[1] = tmp_secs->ssaframesize;
+	tmp_update_field[2] = tmp_secs->size;
+	for (i = 3; i < 8; i++) {
+		tmp_update_field[i] = 0;
+	}
+	emusgx_sha256update(tmp_secs->mrenclave, tmp_update_field);
+	tmp_secs->mrenclave_update_counter += 1;
+
+	emusgx_debug_print("EmuSGX: hash after extend is 0x%016llX\n", *((uint64_t *)(tmp_secs->mrenclave)));
+
+	// Set manager
+	tmp_manager_entry = current->manager_entry; // current->manager_entry is set by when the manager initialized itself
+						    // The manager is already occupied during the global dispatch procedure
+	tmp_manager_entry->secs_pa = epc_page;      // Use physical address of SECS
+	tmp_secs->manager_entry = (uint64_t)tmp_manager_entry;
+
+	// Set EID
+	tmp_secs->eid = vsgx_get_new_eid(); // Get a new EID
+
+	// Set the EPCM entry, first create SECS identifier and store the identifier in EPCM
+	epc_epcm->page_type = SGX_PT_SECS;
+	epc_epcm->enclave_address = tmp_secs; // Should be 0 from the reference but we take a shorcut to store it here
+	epc_epcm->R = 0;
+	epc_epcm->W = 0;
+	epc_epcm->X = 0;
+
+	// Added SECS to achieve dispatching
+	epc_epcm->enclave_secs = (uint64_t)epc_page;
+	epc_epcm->manager_entry = current->manager_entry;
+
+	// Set EPCM entry fields
+	epc_epcm->blocked = 0;
+	epc_epcm->pending = 0;
+	epc_epcm->modified = 0;
+	epc_epcm->valid = 1;
+
+	return 0;
+}
+
+// return 0 if good
+// return 1 if pf on addr
+// return 2 if gp(0)
+uint8_t emusgx_validate_and_do_remote_for_eextend(void *addr) {
+	struct sgx_secs *tmp_secs;
+	uint64_t tmp_enclave_offset;
+	uint64_t tmp_update_field[8];
+	struct emusgx_epcm *addr_epcm;
+	void *addr_buffer;
+	void __user *linaddr;
+	int i;
+	// uint32_t *secs_mrenclave;
+
+	// Check within EPC
+	if (emusgx_check_within_epc(addr)) {
+		return 1;
+	}
+
+	// Make sure no ohter Intel SGX instruction is accessing EPCM
+	// Ignored
+
+	addr_epcm = emusgx_get_epcm(addr);
+
+	if (addr_epcm->valid == 0) {
+		return 1;
+	}
+
+	// Make sure that addr is pointing to a PT_REG or PT_TCS
+	if ((addr_epcm->page_type != SGX_PT_REG)
+	    && (addr_epcm->page_type != SGX_PT_TCS)) {
+		return 1;
+	}
+
+	tmp_secs = emusgx_get_epcm((void *)addr_epcm->enclave_secs)->enclave_address; // Use real data
+
+	linaddr = addr_epcm->enclave_address;
+
+	// Make sure no other instruction is accessing MRENCLAVE or ATTRIBUTES.INIT
+	// Ignored
+
+	// Calculate enclave offset
+	tmp_enclave_offset = (uint64_t)(linaddr - tmp_secs->base);
+	tmp_enclave_offset += ((uint64_t)addr & 0x0FFF);
+
+	// Add ECTEND message and offset to MRENCLAVE
+	tmp_update_field[0] = 0x00444E4554584545; // "EEXTEND"
+	tmp_update_field[1] = tmp_enclave_offset;
+	for (i = 2; i < 8; i++) {
+		tmp_update_field[i] = 0;
+	}
+	emusgx_sha256update(tmp_secs->mrenclave, tmp_update_field);
+	tmp_secs->mrenclave_update_counter += 1;
+
+	// Add 256 bytes to MRENCLAVE, 64 byte at a time
+	addr_buffer = kmalloc(64, GFP_KERNEL);
+	if (addr_buffer == NULL) {
+		pr_info("EmuSGX: Failed to allocate addr_buffer\n");
+		return 2;
+	}
+	for (i = 0; i < 4; i++) {
+		if (copy_from_user(addr_buffer, linaddr + (64 * i), 64)) {
+			// Failed to copy, that's a page fault
+			kfree(addr_buffer);
+			return 1;
+		}
+		emusgx_sha256update(tmp_secs->mrenclave, addr_buffer);
+		tmp_secs->mrenclave_update_counter += 1;
+	}
+	// INC enclave's MRENCLAVE update counter by 4
+	// Done above
+	kfree(addr_buffer);
+
+	emusgx_debug_print("EmuSGX: hash after extend is 0x%016llX\n", *((uint64_t *)(tmp_secs->mrenclave)));
+
+	return 0;
+}
+
+// return rax value
+uint64_t emusgx_validate_and_do_remote_for_einit(struct sgx_sigstruct *sigstruct, struct sgx_secs *secs_pa, struct sgx_einittoken *einittoken) {
+	// RCX -> secs
+	// RDX -> einittoken
+	int i;
+	uint32_t *tmp_mrsigner;
+	// uint64_t *tmp_einittokenkey;
+	// uint64_t *tmp_mac;
+	uint64_t tmp_counter;
+	// struct emusgx_tmp_key_dependencies *tmp_keydependencies;
+	struct sgx_secs *secs;
+
+	// Check within EPC
+	if (emusgx_check_within_epc(secs_pa)) {
+		return EMUSGX_PF_RCX;
+	}
+
+	// Verify signature
+	// With so less information the function emusgx_verify_sigstruct
+	// is very likely to crash. If so, temporarily disable it
+	/*
+	if (emusgx_verify_sigstruct(sigstruct)) {
+		return EMUSGX_INVALID_SIGNATURE;
+	}
+	*/
+
+	if (emusgx_get_epcm(secs_pa)->valid == 0 || emusgx_get_epcm(secs_pa)->page_type != SGX_PT_SECS) {
+		return EMUSGX_PF_RCX;
+	}
+
+	// Switch SECS from PA to real data
+	secs = emusgx_get_epcm(secs_pa)->enclave_address;
+
+	// Make sure no other instruction is accessing MRENCLAVE or ATTRIBUTES.INIT
+	// Ignored
+
+	// Caculate finalized version of MRENCLAVE
+	tmp_counter = secs->mrenclave_update_counter;
+	emusgx_sha256final(secs->mrenclave, tmp_counter);
+
+	emusgx_debug_print("EmuSGX: Counter = %lld\n", secs->mrenclave_update_counter);
+
+	// Verify MRENCLAVE from SIGSTRUCT
+	/*
+	__uaccess_begin();
+	for (i = 0; i < 8; i++) {
+		if (sigstruct->enclavehash[i] != secs->mrenclave[i]) {
+			pr_info("Hash %d: SIG: 0x%08X, MRENCLAVE: 0x%08X", i, sigstruct->enclavehash[i], secs->mrenclave[i]);
+			__uaccess_end();
+			return EMUSGX_INVALID_MEASUREMENT;
+		}
+	}
+	__uaccess_end();
+	*/
+
+	tmp_mrsigner = emusgx_sha256((uint64_t *)(sigstruct->modulus), 384);
+
+	// If INTEL_ONLY attributes are set, SIGSTRUCT must be signed using the Intel Key
+	if (secs->attributes & 0x20) {
+		// Must be signed by Intel
+		// Assume signed by Intel
+		/*for (i = 0; i < 8; i++) {
+			if (emusgx_csr_intelpubkeyhash[i] != tmp_mrsigner[i]) {
+				kfree(tmp_mrsigner);
+				return EMUSGX_INVALID_ATTRIBUTE;
+			}
+		}*/
+	}
+
+	// Verify SIGSTRUCT.MISCSELECT requirements are met
+	if ((secs->miscselect & sigstruct->miscmask) != (sigstruct->miscselect & sigstruct->miscmask)) {
+		kfree(tmp_mrsigner);
+		return EMUSGX_INVALID_ATTRIBUTE;
+	}
+
+	// If EINITTOKEN.VALID[0] is 0, verify the enclave is signed by Intel
+	if ((einittoken->valid & 1) == 0) {
+		// Must be signed by Intel
+		// Assume signed by Intel
+		/*for (i = 0; i < 8; i++) {
+			if (emusgx_csr_intelpubkeyhash[i] != tmp_mrsigner[i]) {
+				kfree(tmp_mrsigner);
+				return EMUSGX_INVALID_EINITTOKEN;
+			}
+		}
+		goto COMMIT;*/
+	}
+
+	// Debug Launch Enclave cannot launch Production Enclaves
+	// NOT SUPPORTED. IF DEBUG, return unsuccessful
+	/* Why not just let it go?
+	if (einittoken->maskedattributesle[0] & 0x2 DEBUG mask || secs->attribute.debug) {
+		kfree(tmp_mrsigner);
+		return EMUSGX_INVALID_EINITTOKEN;
+	}
+	*/
+
+	// Check reserve space in EINIT token includes reserved regions and upper bits in valid field
+	if (einittoken->valid & 0xFFFFFFFE) {
+		kfree(tmp_mrsigner);
+		return EMUSGX_INVALID_EINITTOKEN;
+	}
+	for (i = 0; i < 44; i++) {
+		if (einittoken->reserved1[i]) {
+			kfree(tmp_mrsigner);
+			return EMUSGX_INVALID_EINITTOKEN;
+		}
+	}
+	for (i = 0; i < 32; i++) {
+		if (einittoken->reserved2[i]) {
+			kfree(tmp_mrsigner);
+			return EMUSGX_INVALID_EINITTOKEN;
+		}
+	}
+	for (i = 0; i < 32; i++) {
+		if (einittoken->reserved3[i]) {
+			kfree(tmp_mrsigner);
+			return EMUSGX_INVALID_EINITTOKEN;
+		}
+	}
+	for (i = 0; i < 24; i++) {
+		if (einittoken->reserved4[i]) {
+			kfree(tmp_mrsigner);
+			return EMUSGX_INVALID_EINITTOKEN;
+		}
+	}
+
+	// EINIT token must be <= CR_CPUSVN
+	if (emusgx_compare_cpusvn(einittoken->cpusvnle, emusgx_cr_cpusvn) == 1) {
+		kfree(tmp_mrsigner);
+		return EMUSGX_INVALID_CPUSVN;
+	}
+
+/*	// Assume correct
+	// Derive Launch key used to calculate EINITTOKEN.MAC
+	tmp_keydependencies = kmalloc(sizeof(struct emusgx_tmp_key_dependencies), GFP_KERNEL);
+	tmp_keydependencies->keyname = SGX_LAUNCH_KEY;
+	tmp_keydependencies->isvprodid = einittoken->isvprodidle;
+	tmp_keydependencies->isvsvn = einittoken->isvsvnle;
+	tmp_keydependencies->ownerepoch[0] = emusgx_csr_owner_epoch[0];
+	tmp_keydependencies->ownerepoch[1] = emusgx_csr_owner_epoch[1];
+	tmp_keydependencies->attributes[0] = einittoken->maskedattributesle[0];
+	tmp_keydependencies->attributes[1] = einittoken->maskedattributesle[1];
+	tmp_keydependencies->attributesmask[0] = 0;
+	tmp_keydependencies->attributesmask[1] = 0;
+	for (i = 0; i < 8; i++) {
+		tmp_keydependencies->mrenclave[i] = 0;
+	}
+	for (i = 0; i < 8; i++) {
+		tmp_keydependencies->mrsigner[i] = 0;
+	}
+	for (i = 0; i < 4; i++) {
+		tmp_keydependencies->keyid[i] = einittoken->keyid[i];
+	}
+	tmp_keydependencies->seal_key_fuses[0] = emusgx_cr_seal_fuses[0];
+	tmp_keydependencies->seal_key_fuses[1] = emusgx_cr_seal_fuses[1];
+	tmp_keydependencies->cpusvn[0] = einittoken->cpusvnle[0];
+	tmp_keydependencies->cpusvn[1] = einittoken->cpusvnle[1];
+	for (i = 0; i < 352; i++) {
+		tmp_keydependencies->padding[i] = HARDCODED_PKCS1_5_PADDING[i];
+	}
+	tmp_keydependencies->miscselect = einittoken->maskedmiscselectle;
+	tmp_keydependencies->miscmask = 0;
+
+	// Calculate the derived key
+	tmp_einittokenkey = emusgx_derive_key(tmp_keydependencies);
+	kfree(tmp_keydependencies);
+
+	// Verify EINITTOKEN was generated using this CPU's Launch key and that 
+	// it has not been modified since issuing by the Launch Enclave. Only
+	// 192 bytes of EINITTOKEN are CMACed
+	tmp_mac = emusgx_cmac(tmp_einittokenkey, einittoken, 192);
+	kfree(tmp_einittokenkey);
+	if (einittoken->mac[0] != tmp_mac[0] || einittoken->mac[1] != tmp_mac[1]) {
+		pr_info("EmuSGX: EINITTOKENKEY CMAC incorrect\n");
+		kfree(tmp_mrsigner);
+		return EMUSGX_INVALID_EINITTOKEN;
+	}
+*/
+
+	// Verify EINITTOKEN is for this enclave
+	/*
+	__uaccess_begin();
+	for (i = 0; i < 8; i++) {
+		if (einittoken->mrenclave[i] != secs->mrenclave[i]) {
+			__uaccess_end();
+			kfree(tmp_mrsigner);
+			return EMUSGX_INVALID_MEASUREMENT;
+		}
+		if (einittoken->mrsigner[i] != tmp_mrsigner[i]) {
+			pr_info("EmuSGX: MRSIGNER[%d] = 0x%08X != TMP_MRSIGNER[%d] = 0x%08X\n", i, einittoken->mrsigner[i], i, tmp_mrsigner[i]);
+			__uaccess_end();
+			kfree(tmp_mrsigner);
+			return EMUSGX_INVALID_MEASUREMENT;
+		}
+	}
+
+	// Verify ATTRIBUTES in EINITTOKEN are the same as the enclave's
+	if (einittoken->attributes[0] != secs->attributes || einittoken->attributes[1] != secs->xfrm) {
+		__uaccess_end();
+		kfree(tmp_mrsigner);
+		return EMUSGX_INVALID_ATTRIBUTE;
+	}
+	__uaccess_end();
+	*/
+//COMMIT:
+	// Commit changes to the SECS; Set ISVPRODID, ISVSVN, MRSIGNED, INIT ATTRIBUTE fields in SECS
+	for (i = 0; i < 8; i++) {
+		secs->mrsigner[i] = tmp_mrsigner[i];
+	}
+	secs->isvprodid = sigstruct->isvprodid;
+	secs->isvsvn = sigstruct->isvsvn;
+	// TODO: Padding?
+	// WTF? Top 352 bytes from Signature^3 modulo MRSIGNER?
+	// MRSIGNER is a 32 byte int, how can the modulo be 352 bytes?
+	// Leave it unchanged
+
+	// Mark the SECS as initialized
+	emusgx_mark_secs_inited(secs_pa, 1);
+
+	kfree(tmp_mrsigner);
+
+	return 0;
+}
+
+// return 0 if good
+// return 1 if SGX_MAC_COMPARE_FAIL
+// return 2 if pf on EPC page
+// return 3 if pf on vaslot
+// return 4 if pf on secs
+// return 5 if gp(0)
+uint8_t emusgx_validate_and_do_remote_for_eldb_eldu(void *srcpage, struct sgx_secs *secs_pa, uint64_t *vaslot_pa, void *epc_page, struct sgx_pcmd *pcmd, uint64_t linaddr, uint8_t block, uint8_t *free_manager) {
+	// Check concurrency of EPC and VASLOT by other Intel SGX instructions
+	// Ignored
+	struct emusgx_mac_header tmp_header;
+	uint64_t tmp_ver;
+	void *epc_page_buffer;
+	struct sgx_secs *secs;
+	uint64_t *vaslot;
+	uint16_t page_type = pcmd->secinfo.flags.page_type;
+
+	struct emusgx_epcm *epc_epcm;
+	struct emusgx_epcm *vaslot_epcm;
+	int decrypt_ret;
+
+	*free_manager = 0;
+
+	// Check within EPC
+	if (emusgx_check_within_epc(epc_page)) {
+		if (page_type == SGX_PT_SECS)
+			*free_manager = 1;
+		return 2;
+	}
+
+	if (emusgx_check_within_epc(vaslot_pa)) {
+		if (page_type == SGX_PT_SECS)
+			*free_manager = 1;
+		return 3;
+	}
+
+	// Verify EPCM attributes of EPC page, VA and SECS
+	if (emusgx_get_epcm(epc_page)->valid == 1) {
+		if (page_type == SGX_PT_SECS)
+			*free_manager = 1;
+		return 2;
+	}
+
+	vaslot_epcm = emusgx_get_epcm((void *)((uint64_t)vaslot_pa & ~((uint64_t)0x0FFF)));
+
+	if ((vaslot_epcm->valid == 0) || (vaslot_epcm->page_type != SGX_PT_VA)) {
+		if (page_type == SGX_PT_SECS)
+			*free_manager = 1;
+		return 3;
+	}
+
+	epc_epcm = emusgx_get_epcm(epc_page);
+
+	// Copy PCMD into scratch buffer
+	// The PCMD here is already a copy
+
+	// Zero out TMP_HEADER
+	memset(&tmp_header, 0 , sizeof(tmp_header));
+
+	memcpy(&(tmp_header.secinfo), &(pcmd->secinfo), sizeof(struct sgx_secinfo));
+	// tmp_header.rsvd?
+	tmp_header.linaddr = linaddr;
+
+	// Verify various attributes of SECS parameter
+	if (tmp_header.secinfo.flags.page_type == SGX_PT_REG ||
+	    tmp_header.secinfo.flags.page_type == SGX_PT_TCS ||
+	    tmp_header.secinfo.flags.page_type == SGX_PT_TRIM) {
+		if ((uint64_t)secs_pa % 4096 != 0) {
+			pr_info("vSGX: ELDB/ELDU: SECS not aligned\n");
+			if (page_type == SGX_PT_SECS)
+				*free_manager = 1;
+			return 5;
+		}
+
+		// Check within EPC
+		if (emusgx_check_within_epc(secs_pa)) {
+			if (page_type == SGX_PT_SECS)
+				*free_manager = 1;
+			return 4;
+		}
+
+		// if other instructions modifying SECS
+		// Ignored
+
+		if (emusgx_get_epcm(secs_pa)->valid == 0 || emusgx_get_epcm(secs_pa)->page_type != SGX_PT_SECS) {
+			if (page_type == SGX_PT_SECS)
+				*free_manager = 1;
+			return 4;
+		}
+
+		secs = emusgx_get_epcm(secs_pa)->enclave_address;
+	}
+	else if (tmp_header.secinfo.flags.page_type == SGX_PT_SECS ||
+	    	 tmp_header.secinfo.flags.page_type == SGX_PT_VA) {
+		if ((uint64_t)secs_pa != 0) {
+			if (page_type == SGX_PT_SECS)
+				*free_manager = 1;
+			return 5;
+		}
+	}
+	else {
+		pr_info("vSGX: ELDB/ELDU: Not a valid page type\n");
+		if (page_type == SGX_PT_SECS)
+			*free_manager = 1;
+		return 5;
+	}
+
+	if (tmp_header.secinfo.flags.page_type == SGX_PT_REG ||
+	    tmp_header.secinfo.flags.page_type == SGX_PT_TCS ||
+	    tmp_header.secinfo.flags.page_type == SGX_PT_TRIM) {
+		tmp_header.eid = secs->eid;
+	}
+	else {
+		tmp_header.eid = 0;
+	}
+
+	// Copy 4KBytes SRCPGE to secure location
+	// memcpy(epc_page, srcpage, 4096); Just decrypt
+	vaslot = (void *)vaslot_epcm->enclave_address;
+	tmp_ver = vaslot[0];
+	// Does not make sense
+	// tmp_ver = tmp_ver << 32;
+
+	// Decrypt and MAC page. AES_GCM_DEC has 2 outputs, {plain text, MAC}
+	// Parameters for AES_GCM_DEC {Key, Counter, ...}
+	// Decrypted data must be copied to epc_page later
+	epc_page_buffer = kmalloc(4096, GFP_KERNEL);
+
+	decrypt_ret = emusgx_aes_128_gcm_dec(emusgx_cr_base_pk, &tmp_ver, &tmp_header, sizeof(struct emusgx_mac_header), srcpage, 4096, epc_page_buffer, pcmd->mac);
+	if (decrypt_ret == -EBADMSG) {
+		kfree(epc_page_buffer);
+		if (page_type == SGX_PT_SECS)
+			*free_manager = 1;
+		return 2;
+	}
+	if (decrypt_ret != 0) {
+		// Shit happened
+		// Crash GP(0)
+		pr_info("EmuSGX: Unexpected decryption issue\n");
+		kfree(epc_page_buffer);
+		if (page_type == SGX_PT_SECS)
+			*free_manager = 1;
+		return 5;
+	}
+
+	if (tmp_header.secinfo.flags.page_type == SGX_PT_SECS) {
+		epc_epcm->enclave_address = epc_page_buffer;
+	}
+	else if (tmp_header.secinfo.flags.page_type == SGX_PT_VA) {
+		epc_epcm->enclave_address = epc_page_buffer;
+	}
+	else {
+		// Map linaddr to the userspace
+		vsgx_check_and_unmap_page((void __user *)linaddr);
+		vsgx_map_empty_page_to_user_space((void __user *)linaddr);
+
+		if (copy_to_user((void __user *)linaddr, epc_page_buffer, 4096)) {
+			pr_info("EmuSGX: Failed to copy to linaddr in user space for eldb/eldu\n");
+			kfree(epc_page_buffer);
+			vsgx_check_and_unmap_page((void __user *)linaddr);
+			if (page_type == SGX_PT_SECS)
+				*free_manager = 1;
+			return 5;
+		}
+		kfree(epc_page_buffer);
+
+		// Create mapping record
+		vsgx_map_epc_to_vaddr((void __user *)linaddr, (uint64_t)epc_page);
+	}
+
+	// Check version before commiting
+	// Does not make sense
+	/*
+	if (vaslot[0] != 0) {
+		return 5;
+	}
+	else {
+		vaslot[0] = tmp_ver;
+	}*/
+
+	// Commit EPCM changes
+	epc_epcm->page_type = tmp_header.secinfo.flags.page_type;
+	epc_epcm->R = tmp_header.secinfo.flags.R;
+	epc_epcm->W = tmp_header.secinfo.flags.W;
+	epc_epcm->X = tmp_header.secinfo.flags.X;
+	epc_epcm->pending = tmp_header.secinfo.flags.pending;
+	epc_epcm->modified = tmp_header.secinfo.flags.modified;
+	if (tmp_header.secinfo.flags.page_type != SGX_PT_SECS
+	    && tmp_header.secinfo.flags.page_type != SGX_PT_VA) {
+		epc_epcm->enclave_address = (void *)tmp_header.linaddr;
+	}
+
+	if (block && ((tmp_header.secinfo.flags.page_type != SGX_PT_SECS) && (tmp_header.secinfo.flags.page_type != SGX_PT_VA))) {
+		epc_epcm->blocked = 1;
+	}
+	else {
+		epc_epcm->blocked = 0;
+	}
+
+	if (tmp_header.secinfo.flags.page_type == SGX_PT_SECS) {
+		current->manager_entry->secs_pa = epc_page; // Use physical address of SECS
+		((struct sgx_secs *)epc_page_buffer)->manager_entry = (uint64_t)current->manager_entry;
+
+		// Don't change EID
+	}
+	if (tmp_header.secinfo.flags.page_type == SGX_PT_VA) {
+		// VA does not have a manager_entry
+		epc_epcm->manager_entry = NULL;
+	}
+	else {
+		epc_epcm->manager_entry = current->manager_entry;
+	}
+
+	epc_epcm->valid = 1;
+
+	return 0;
+}
+
+// return rax value
+uint64_t emusgx_validate_and_do_remote_for_emodpr(void *epc_page, uint8_t R, uint8_t W, uint8_t X) {
+	struct emusgx_epcm *epc_epcm;
+	struct sgx_secs *tmp_secs_pa;
+
+	// Check within EPC
+	if (emusgx_check_within_epc(epc_page)) {
+		return EMUSGX_PF_RCX;
+	}
+
+	// Check concurrency with SGX1 or SGX2 instructions on the EPC page
+	// Ignored
+
+	epc_epcm = emusgx_get_epcm(epc_page);
+
+	if (epc_epcm->valid == 0) {
+		return EMUSGX_PF_RCX;
+	}
+
+	// Check the EPC page for concurrency
+	// Ignored
+	// Fail = return EMUSGX_LOCKFAIL;
+
+	if (epc_epcm->pending != 0 || epc_epcm->modified != 0) {
+		return EMUSGX_PAGE_NOT_MODIFIABLE;
+	}
+
+	if (epc_epcm->page_type != SGX_PT_REG) {
+		return EMUSGX_PF_RCX;
+	}
+
+	tmp_secs_pa = (void *)epc_epcm->enclave_secs;
+
+	if (!emusgx_is_secs_inited(tmp_secs_pa)) {
+		return EMUSGX_GP;
+	}
+
+	// Check concurrency with ETRACK
+	// Ignored
+
+	// Update EPCM permissions
+	epc_epcm->R = epc_epcm->R && R;
+	epc_epcm->W = epc_epcm->R && W;
+	epc_epcm->X = epc_epcm->R && X;
+
+	return 0;
+}
+
+// return rax value
+uint64_t emusgx_validate_and_do_remote_for_emodt(void *epc_page, uint8_t R, uint8_t W, uint8_t page_type) {
+	struct emusgx_epcm *epc_epcm;
+	struct sgx_secs *tmp_secs_pa;
+
+	// Check within EPC
+	if (emusgx_check_within_epc(epc_page)) {
+		return EMUSGX_PF_RCX;
+	}
+
+	// Check concurrency with SGX1 instructions on the EPC page
+	// Ignored
+
+	epc_epcm = emusgx_get_epcm(epc_page);
+
+	if (epc_epcm->valid == 0 || !(epc_epcm->page_type == SGX_PT_REG || epc_epcm->page_type == SGX_PT_TCS)) {
+		return EMUSGX_PF_RCX;
+	}
+
+	// Check the EPC page for concurrency
+	// Ignored
+
+	// Check for mis-configured SECINFO flags
+	if (epc_epcm->R == 0 && R == 0 && W != 0) {
+		return EMUSGX_LOCKFAIL;
+	}
+
+	if (epc_epcm->pending != 0 || epc_epcm->modified != 0) {
+		return EMUSGX_PAGE_NOT_MODIFIABLE;
+	}
+
+	tmp_secs_pa = (void *)epc_epcm->enclave_secs;
+
+	if (!emusgx_is_secs_inited(tmp_secs_pa)) {
+		return EMUSGX_GP;
+	}
+
+	// Check concurrency with ETRACK
+	// Ignored
+
+	// Update EPCM fields
+	epc_epcm->modified = 1;
+	epc_epcm->R = 0;
+	epc_epcm->W = 0;
+	epc_epcm->X = 0;
+	epc_epcm->page_type = page_type;
+
+	return 0;
+}
+
+// return 0 if good
+// return 1 if pf on EPC page
+uint8_t emusgx_validate_and_do_remote_for_epa(void *epc_page) {
+	struct emusgx_epcm *epc_epcm;
+	void *va_page;
+
+	// Check within EPC
+	if (emusgx_check_within_epc(epc_page)) {
+		pr_err("vSGX: PF: 0x%16llX not in EPC\n", (uint64_t)epc_page);
+		return 1;
+	}
+
+	epc_epcm = emusgx_get_epcm(epc_page);
+
+	// Check concurrency with other Intel SGX instructions
+	// Ignored
+
+	// Check EPC page must be empty
+	if (epc_epcm->valid != 0) {
+		pr_err("vSGX: PF: EPCM != 0 for EPA\n");
+		return 1;
+	}
+
+	// Clears EPC page
+	va_page = kmalloc(4096, GFP_KERNEL);
+	memset(va_page, 0, 4096);
+
+	epc_epcm->page_type = SGX_PT_VA;
+	epc_epcm->enclave_address = va_page; // Trick
+	epc_epcm->manager_entry = NULL; // VA does not have a manager_entry
+	epc_epcm->blocked = 0;
+	epc_epcm->pending = 0;
+	epc_epcm->modified = 0;
+	epc_epcm->R = 0;
+	epc_epcm->W = 0;
+	epc_epcm->X = 0;
+	epc_epcm->valid = 1;
+
+	return 0;
+}
+
+// return rax value
+uint64_t emusgx_validate_and_do_remote_for_eremove(void *epc_page, uint8_t *free_manager) {
+	struct emusgx_epcm *epc_epcm;
+	void __user *linaddr;
+
+	// Check within EPC
+	if (emusgx_check_within_epc(epc_page)) {
+		return EMUSGX_PF_RCX;
+	}
+
+	epc_epcm = emusgx_get_epcm(epc_page);
+
+	// Check EPC page for concurrency
+	// Ignored
+
+	// If RCX is already unused, nothing to do
+	if (epc_epcm->valid == 0 || (epc_epcm->page_type == SGX_PT_TRIM && epc_epcm->modified == 0)) {
+		return 0;
+	}
+
+	if (epc_epcm->page_type == SGX_PT_VA) {
+		epc_epcm->valid = 0;
+		return 0;
+	}
+
+	if (epc_epcm->page_type == SGX_PT_SECS) {
+		if (emusgx_secs_has_associated_page(epc_page)) {
+			return EMUSGX_CHILD_PRESENT;
+		}
+		// SECS is removed. We can now free the manager
+		*free_manager = 1;
+		epc_epcm->valid = 0;
+		return 0;
+	}
+
+	// tmp_secs = epc_epcm->enclave_secs
+
+	// if (other threads active using secs)
+	// return EMUSGX_ENCLAVE_ACT
+	// Ignored
+
+	// We now unmap the page from the manager
+	linaddr = (void __user *)epc_epcm->enclave_address;
+	vsgx_check_and_unmap_page((void __user *)linaddr);
+	vsgx_unmap_epc_from_vaddr((void __user *)linaddr);
+	epc_epcm->valid = 0;
+
+	return 0;
+}
+
+// return rax value
+uint64_t emusgx_validate_and_do_remote_for_ewb(void *epc_page, void *vaslot_pa, struct sgx_pcmd *pcmd, void *srcpage, uint64_t *linaddr, uint8_t *free_manager) {
+	struct emusgx_epcm *epc_epcm;
+	struct emusgx_epcm *va_epcm;
+	struct sgx_secs __user *tmp_secs;
+	struct emusgx_mac_header tmp_header;
+	uint64_t tmp_pcmd_enclaveid = 0;
+	uint64_t tmp_ver;
+	uint64_t *vaslot;
+	int i;
+	int encrypt_ret;
+	void *epc_page_buffer;
+
+	// Check within EPC
+	if (emusgx_check_within_epc(epc_page)) {
+		return EMUSGX_PF_RCX;
+	}
+
+	if (emusgx_check_within_epc(vaslot_pa)) {
+		return EMUSGX_PF_RDX;
+	}
+
+	epc_epcm = emusgx_get_epcm(epc_page);
+	va_epcm = emusgx_get_epcm((void *)((uint64_t)vaslot_pa & ~((uint64_t)0x0FFF)));
+
+	// Check for concurrent Intel SGX instruction access to the page
+	// Ignored
+
+	// Check if the VA page is being removed or changed
+	// Ignored
+
+	// Verify that EPCPAGE and VASLOT page are valid EPC pages and RDX is VA
+	if (epc_epcm->valid == 0) {
+		return EMUSGX_PF_RCX;
+	}
+
+	if (va_epcm->valid == 0 || va_epcm->page_type != SGX_PT_VA) {
+		return EMUSGX_PF_RDX;
+	}
+
+	// Perform page-type-specific exception checks
+	if (epc_epcm->page_type == SGX_PT_REG || epc_epcm->page_type == SGX_PT_TCS || epc_epcm->page_type == SGX_PT_TRIM) {
+		tmp_secs = (struct sgx_secs *)(emusgx_get_epcm((void *)epc_epcm->enclave_secs)->enclave_address);
+		// Check that EBLOCK has occurred correctly
+		// Ignored
+	}
+
+	// Perform page-type-specific checks
+	if (epc_epcm->page_type == SGX_PT_REG || epc_epcm->page_type == SGX_PT_TCS || epc_epcm->page_type == SGX_PT_TRIM) {
+		// Check to see if the page is evictable
+		if (epc_epcm->blocked == 0) {
+			return EMUSGX_PAGE_NOT_BLOCKED;
+		}
+
+		// Check if tracking done correctly
+		// Ignored
+
+		// Obtain EID to establish cryptiographic binding between the paged-out page and the enclave
+		tmp_header.eid = tmp_secs->eid;
+		tmp_pcmd_enclaveid = tmp_secs->eid;
+	}
+	else if (epc_epcm->page_type == SGX_PT_SECS) {
+		// Check that there are no child pages inside the enclave
+		if (emusgx_secs_has_associated_page(epc_page)) {
+			return EMUSGX_CHILD_PRESENT;
+		}
+		tmp_header.eid = 0;
+		tmp_pcmd_enclaveid = ((struct sgx_secs *)(epc_epcm->enclave_address))->eid;
+	}
+	else if (epc_epcm->page_type == SGX_PT_VA) {
+		tmp_header.eid = 0;
+		tmp_pcmd_enclaveid = 0;
+	}
+
+	// Zero out TMP_HEADER
+	memset(&tmp_header, 0 , sizeof(tmp_header));
+
+	if (epc_epcm->page_type == SGX_PT_VA || epc_epcm->page_type == SGX_PT_SECS) {
+		tmp_header.linaddr = 0;
+	}
+	else {
+		tmp_header.linaddr = (uint64_t)epc_epcm->enclave_address;
+	}
+	tmp_header.secinfo.flags.page_type = epc_epcm->page_type;
+	tmp_header.secinfo.flags.R = epc_epcm->R;
+	tmp_header.secinfo.flags.W = epc_epcm->W;
+	tmp_header.secinfo.flags.X = epc_epcm->X;
+	tmp_header.secinfo.flags.pending = epc_epcm->pending;
+	tmp_header.secinfo.flags.modified = epc_epcm->modified;
+
+	// Other fields zero
+	tmp_header.secinfo.flags.reserved = 0;
+	tmp_header.secinfo.flags.reserved2 = 0;
+	for (i = 0; i < 7; i++) {
+		tmp_header.secinfo.reserved[i] = 0;
+	}
+
+	// TODO: Should we use a randomly-generated IV?
+	tmp_ver = 0;
+	epc_page_buffer = kmalloc(4096, GFP_KERNEL);
+
+	// Encrypt the page, RCX could be encrypted in place. AES-GCM produces 2 values, {ciphertext, MAC}
+	// AES-GCM input parameters: key, GCM Counter, MAC_HDR, MAC_HDR_SIZE, SRC, SRC_SIZE
+	if (copy_from_user(epc_page_buffer, (void __user *)epc_epcm->enclave_address, 4096)) {
+		pr_info("EmuSGX: Failed to copy page from user address in ewb\n");
+		kfree(epc_page_buffer);
+		return EMUSGX_GP;
+	}
+	encrypt_ret = emusgx_aes_128_gcm_enc(emusgx_cr_base_pk, &tmp_ver, &tmp_header, sizeof(struct emusgx_mac_header), epc_page_buffer, 4096, srcpage, &(pcmd->mac[0]));
+	kfree(epc_page_buffer);
+
+	if (encrypt_ret != 0) {
+		pr_info("EmuSGX: Unexpected encryption issue\n");
+		return EMUSGX_GP;
+	}
+
+	// Write the output
+	pcmd->secinfo.flags.reserved = 0;
+	pcmd->secinfo.flags.reserved2 = 0;
+	pcmd->secinfo.flags.page_type = epc_epcm->page_type;
+	pcmd->secinfo.flags.R = epc_epcm->R;
+	pcmd->secinfo.flags.W = epc_epcm->W;
+	pcmd->secinfo.flags.X = epc_epcm->X;
+	pcmd->secinfo.flags.pending = epc_epcm->pending;
+	pcmd->secinfo.flags.modified = epc_epcm->modified;
+	for (i = 0; i < 7; i++) {
+		pcmd->secinfo.reserved[i] = 0;
+	}
+	for (i = 0; i < 40; i++) {
+		pcmd->reserved[i] = 0;
+	}
+	pcmd->enclaveid = tmp_pcmd_enclaveid;
+	*linaddr = (uint64_t)epc_epcm->enclave_address;
+
+	// Check if version array slot was empty
+	vaslot = (uint64_t *)(va_epcm->enclave_address);
+	if (vaslot[0]) {
+		return EMUSGX_VA_SLOT_OCCUPIED;
+	}
+
+	// Write version to Version Array slot
+	vaslot[0] = tmp_ver;
+
+	// Unmap the page
+	if (epc_epcm->page_type == SGX_PT_VA || epc_epcm->page_type == SGX_PT_SECS) {
+		// Free the page
+		kfree((void *)(epc_epcm->enclave_address));
+	}
+	else {
+		vsgx_check_and_unmap_page((void __user *)(epc_epcm->enclave_address));
+		vsgx_unmap_epc_from_vaddr((void __user *)(epc_epcm->enclave_address));
+	}
+
+	// Free up EPCM Entry
+	epc_epcm->valid = 0;
+	
+	if (epc_epcm->page_type == SGX_PT_SECS) {
+		// After SECS is written back, the manager is freed
+		*free_manager = 1;
+	}
+
+	return 0;
+}
+
+void emusgx_do_remote_for_encls(uint8_t instr, void *package) {
+	struct emusgx_raw_response response;
+	struct emusgx_raw_response_with_page* response_with_page;
+	uint8_t with_page = 0;
+	uint8_t free_manager = 0;
+
+	response.instr = instr;
+	response.write_back = 0;
+
+	// A huge switch on instr
+	switch(instr) {
+	case EMUSGX_S_ECREATE:
+		// The global dispatcher will occupy the current manager for us
+		response.response = emusgx_validate_and_do_remote_for_ecreate(&(((struct emusgx_ecreate_package *)package)->srcpage[0]), (void *)(((struct emusgx_ecreate_package *)package)->epc_page));
+		if (response.response != 0) {
+			// This manager should be set back to free
+			// due to that ECREATE failed to create a 
+			// new enclave
+			emusgx_free_current_manager();
+		}
+		break;
+	case EMUSGX_S_EADD:
+		response.response = emusgx_validate_and_do_remote_for_eadd((void *)(((struct emusgx_eadd_package *)package)->secs), (void *)(((struct emusgx_eadd_package *)package)->epc_page), (void *)(((struct emusgx_eadd_package *)package)->linaddr), &(((struct emusgx_eadd_package *)package)->srcpage[0]), &(((struct emusgx_eadd_package *)package)->secinfo));
+		break;
+	case EMUSGX_S_EINIT:
+		response.response = emusgx_validate_and_do_remote_for_einit(&(((struct emusgx_einit_package *)package)->sigstruct), (void *)(((struct emusgx_einit_package *)package)->secs), &(((struct emusgx_einit_package *)package)->einittoken));
+		break;
+	case EMUSGX_S_EREMOVE:
+		response.response = emusgx_validate_and_do_remote_for_eremove((void *)(((struct emusgx_eremove_package *)package)->epc_page), &free_manager);
+		if (free_manager) {
+			// SECS freed. The enclave is terminated, can be
+			// freed
+			emusgx_free_current_manager();
+		}
+		break;
+	case EMUSGX_S_EEXTEND:
+		response.response = emusgx_validate_and_do_remote_for_eextend((void *)(((struct emusgx_eextend_package *)package)->addr));
+		break;
+	case EMUSGX_S_ELDB:
+	case EMUSGX_S_ELDU:
+		response.response = emusgx_validate_and_do_remote_for_eldb_eldu(&(((struct emusgx_eldb_eldu_package *)package)->srcpage[0]), (void *)(((struct emusgx_eldb_eldu_package *)package)->secs), (void *)(((struct emusgx_eldb_eldu_package *)package)->vaslot), (void *)(((struct emusgx_eldb_eldu_package *)package)->epc_page), &(((struct emusgx_eldb_eldu_package *)package)->pcmd), (((struct emusgx_eldb_eldu_package *)package)->linaddr), (((struct emusgx_eldb_eldu_package *)package)->block), &free_manager);
+		if (free_manager) {
+			// Failed to load a SECS. The manager can be
+			// freed
+			emusgx_free_current_manager();
+		}
+		break;
+	case EMUSGX_S_EBLOCK:
+		response.response = emusgx_do_remote_for_eblock((void *)(((struct emusgx_eblock_package *)package)->epc_page));
+		break;
+	case EMUSGX_S_EPA:
+		response.response = emusgx_validate_and_do_remote_for_epa((void *)(((struct emusgx_epa_package *)package)->epc_page));
+		break;
+	case EMUSGX_S_EWB:
+		response_with_page = kmalloc(sizeof(struct emusgx_raw_response_with_page), GFP_KERNEL);
+		if (response_with_page == NULL) {
+			pr_info("EmuSGX: Failed to allocate a response with page write back\n");
+			response.response = EMUSGX_GP;
+		}
+		response_with_page->response = emusgx_validate_and_do_remote_for_ewb((void *)(((struct emusgx_ewb_package *)package)->epc_page), (void *)(((struct emusgx_ewb_package *)package)->vaslot), &(response_with_page->pcmd), response_with_page->page, &(response_with_page->linaddr), &free_manager);
+		if (response_with_page->response != 0) {
+			// Use minimum response to save transfer data
+			response.response = response_with_page->response;
+			kfree(response_with_page);
+		}
+		else {
+			response_with_page->instr = instr;
+			response_with_page->write_back = 1;
+			with_page = 1;
+		}
+		if (free_manager) {
+			// SECS freed. The enclave is terminated, can be
+			// freed
+			emusgx_free_current_manager();
+		}
+		break;
+	case EMUSGX_S_EAUG:
+		response.response = emusgx_validate_and_do_remote_for_eaug((void *)(((struct emusgx_eaug_package *)package)->secs), (void *)(((struct emusgx_eaug_package *)package)->linaddr), (void *)(((struct emusgx_eaug_package *)package)->epc_addr));
+		break;
+	case EMUSGX_S_EMODPR:
+		response.response = emusgx_validate_and_do_remote_for_emodpr((void *)(((struct emusgx_emodpr_package *)package)->epc_page), (((struct emusgx_emodpr_package *)package)->R), (((struct emusgx_emodpr_package *)package)->W), (((struct emusgx_emodpr_package *)package)->X));
+		break;
+	case EMUSGX_S_EMODT:
+		response.response = emusgx_validate_and_do_remote_for_emodt((void *)(((struct emusgx_emodt_package *)package)->epc_page), (((struct emusgx_emodt_package *)package)->R), (((struct emusgx_emodt_package *)package)->W), (((struct emusgx_emodt_package *)package)->page_type));
+		break;
+	default:
+		// False package
+		// Ignore it
+		return;
+	}
+
+
+	if (with_page) {
+		if (emusgx_send_data(response_with_page, sizeof(struct emusgx_raw_response_with_page))) {
+			pr_info("EmuSGX: Failed to response with page\n");
+		}
+		kfree(response_with_page);
+	}
+	else {
+		if (emusgx_send_data(&response, sizeof(struct emusgx_raw_response))) {
+			pr_info("EmuSGX: Failed to response\n");
+		}
+	}
+}
\ No newline at end of file
diff --git a/kernel/emusgx/enclu.c b/kernel/emusgx/enclu.c
new file mode 100644
index 000000000..5823a07fb
--- /dev/null
+++ b/kernel/emusgx/enclu.c
@@ -0,0 +1,1003 @@
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/slab.h> 
+#include <linux/uaccess.h>
+#include <linux/string.h>
+
+#include <emusgx/emusgx.h>
+#include <emusgx/emusgx_fault.h>
+#include <emusgx/emusgx_internal.h>
+#include <emusgx/emusgx_sender.h>
+#include <emusgx/emusgx_debug.h>
+
+void emusgx_ereport(struct emusgx_regs *reg_status, struct pt_regs *ptrace_regs) {
+	// RBX targetinfo
+	// RCX address of report data 
+	// RDX output data
+	struct sgx_targetinfo __user *targetinfo = (struct sgx_targetinfo *)(reg_status->rbx);
+	void *targetinfo_pa = (void *)vsgx_vaddr_to_paddr(targetinfo, current->manager_entry->manager_nr);
+	uint8_t __user *reportdata = (uint8_t *)(reg_status->rcx);
+	void *reportdata_pa = (void *)vsgx_vaddr_to_paddr(reportdata, current->manager_entry->manager_nr);
+	struct sgx_report *tmp_report;
+	struct sgx_report *outputdata = (struct sgx_report*)(reg_status->rdx);
+	void *outputdata_pa = (void *)vsgx_vaddr_to_paddr(outputdata, current->manager_entry->manager_nr);
+	uint64_t *tmp_reportkey;
+	uint64_t *tmp_mac;
+	struct emusgx_tmp_key_dependencies *tmp_keydependencies;
+	int i;
+
+	struct emusgx_epcm *targetinfo_epcm;
+	struct emusgx_epcm *reportdata_epcm;
+	struct emusgx_epcm *outputdata_epcm;
+
+	// Address verification for TARGETINFO
+	if (reg_status->rbx % 128 != 0) {
+		emusgx_gp(0, ptrace_regs);
+	}
+
+	if (!vsgx_check_in_elrange(targetinfo)) {
+		emusgx_gp(0, ptrace_regs);
+	}
+
+	// Check within EPC
+	if (emusgx_check_within_epc(targetinfo_pa)) {
+		emusgx_pf(targetinfo, ptrace_regs);
+	}
+
+	targetinfo_epcm = emusgx_get_epcm(targetinfo_pa);
+
+	if (targetinfo_epcm->valid == 0) {
+		emusgx_pf(targetinfo, ptrace_regs);
+	}
+
+	if (targetinfo_epcm->blocked == 1) {
+		emusgx_pf(targetinfo, ptrace_regs);
+	}
+
+	// Check page parameters for correctness
+	if ((targetinfo_epcm->page_type != SGX_PT_REG) || (targetinfo_epcm->enclave_secs != current->secs_pa) || (targetinfo_epcm->pending == 1) ||
+		(targetinfo_epcm->modified == 1) || ((unsigned long)(targetinfo_epcm->enclave_address) != (reg_status->rbx - (reg_status->rbx % 0x1000))) ||
+		(targetinfo_epcm->R == 0)) {
+		emusgx_pf(targetinfo, ptrace_regs);
+	}
+
+	// Copy TARGETINFO
+	targetinfo = kmalloc(sizeof(struct sgx_targetinfo), GFP_KERNEL);
+	if (copy_from_user(targetinfo, (struct sgx_targetinfo *)(reg_status->rbx), sizeof(struct sgx_targetinfo))) {
+		pr_info("EmuSGX: Failed to copy from user\n");
+	}
+
+	// Address verification for REPORTDATA
+	if (reg_status->rcx % 128 != 0) {
+		emusgx_gp(0, ptrace_regs);
+	}
+
+	if (!vsgx_check_in_elrange(reportdata)) {
+		emusgx_gp(0, ptrace_regs);
+	}
+
+	// Check within EPC
+	if (emusgx_check_within_epc(reportdata_pa)) {
+		emusgx_pf(reportdata, ptrace_regs);
+	}
+
+	reportdata_epcm = emusgx_get_epcm(reportdata_pa);
+
+	if (reportdata_epcm->valid == 0) {
+		emusgx_pf(reportdata, ptrace_regs);
+	}
+
+	if (reportdata_epcm->blocked == 1) {
+		emusgx_pf(reportdata, ptrace_regs);
+	}
+
+	// Check page parameters for correctness
+	if ((reportdata_epcm->page_type != SGX_PT_REG) || (reportdata_epcm->enclave_secs != current->secs_pa) || (reportdata_epcm->pending == 1) ||
+		(reportdata_epcm->modified == 1) || ((unsigned long)(reportdata_epcm->enclave_address) != (reg_status->rcx - (reg_status->rcx % 0x1000))) ||
+		(reportdata_epcm->R == 0)) {
+		emusgx_pf(reportdata, ptrace_regs);
+	}
+	// Copy of REPORTDATA will be done by directly copying the result into report
+
+	// Address verification for OUTPUTDATA
+	if (reg_status->rdx % 512 != 0) {
+		emusgx_gp(0, ptrace_regs);
+	}
+
+	if (!vsgx_check_in_elrange(outputdata)) {
+		emusgx_gp(0, ptrace_regs);
+	}
+
+	// Check within EPC
+	if (emusgx_check_within_epc(outputdata_pa)) {
+		emusgx_pf(outputdata, ptrace_regs);
+	}
+
+	outputdata_epcm = emusgx_get_epcm(outputdata_pa);
+
+	if (outputdata_epcm->valid == 0) {
+		emusgx_pf(outputdata, ptrace_regs);
+	}
+
+	if (outputdata_epcm->blocked == 1) {
+		emusgx_pf(outputdata, ptrace_regs);
+	}
+
+	// Check page parameters for correctness
+	if ((outputdata_epcm->page_type != SGX_PT_REG) || (outputdata_epcm->enclave_secs != current->secs_pa) ||
+		((unsigned long)(outputdata_epcm->enclave_address) != (reg_status->rdx - (reg_status->rdx % 0x1000))) ||
+		(outputdata_epcm->W == 0)) {
+		emusgx_pf(outputdata, ptrace_regs);
+	}
+
+	// REPORT MAC needs to be computed over data which cannot be modified
+	tmp_report = kmalloc(sizeof(struct sgx_report), GFP_KERNEL);
+	memset(tmp_report, 0, sizeof(struct sgx_report));
+	tmp_report->cpusvn[0] = emusgx_cr_cpusvn[0];
+	tmp_report->cpusvn[1] = emusgx_cr_cpusvn[1];
+	tmp_report->isvprodid = current->secs->isvprodid;
+	tmp_report->isvsvn = current->secs->isvsvn;
+	tmp_report->attributes[0] = current->secs->attributes;
+	tmp_report->attributes[1] = current->secs->xfrm;
+	if (copy_from_user(tmp_report->reportdata, reportdata, 64)) {
+		// Failed to read
+		pr_err("vSGX: Failed to read report data\n");
+		emusgx_pf(reportdata, ptrace_regs);
+	}
+	for (i = 0; i < 8; i++) {
+		tmp_report->mrenclave[i] = current->secs->mrenclave[i];
+	}
+	for (i = 0; i < 8; i++) {
+		tmp_report->mrsigner[i] = current->secs->mrsigner[i];
+	}
+	for (i = 0; i < 4; i++) {
+		tmp_report->keyid[i] = emusgx_cr_report_keyid[i];
+	}
+	tmp_report->miscselect = current->secs->miscselect;
+
+	tmp_keydependencies = kmalloc(sizeof(struct emusgx_tmp_key_dependencies), GFP_KERNEL);
+
+	// Derive the report key
+	tmp_keydependencies->keyname = SGX_REPORT_KEY;
+	tmp_keydependencies->isvprodid = 0;
+	tmp_keydependencies->isvsvn = 0;
+	tmp_keydependencies->ownerepoch[0] =emusgx_csr_owner_epoch[0];
+	tmp_keydependencies->ownerepoch[1] =emusgx_csr_owner_epoch[1];
+	tmp_keydependencies->attributes[0] = current->secs->attributes;
+	tmp_keydependencies->attributes[1] = current->secs->xfrm;
+	tmp_keydependencies->attributesmask[0] = 0;
+	tmp_keydependencies->attributesmask[1] = 0;
+	for (i = 0; i < 8; i++) {
+		tmp_keydependencies->mrenclave[i] = targetinfo->measurement[i];
+	}
+	for (i = 0; i < 8; i++) {
+		tmp_keydependencies->mrsigner[i] = 0;
+	}
+	for (i = 0; i < 4; i++) {
+		tmp_keydependencies->keyid[i] = tmp_report->keyid[i];
+	}
+	tmp_keydependencies->seal_key_fuses[0] = emusgx_cr_seal_fuses[0];
+	tmp_keydependencies->seal_key_fuses[1] = emusgx_cr_seal_fuses[1];
+	tmp_keydependencies->cpusvn[0] = emusgx_cr_cpusvn[0];
+	tmp_keydependencies->cpusvn[1] = emusgx_cr_cpusvn[1];
+	for (i = 0; i < 352; i++) {
+		tmp_keydependencies->padding[i] = current->secs->padding[i];
+	}
+	tmp_keydependencies->miscselect = targetinfo->miscselect;
+	tmp_keydependencies->miscmask = 0;
+	
+	// Calculate the derived key
+	// 128 bit
+	// pr_info("EmuSGX: Get key\n");
+	tmp_reportkey = emusgx_derive_key(tmp_keydependencies);
+
+	kfree(tmp_keydependencies);
+
+	// call cryptographic CMAC function
+	// pr_info("EmuSGX: Got key\n");
+	tmp_mac = emusgx_cmac(tmp_reportkey, tmp_report, 384);
+	tmp_report->mac[0] = tmp_mac[0];
+	tmp_report->mac[1] = tmp_mac[1];
+	// pr_info("EmuSGX: CMACed\n");
+
+	tmp_reportkey[0] = 0;
+	tmp_reportkey[1] = 0;
+	kfree(tmp_reportkey);
+	tmp_mac[0] = 0;
+	tmp_mac[1] = 0;
+	kfree(tmp_mac);
+
+	// Copy tmp report to output data
+	// pr_info("EmuSGX: Copy\n");
+	if (copy_to_user(outputdata, tmp_report, 432) != 0) {
+		pr_info("EmuSGX: Failed to copy to user\n");
+	}
+	// pr_info("EmuSGX: Copied\n");
+
+	memset(targetinfo, 0, sizeof(struct sgx_targetinfo));
+	kfree(targetinfo);
+}
+
+void emusgx_egetkey(struct emusgx_regs *reg_status, struct pt_regs *ptrace_regs) {
+	struct sgx_keyrequest *keyrequest = (struct sgx_keyrequest *)(reg_status->rbx);
+	void *keyrequest_pa = (void *)vsgx_vaddr_to_paddr(keyrequest, current->manager_entry->manager_nr);
+	void *outputdata = (void *)(reg_status->rcx);
+	void *outputdata_pa = (void *)vsgx_vaddr_to_paddr(outputdata, current->manager_entry->manager_nr);
+	uint64_t *derived_key;
+	uint64_t tmp_attributes[2];
+	uint32_t tmp_micselect;
+	uint32_t tmp_mrenclave[8];
+	uint32_t tmp_mrsigner[8];
+	struct emusgx_tmp_key_dependencies *tmp_keydependencies;
+	int i;
+
+	struct emusgx_epcm *keyrequest_epcm;
+	struct emusgx_epcm *outputdata_epcm;
+
+	// Make sure KEYREQUEST is properly aligned
+	if (reg_status->rbx % 128 != 0) {
+		emusgx_gp(0, ptrace_regs);
+	}
+
+	if (!vsgx_check_in_elrange(keyrequest)) {
+		emusgx_gp(0, ptrace_regs);
+	}
+
+	// Check within EPC
+	if (emusgx_check_within_epc(keyrequest_pa)) {
+		emusgx_pf(keyrequest, ptrace_regs);
+	}
+
+	keyrequest_epcm = emusgx_get_epcm(keyrequest_pa);
+
+	if (keyrequest_epcm->valid == 0) {
+		emusgx_pf(keyrequest, ptrace_regs);
+	}
+
+	if (keyrequest_epcm->blocked == 1) {
+		emusgx_pf(keyrequest, ptrace_regs);
+	}
+
+	// Check page parameters for correctness
+	if ((keyrequest_epcm->page_type != SGX_PT_REG) || (keyrequest_epcm->enclave_secs != current->secs_pa) ||
+		(keyrequest_epcm->pending == 1) || (keyrequest_epcm->modified == 1) ||
+		((unsigned long)(keyrequest_epcm->enclave_address) != (reg_status->rbx - (reg_status->rbx % 0x1000))) ||
+		(keyrequest_epcm->R == 0)) {
+		emusgx_pf(keyrequest, ptrace_regs);
+	}
+
+	// Make sure OUTPUTDATA is properly aligned
+	if (reg_status->rcx % 16 != 0) {
+		emusgx_gp(0, ptrace_regs);
+	}
+	
+	if (!vsgx_check_in_elrange(outputdata)) {
+		emusgx_gp(0, ptrace_regs);
+	}
+
+	if (emusgx_check_within_epc(outputdata_pa)) {
+		emusgx_pf(outputdata, ptrace_regs);
+	}
+
+	outputdata_epcm = emusgx_get_epcm(outputdata_pa);
+
+	if (outputdata_epcm->valid == 0) {
+		emusgx_pf(outputdata, ptrace_regs);
+	}
+
+	if (outputdata_epcm->blocked == 1) {
+		emusgx_pf(outputdata, ptrace_regs);
+	}
+
+	// Check page parameters for correctness
+	if ((outputdata_epcm->page_type != SGX_PT_REG) || (outputdata_epcm->enclave_secs != current->secs_pa) ||
+		(outputdata_epcm->pending == 1) || (outputdata_epcm->modified == 1) ||
+		((unsigned long)(outputdata_epcm->enclave_address) != (reg_status->rcx - (reg_status->rcx % 0x1000))) ||
+		(outputdata_epcm->W == 0)) {
+		emusgx_pf(outputdata, ptrace_regs);
+	}
+
+	// Copy KEYREQUEST into the kernel space
+	keyrequest = kmalloc(sizeof(struct sgx_keyrequest), GFP_KERNEL);
+	if (copy_from_user(keyrequest, (struct sgx_keyrequest *)(reg_status->rbx), sizeof(struct sgx_keyrequest))) {
+		pr_info("EmuSGX: Failed to copy from user\n");
+	}
+
+	// Verify RESERVED spaces in KEYREQUEST are valid
+	if (keyrequest->reserved != 0 || keyrequest->policy.reserved != 0) {
+		emusgx_pf(keyrequest, ptrace_regs);
+	}
+
+	// Determine which enclave attributes that must be included in the key
+	tmp_attributes[0] = (0x3 | keyrequest->attributemask[0]) & (current->secs->attributes);
+	tmp_attributes[1] = keyrequest->attributemask[1] & (current->secs->xfrm);
+
+	// Compute MISCSELECT fields to be included
+	tmp_micselect = keyrequest->miscmask & (current->secs->miscselect);
+
+	// HUGE SWITCH CASE
+	tmp_keydependencies = kmalloc(sizeof(struct emusgx_tmp_key_dependencies), GFP_KERNEL);
+	switch(keyrequest->keyname) {
+	case SGX_SEAL_KEY:
+		if (emusgx_compare_cpusvn(keyrequest->cpusvn, emusgx_get_cpusvn())) {
+			reg_status->flags.ZF = 1;
+			reg_status->rax = EMUSGX_INVALID_CPUSVN;
+			goto EXIT;
+		}
+		if (keyrequest->isvsvn > current->secs->isvsvn) {
+			reg_status->flags.ZF = 1;
+			reg_status->rax = EMUSGX_INVALID_ISVSVN;
+			goto EXIT;
+		}
+		// include enclave identity?
+		for (i = 0; i < 8; i++) {
+			tmp_mrenclave[i] = 0;
+		}
+		if (keyrequest->policy.mrsigner) {
+			for (i = 0; i < 8; i++) {
+				tmp_mrenclave[i] = current->secs->mrenclave[i];
+			}
+		}
+		// include enclave author?
+		for (i = 0; i < 8; i++) {
+			tmp_mrsigner[i] = 0;
+		}
+		if (keyrequest->policy.mrsigner) {
+			for (i = 0; i < 8; i++) {
+				tmp_mrsigner[i] = current->secs->mrsigner[i];
+			}
+		}
+
+		// Determine values key is based on
+		tmp_keydependencies->keyname = SGX_SEAL_KEY;
+		tmp_keydependencies->isvprodid = current->secs->isvprodid;
+		tmp_keydependencies->isvsvn = keyrequest->isvsvn;
+		tmp_keydependencies->ownerepoch[0] =emusgx_csr_owner_epoch[0];
+		tmp_keydependencies->ownerepoch[1] =emusgx_csr_owner_epoch[1];
+		tmp_keydependencies->attributes[0] = tmp_attributes[0];
+		tmp_keydependencies->attributes[1] = tmp_attributes[1];
+		tmp_keydependencies->attributesmask[0] = keyrequest->attributemask[0];
+		tmp_keydependencies->attributesmask[1] = keyrequest->attributemask[1];
+		for (i = 0; i < 8; i++) {
+			tmp_keydependencies->mrenclave[i] = tmp_mrenclave[i];
+		}
+		for (i = 0; i < 8; i++) {
+			tmp_keydependencies->mrsigner[i] = tmp_mrsigner[i];
+		}
+		for (i = 0; i < 4; i++) {
+			tmp_keydependencies->keyid[i] = keyrequest->keyid[i];
+		}
+		tmp_keydependencies->seal_key_fuses[0] = emusgx_cr_seal_fuses[0];
+		tmp_keydependencies->seal_key_fuses[1] = emusgx_cr_seal_fuses[1];
+		tmp_keydependencies->cpusvn[0] = keyrequest->cpusvn[0];
+		tmp_keydependencies->cpusvn[1] = keyrequest->cpusvn[1];
+		for (i = 0; i < 352; i++) {
+			tmp_keydependencies->padding[i] = current->secs->padding[i];
+		}
+		tmp_keydependencies->miscselect = tmp_micselect;
+		tmp_keydependencies->miscmask = ~(keyrequest->miscmask);
+		break;
+	case SGX_REPORT_KEY:
+		tmp_keydependencies->keyname = SGX_REPORT_KEY;
+		tmp_keydependencies->isvprodid = 0;
+		tmp_keydependencies->isvsvn = 0;
+		tmp_keydependencies->ownerepoch[0] =emusgx_csr_owner_epoch[0];
+		tmp_keydependencies->ownerepoch[1] =emusgx_csr_owner_epoch[1];
+		tmp_keydependencies->attributes[0] = current->secs->attributes;
+		tmp_keydependencies->attributes[1] = current->secs->xfrm;
+		tmp_keydependencies->attributesmask[0] = 0;
+		tmp_keydependencies->attributesmask[1] = 0;
+		for (i = 0; i < 8; i++) {
+			tmp_keydependencies->mrenclave[i] = current->secs->mrenclave[i];
+		}
+		for (i = 0; i < 8; i++) {
+			tmp_keydependencies->mrsigner[i] = 0;
+		}
+		for (i = 0; i < 4; i++) {
+			tmp_keydependencies->keyid[i] = keyrequest->keyid[i];
+		}
+		tmp_keydependencies->seal_key_fuses[0] = emusgx_cr_seal_fuses[0];
+		tmp_keydependencies->seal_key_fuses[1] = emusgx_cr_seal_fuses[1];
+		tmp_keydependencies->cpusvn[0] = keyrequest->cpusvn[0];
+		tmp_keydependencies->cpusvn[1] = keyrequest->cpusvn[1];
+		for (i = 0; i < 352; i++) {
+			tmp_keydependencies->padding[i] = HARDCODED_PKCS1_5_PADDING[i];
+		}
+		tmp_keydependencies->miscselect = tmp_micselect;
+		tmp_keydependencies->miscmask = 0;
+		break;
+	case SGX_LAUNCH_KEY:
+		if (current->secs->attribute.einittokenkey == 0) {
+			reg_status->flags.ZF = 1;
+			reg_status->rax = EMUSGX_INVALID_ATTRIBUTE;
+			kfree(tmp_keydependencies);
+			goto EXIT;
+		}
+		if (emusgx_compare_cpusvn(keyrequest->cpusvn, emusgx_get_cpusvn())) {
+			reg_status->flags.ZF = 1;
+			reg_status->rax = EMUSGX_INVALID_CPUSVN;
+			kfree(tmp_keydependencies);
+			goto EXIT;
+		}
+		if (keyrequest->isvsvn > current->secs->isvsvn) {
+			reg_status->flags.ZF = 1;
+			reg_status->rax = EMUSGX_INVALID_ISVSVN;
+			kfree(tmp_keydependencies);
+			goto EXIT;
+		}
+		// Determine values key is based on
+		tmp_keydependencies->keyname = SGX_LAUNCH_KEY;
+		tmp_keydependencies->isvprodid = current->secs->isvprodid;
+		tmp_keydependencies->isvsvn = keyrequest->isvsvn;
+		tmp_keydependencies->ownerepoch[0] =emusgx_csr_owner_epoch[0];
+		tmp_keydependencies->ownerepoch[1] =emusgx_csr_owner_epoch[1];
+		tmp_keydependencies->attributes[0] = tmp_attributes[0];
+		tmp_keydependencies->attributes[1] = tmp_attributes[1];
+		tmp_keydependencies->attributesmask[0] = 0;
+		tmp_keydependencies->attributesmask[1] = 0;
+		for (i = 0; i < 8; i++) {
+			tmp_keydependencies->mrenclave[i] = 0;
+		}
+		for (i = 0; i < 8; i++) {
+			tmp_keydependencies->mrsigner[i] = 0;
+		}
+		for (i = 0; i < 4; i++) {
+			tmp_keydependencies->keyid[i] = keyrequest->keyid[i];
+		}
+		tmp_keydependencies->seal_key_fuses[0] = emusgx_cr_seal_fuses[0];
+		tmp_keydependencies->seal_key_fuses[1] = emusgx_cr_seal_fuses[1];
+		tmp_keydependencies->cpusvn[0] = keyrequest->cpusvn[0];
+		tmp_keydependencies->cpusvn[1] = keyrequest->cpusvn[1];
+		for (i = 0; i < 352; i++) {
+			tmp_keydependencies->padding[i] = current->secs->padding[i];
+		}
+		tmp_keydependencies->miscselect = tmp_micselect;
+		tmp_keydependencies->miscmask = 0;
+		break;
+	case SGX_PROVISION_KEY:
+		if (current->secs->attribute.provisionkey == 0) {
+			reg_status->flags.ZF = 1;
+			reg_status->rax = EMUSGX_INVALID_ATTRIBUTE;
+			kfree(tmp_keydependencies);
+			goto EXIT;
+		}
+		if (emusgx_compare_cpusvn(keyrequest->cpusvn, emusgx_get_cpusvn())) {
+			reg_status->flags.ZF = 1;
+			reg_status->rax = EMUSGX_INVALID_CPUSVN;
+			kfree(tmp_keydependencies);
+			goto EXIT;
+		}
+		if (keyrequest->isvsvn > current->secs->isvsvn) {
+			reg_status->flags.ZF = 1;
+			reg_status->rax = EMUSGX_INVALID_ISVSVN;
+			kfree(tmp_keydependencies);
+			goto EXIT;
+		}
+		// Determine values key is based on
+		tmp_keydependencies->keyname = SGX_PROVISION_KEY;
+		tmp_keydependencies->isvprodid = current->secs->isvprodid;
+		tmp_keydependencies->isvsvn = keyrequest->isvsvn;
+		tmp_keydependencies->ownerepoch[0] = 0;
+		tmp_keydependencies->ownerepoch[1] = 0;
+		tmp_keydependencies->attributes[0] = tmp_attributes[0];
+		tmp_keydependencies->attributes[1] = tmp_attributes[1];
+		tmp_keydependencies->attributesmask[0] = keyrequest->attributemask[0];
+		tmp_keydependencies->attributesmask[1] = keyrequest->attributemask[1];
+		for (i = 0; i < 8; i++) {
+			tmp_keydependencies->mrenclave[i] = 0;
+		}
+		for (i = 0; i < 8; i++) {
+			tmp_keydependencies->mrsigner[i] = current->secs->mrsigner[i];
+		}
+		for (i = 0; i < 4; i++) {
+			tmp_keydependencies->keyid[i] = 0;
+		}
+		tmp_keydependencies->seal_key_fuses[0] = 0;
+		tmp_keydependencies->seal_key_fuses[1] = 0;
+		tmp_keydependencies->cpusvn[0] = keyrequest->cpusvn[0];
+		tmp_keydependencies->cpusvn[1] = keyrequest->cpusvn[1];
+		for (i = 0; i < 352; i++) {
+			tmp_keydependencies->padding[i] = current->secs->padding[i];
+		}
+		tmp_keydependencies->miscselect = tmp_micselect;
+		tmp_keydependencies->miscmask = ~(keyrequest->miscmask);
+		break;
+	case SGX_PROVISION_SEAL_KEY:
+		if (current->secs->attribute.provisionkey == 0) {
+			reg_status->flags.ZF = 1;
+			reg_status->rax = EMUSGX_INVALID_ATTRIBUTE;
+			kfree(tmp_keydependencies);
+			goto EXIT;
+		}
+		if (emusgx_compare_cpusvn(keyrequest->cpusvn, emusgx_get_cpusvn())) {
+			reg_status->flags.ZF = 1;
+			reg_status->rax = EMUSGX_INVALID_CPUSVN;
+			kfree(tmp_keydependencies);
+			goto EXIT;
+		}
+		if (keyrequest->isvsvn > current->secs->isvsvn) {
+			reg_status->flags.ZF = 1;
+			reg_status->rax = EMUSGX_INVALID_ISVSVN;
+			kfree(tmp_keydependencies);
+			goto EXIT;
+		}
+		// Determine values key is based on
+		tmp_keydependencies->keyname = SGX_PROVISION_SEAL_KEY;
+		tmp_keydependencies->isvprodid = current->secs->isvprodid;
+		tmp_keydependencies->isvsvn = keyrequest->isvsvn;
+		tmp_keydependencies->ownerepoch[0] = 0;
+		tmp_keydependencies->ownerepoch[1] = 0;
+		tmp_keydependencies->attributes[0] = tmp_attributes[0];
+		tmp_keydependencies->attributes[1] = tmp_attributes[1];
+		tmp_keydependencies->attributesmask[0] = keyrequest->attributemask[1];
+		tmp_keydependencies->attributesmask[1] = keyrequest->attributemask[1];
+		for (i = 0; i < 8; i++) {
+			tmp_keydependencies->mrenclave[i] = 0;
+		}
+		for (i = 0; i < 8; i++) {
+			tmp_keydependencies->mrsigner[i] = current->secs->mrsigner[i];
+		}
+		for (i = 0; i < 4; i++) {
+			tmp_keydependencies->keyid[i] = 0;
+		}
+		tmp_keydependencies->seal_key_fuses[0] = emusgx_cr_seal_fuses[0];
+		tmp_keydependencies->seal_key_fuses[1] = emusgx_cr_seal_fuses[1];
+		tmp_keydependencies->cpusvn[0] = keyrequest->cpusvn[0];
+		tmp_keydependencies->cpusvn[1] = keyrequest->cpusvn[1];
+		for (i = 0; i < 352; i++) {
+			tmp_keydependencies->padding[i] = current->secs->padding[i];
+		}
+		tmp_keydependencies->miscselect = tmp_micselect;
+		tmp_keydependencies->miscmask = ~(keyrequest->miscmask);
+		break;
+	default:
+		reg_status->flags.ZF = 1;
+		reg_status->rax = EMUSGX_INVALID_KEYNAME;
+		kfree(tmp_keydependencies);
+		goto EXIT;
+	}
+
+
+	derived_key = emusgx_derive_key(tmp_keydependencies);
+	kfree(tmp_keydependencies);
+	if (copy_to_user(outputdata, derived_key, 16) != 0) {
+		pr_info("EmuSGX: Failed to copy to user\n");
+	} // 128-bit (16 byte) key
+	derived_key[0] = 0; // clear the temp key
+	derived_key[1] = 0;
+	kfree(derived_key); // "the user is responsible for freeing the key"
+	reg_status->rax = 0;
+	reg_status->flags.ZF = 0;
+EXIT:
+
+	memset(keyrequest, 0, sizeof(struct sgx_keyrequest));
+	kfree(keyrequest);
+	reg_status->flags.CF = 0;
+	reg_status->flags.PF = 0;
+	reg_status->flags.AF = 0;
+	reg_status->flags.OF = 0;
+	reg_status->flags.SF = 0;
+}
+
+void emusgx_eenter(struct emusgx_regs *reg_status, struct pt_regs *ptrace_regs) {
+	emusgx_gp(0, ptrace_regs);
+}
+
+void emusgx_eresume(struct emusgx_regs *reg_status, struct pt_regs *ptrace_regs) {
+	emusgx_gp(0, ptrace_regs);
+}
+
+void emusgx_eaccept(struct emusgx_regs *reg_status, struct pt_regs *ptrace_regs) {
+	struct sgx_secinfo *scratch_secinfo = (struct sgx_secinfo *)(reg_status->rbx);
+	void *secinfo_pa = (void *)vsgx_vaddr_to_paddr(scratch_secinfo, current->manager_entry->manager_nr);
+	struct sgx_tcs *tmp_tcs;
+	void *epc_page_va = (void *)(reg_status->rcx);
+	void *epc_page_pa = (void *)vsgx_vaddr_to_paddr(epc_page_va, current->manager_entry->manager_nr);
+	int i;
+
+	struct emusgx_epcm *secinfo_epcm;
+	struct emusgx_epcm *epc_epcm;
+
+	// SECINFO must be 64 aligned
+	if (reg_status->rbx % 64 != 0) {
+		emusgx_gp(0, ptrace_regs);
+	}
+
+	if (!vsgx_check_in_elrange(scratch_secinfo)) {
+		emusgx_gp(0, ptrace_regs);
+	}
+
+	// Check within EPC
+	if (emusgx_check_within_epc(secinfo_pa)) {
+		emusgx_pf(scratch_secinfo, ptrace_regs);
+	}
+
+	secinfo_epcm = emusgx_get_epcm(secinfo_pa);
+
+	// Check for SECINFO EPCM
+	if ((secinfo_epcm->valid == 0) || (secinfo_epcm->R == 0) ||
+		(secinfo_epcm->pending != 0) || (secinfo_epcm->modified != 0) ||
+		(secinfo_epcm->blocked !=0) || (secinfo_epcm->page_type != SGX_PT_REG) ||
+		(secinfo_epcm->enclave_secs != current->secs_pa) || (secinfo_epcm->enclave_address != scratch_secinfo)) {
+		emusgx_pf(scratch_secinfo, ptrace_regs);
+	}
+	
+	// Copy SECINFO from user
+	scratch_secinfo = kmalloc(sizeof(struct sgx_secinfo), GFP_KERNEL);
+	if (copy_from_user(scratch_secinfo, (struct sgx_secinfo *)(reg_status->rbx), sizeof(struct sgx_secinfo))) {
+		pr_info("EmuSGX: Failed to copy from user\n");
+	}
+
+	// Check for mis-configured SECINFO flags
+	if (scratch_secinfo->flags.reserved != 0) {
+		emusgx_gp(0, ptrace_regs);
+	}
+
+	// EPC page must be 512 aligned
+	if (reg_status->rcx % 512 != 0) {
+		emusgx_gp(0, ptrace_regs);
+	}
+
+	if (!vsgx_check_in_elrange(epc_page_va)) {
+		emusgx_gp(0, ptrace_regs);
+	}
+
+	// Check within EPC
+	if (emusgx_check_within_epc(epc_page_pa)) {
+		emusgx_pf(epc_page_va, ptrace_regs);
+	}
+
+	epc_epcm = emusgx_get_epcm(epc_page_pa);
+
+	// Check that the combination of requested PT, PENDING and MODIFIED is legal
+	if (! (((scratch_secinfo->flags.page_type == SGX_PT_REG) && (scratch_secinfo->flags.modified == 0)) ||
+		((scratch_secinfo->flags.page_type == SGX_PT_TRIM) && (scratch_secinfo->flags.pending == 0) &&
+		(scratch_secinfo->flags.modified == 1)))) {
+		emusgx_gp(0, ptrace_regs);
+	}
+
+	// Check security attributes of the destination EPC page
+	if ((epc_epcm->valid == 0) || (epc_epcm->blocked != 0) ||
+		((epc_epcm->page_type != SGX_PT_REG) && (epc_epcm->page_type != SGX_PT_TCS) && (epc_epcm->page_type != SGX_PT_TRIM)) ||
+		(epc_epcm->enclave_secs != current->secs_pa)) {
+		emusgx_pf(epc_page_va, ptrace_regs);
+	}
+	
+	// Check for concurrency
+	// TODO
+
+	// Recheck .. ?
+
+	// Verify that  accept request matches current EPC page settings 
+	if ((epc_epcm->enclave_address != epc_page_va) || (epc_epcm->pending != scratch_secinfo->flags.pending) ||
+		(epc_epcm->modified != scratch_secinfo->flags.modified) || (epc_epcm->R != scratch_secinfo->flags.R) ||
+		(epc_epcm->W != scratch_secinfo->flags.W) || (epc_epcm->X != scratch_secinfo->flags.X) ||
+		(epc_epcm->page_type != scratch_secinfo->flags.page_type)) {
+		reg_status->flags.ZF = 1;
+		reg_status->rax = EMUSGX_PAGE_ATTRIBUTES_MISMATCH;
+		pr_info("vSGX: EACCEPT runs into page attributes mismatch\n");
+		goto DONE;
+	}
+
+	// TCS
+	if (scratch_secinfo->flags.page_type == SGX_PT_TCS) {
+		tmp_tcs = epc_page_va;
+		__uaccess_begin();
+		if (tmp_tcs->flags.reserved != 0) {
+			__uaccess_end();
+			emusgx_gp(0, ptrace_regs);
+			return;
+		}
+		for (i = 0; i < 4016; i++) {
+			if (tmp_tcs->reserved4[i] != 0) {
+				__uaccess_end();
+				emusgx_gp(0, ptrace_regs);
+				return;
+			}
+		}
+		// Check that TCS.FLAGS.DBGOPTIN, TCS stack and TCS status are correctly initialized
+		if (tmp_tcs->flags.dbgoptin != 0) {
+			__uaccess_end();
+			emusgx_gp(0, ptrace_regs);
+			return;
+		}
+		/*if (tmp_tcs->cssa >= tmp_tcs->nssa) {
+			emusgx_gp(0, ptrace_regs);
+		}
+		if (tmp_tcs->aep != 0) {
+			emusgx_gp(0, ptrace_regs);
+		}*/ // Does not matter. No SSA support nor an AEP would happen
+		if (tmp_tcs->state != 0) {
+			__uaccess_end();
+			emusgx_gp(0, ptrace_regs);
+			return;
+		}
+		__uaccess_end();
+	}
+
+	epc_epcm->pending = 0;
+	epc_epcm->modified = 0;
+	
+
+	// Clear EAX and ZF to  indicate successful completion
+	reg_status->flags.ZF = 0;
+	reg_status->rax = EMUSGX_SUCCESS;
+
+DONE:
+	memset(scratch_secinfo, 0, sizeof(struct sgx_secinfo));
+	kfree(scratch_secinfo);
+	reg_status->flags.CF = 0;
+	reg_status->flags.PF = 0;
+	reg_status->flags.AF = 0;
+	reg_status->flags.OF = 0;
+	reg_status->flags.SF = 0;
+}
+
+void emusgx_emodpe(struct emusgx_regs *reg_status, struct pt_regs *ptrace_regs) {
+	struct sgx_secinfo *scratch_secinfo = (struct sgx_secinfo *)(reg_status->rbx);
+	void *secinfo_pa = (void *)vsgx_vaddr_to_paddr(scratch_secinfo, current->manager_entry->manager_nr);
+	void *epc_page_va = (void *)(reg_status->rcx);
+	void *epc_page_pa = (void *)vsgx_vaddr_to_paddr(epc_page_va, current->manager_entry->manager_nr);
+
+	struct emusgx_epcm *secinfo_epcm;
+	struct emusgx_epcm *epc_epcm;
+
+	// SECINFO must be 64 aligned
+	if (reg_status->rbx % 64 != 0) {
+		emusgx_gp(0, ptrace_regs);
+	}
+	
+	// EPC page must be 4K aligned
+	if (reg_status->rcx % 4096 != 0) {
+		emusgx_gp(0, ptrace_regs);
+	}
+
+	if (!vsgx_check_in_elrange(scratch_secinfo)) {
+		emusgx_gp(0, ptrace_regs);
+	}
+
+	if (!vsgx_check_in_elrange(epc_page_va)) {
+		emusgx_gp(0, ptrace_regs);
+	}
+
+	// Check within EPC
+	if (emusgx_check_within_epc(epc_page_pa)) {
+		emusgx_pf(epc_page_va, ptrace_regs);
+	}
+	
+	if (emusgx_check_within_epc(secinfo_pa)) {
+		emusgx_pf(scratch_secinfo, ptrace_regs);
+	}
+
+	secinfo_epcm = emusgx_get_epcm(secinfo_pa);
+
+	// Check for SECINFO EPCM
+	if ((secinfo_epcm->valid == 0) || (secinfo_epcm->R == 0) ||
+		(secinfo_epcm->pending != 0) || (secinfo_epcm->modified != 0) ||
+		(secinfo_epcm->blocked !=0) || (secinfo_epcm->page_type != SGX_PT_REG) ||
+		(secinfo_epcm->enclave_secs != current->secs_pa) || (secinfo_epcm->enclave_address != scratch_secinfo)) {
+		emusgx_pf(scratch_secinfo, ptrace_regs);
+	}
+	
+	// Copy SECINFO from user
+	scratch_secinfo = kmalloc(sizeof(struct sgx_secinfo), GFP_KERNEL);
+	if (copy_from_user(scratch_secinfo, (struct sgx_secinfo*)(reg_status->rbx), sizeof(struct sgx_secinfo))) {
+		pr_info("EmuSGX: Failed to copy from user\n");
+	}
+
+	// check for mis-configured SECINFO flags
+	if (scratch_secinfo->flags.reserved != 0) {
+		emusgx_gp(0, ptrace_regs);
+	}
+
+	epc_epcm = emusgx_get_epcm(epc_page_pa);
+	
+	// Check security attributes of the destination EPC page
+	if ((epc_epcm->valid == 0) || (epc_epcm->pending != 0) || (epc_epcm->modified != 0) ||
+		(epc_epcm->blocked != 0) || (epc_epcm->page_type != SGX_PT_REG) ||
+		(epc_epcm->enclave_secs != current->secs_pa)) {
+		emusgx_pf(epc_page_va, ptrace_regs);
+	}
+	
+	// Check for concurrency
+	// TODO
+
+	// Recheck .. ?
+	if ((epc_epcm->enclave_address == epc_page_va)) {
+		emusgx_pf(epc_page_va, ptrace_regs);
+	}
+
+	// check for mis-configured SECINFO flags
+	if ((epc_epcm->R == 0) && (scratch_secinfo->flags.R == 0) && (scratch_secinfo->flags.W != 0)) {
+		emusgx_gp(0, ptrace_regs);
+	}
+
+	// Update EPCM permissions
+	epc_epcm->R = epc_epcm->R || scratch_secinfo->flags.R;
+	epc_epcm->W = epc_epcm->W || scratch_secinfo->flags.W;
+	epc_epcm->X = epc_epcm->X || scratch_secinfo->flags.X;
+
+	memset(scratch_secinfo, 0, sizeof(struct sgx_secinfo));
+	kfree(scratch_secinfo);
+}
+
+void emusgx_eacceptcopy(struct emusgx_regs *reg_status, struct pt_regs *ptrace_regs) {
+	struct sgx_secinfo *scratch_secinfo = (struct sgx_secinfo *)(reg_status->rbx);
+	void *secinfo_pa = (void *)vsgx_vaddr_to_paddr(scratch_secinfo, current->manager_entry->manager_nr);
+	void *epc_dest_page_va = (void *)(reg_status->rcx);
+	void *epc_dest_page_pa = (void *)vsgx_vaddr_to_paddr(epc_dest_page_va, current->manager_entry->manager_nr);
+	void *epc_src_page_va = (void *)(reg_status->rdx);
+	void *epc_src_page_pa = (void *)vsgx_vaddr_to_paddr(epc_src_page_va, current->manager_entry->manager_nr);
+
+	struct emusgx_epcm *secinfo_epcm;
+	struct emusgx_epcm *epc_dest_epcm;
+	struct emusgx_epcm *epc_src_epcm;
+
+	// SECINFO must be 64 aligned
+	if (reg_status->rbx % 64 != 0) {
+		emusgx_gp(0, ptrace_regs);
+	}
+
+	// Two EPC pages must be 4KB aligned
+	if ((reg_status->rcx % 4096 != 0) || (reg_status->rdx % 4096 != 0)) {
+		emusgx_gp(0, ptrace_regs);
+	}
+
+	if (!vsgx_check_in_elrange(scratch_secinfo) || !vsgx_check_in_elrange(epc_dest_page_va) || !vsgx_check_in_elrange(epc_src_page_va)) {
+		emusgx_gp(0, ptrace_regs);
+	}
+
+	// Check within EPC
+	if (emusgx_check_within_epc(epc_src_page_pa)) {
+		emusgx_pf(epc_src_page_va, ptrace_regs);
+	}
+	
+	if (emusgx_check_within_epc(epc_dest_page_pa)) {
+		emusgx_pf(epc_dest_page_va, ptrace_regs);
+	}
+
+	if (emusgx_check_within_epc(secinfo_pa)) {
+		emusgx_pf(scratch_secinfo, ptrace_regs);
+	}
+
+	secinfo_epcm = emusgx_get_epcm(secinfo_pa);
+
+	// Check for SECINFO EPCM
+	if ((secinfo_epcm->valid == 0) || (secinfo_epcm->R == 0) ||
+		(secinfo_epcm->pending != 0) || (secinfo_epcm->modified != 0) ||
+		(secinfo_epcm->blocked !=0) || (secinfo_epcm->page_type != SGX_PT_REG) ||
+		(secinfo_epcm->enclave_secs != current->secs_pa) || (secinfo_epcm->enclave_address != scratch_secinfo)) {
+		emusgx_pf(scratch_secinfo, ptrace_regs);
+	}
+	
+	// Copy SECINFO from user
+	scratch_secinfo = kmalloc(sizeof(struct sgx_secinfo), GFP_KERNEL);
+	if (copy_from_user(scratch_secinfo, (struct sgx_secinfo*)(reg_status->rbx), sizeof(struct sgx_secinfo))) {
+		pr_info("EmuSGX: Failed to copy from user\n");
+	}
+
+	// check for mis-configured SECINFO flags
+	if ((scratch_secinfo->flags.reserved != 0) || ((scratch_secinfo->flags.R == 0) && (scratch_secinfo->flags.W != 0)) ||
+		(scratch_secinfo->flags.page_type != SGX_PT_REG)) {
+		emusgx_gp(0, ptrace_regs);
+	}
+
+	epc_src_epcm = emusgx_get_epcm(epc_src_page_pa);
+
+	// Check security attributes of the source EPC page
+	if ((epc_src_epcm->valid == 0) || (epc_src_epcm->pending != 0) || (epc_src_epcm->modified != 0) ||
+		(epc_src_epcm->blocked != 0) || (epc_src_epcm->page_type != SGX_PT_REG) || (epc_src_epcm->enclave_secs != current->secs_pa) ||
+		(epc_src_epcm->enclave_address != epc_src_page_va)) {
+		emusgx_pf(epc_src_page_va, ptrace_regs);
+	}
+
+	epc_dest_epcm = emusgx_get_epcm(epc_dest_page_pa);
+
+	// Check security attributes of the destination EPC page
+	if ((epc_dest_epcm->valid == 0) || (epc_dest_epcm->pending != 1) || (epc_dest_epcm->modified != 0) || 
+		(epc_dest_epcm->page_type != SGX_PT_REG) || (epc_dest_epcm->enclave_secs != current->secs_pa)) {
+		reg_status->flags.ZF = 1;
+		reg_status->rax = EMUSGX_PAGE_ATTRIBUTES_MISMATCH;
+		goto DONE;
+	}
+
+	// Check for concurrency
+	// Recheck dest .. ?
+	if ((epc_dest_epcm->valid == 0) || (epc_dest_epcm->pending != 1) || (epc_dest_epcm->modified != 0) || 
+		(epc_dest_epcm->R != 1) || (epc_dest_epcm->W != 1) || (epc_dest_epcm->X != 0) || 
+		(epc_dest_epcm->page_type != scratch_secinfo->flags.page_type) || (epc_dest_epcm->enclave_secs != current->secs_pa)) { 
+		emusgx_pf(epc_dest_page_va, ptrace_regs);
+	}
+
+	if (epc_dest_epcm->enclave_address != epc_dest_page_va) {
+		emusgx_pf(epc_dest_page_va, ptrace_regs);
+	}
+
+	// Copy 4KB from source to dest
+	// allocate 4KB buffer
+	__uaccess_begin();
+	// Direct copy
+	memcpy(epc_dest_page_va, epc_src_page_va, 4096);
+	__uaccess_end();
+
+	// Clear EAX and ZF to  indicate successful completion
+	reg_status->flags.ZF = 0;
+	reg_status->rax = EMUSGX_SUCCESS;
+
+DONE:
+	memset(scratch_secinfo, 0, sizeof(struct sgx_secinfo));
+	kfree(scratch_secinfo);
+	reg_status->flags.CF = 0;
+	reg_status->flags.PF = 0;
+	reg_status->flags.AF = 0;
+	reg_status->flags.OF = 0;
+	reg_status->flags.SF = 0;
+}
+
+void emusgx_eexit(struct emusgx_regs *reg_status, struct pt_regs *ptrace_regs) {
+	// RBX: Target address outside the enclave
+	// RCX: Address of the current AEP
+	struct emusgx_eexit_package *package;
+
+	// Make sure all pages are written back to the guest VM
+	emusgx_debug_print("EmuSGX: Exiting... Now syncing all pages\n");
+	emusgx_sync_all_pages();
+	emusgx_debug_print("EmuSGX: Pages are synced\n");
+
+	__uaccess_begin();
+	current->tcs->state = 0;
+	__uaccess_end();
+	
+	package = kmalloc(sizeof(struct emusgx_eexit_package), GFP_KERNEL);
+
+	package->instr = EMUSGX_S_EEXIT;
+
+	// Basically just send the package
+	package->regs.ip = ptrace_regs->bx;
+	package->regs.r15 = ptrace_regs->r15;
+	package->regs.r14 = ptrace_regs->r14;
+	package->regs.r13 = ptrace_regs->r13;
+	package->regs.r12 = ptrace_regs->r12;
+	package->regs.bp = ptrace_regs->bp;
+	package->regs.bx = ptrace_regs->bx;
+	package->regs.r11 = ptrace_regs->r11;
+	package->regs.r10 = ptrace_regs->r10;
+	package->regs.r9 = ptrace_regs->r9;
+	package->regs.r8 = ptrace_regs->r8;
+	package->regs.ax = ptrace_regs->ax;
+	package->regs.cx = 0; // Current AEP. We do not use AEX so just leave it 0
+	package->regs.dx = ptrace_regs->dx;
+	package->regs.si = ptrace_regs->si;
+	package->regs.di = ptrace_regs->di;
+	package->regs.flags = ptrace_regs->flags;
+	package->regs.sp = ptrace_regs->sp;
+
+	package->pid = current->emusgx_pid;
+
+	// Send package
+	if (emusgx_send_data(package, sizeof(struct emusgx_eexit_package))) {
+		pr_info("EmuSGX: EEXIT failed to send data\n");
+	}
+
+	kfree(package);
+}
+
+void (*emusgx_enclu_handlers[8])(struct emusgx_regs*, struct pt_regs *) = {
+	&emusgx_ereport,
+	&emusgx_egetkey,
+	&emusgx_eenter,
+	&emusgx_eresume,
+	&emusgx_eexit,
+	&emusgx_eaccept,
+	&emusgx_emodpe,
+	&emusgx_eacceptcopy
+};
+
+void emusgx_handle_enclu(struct emusgx_regs *reg_status, struct pt_regs *ptrace_regs) {
+	if ((uint32_t)(reg_status->rax) <= 7)
+		(*emusgx_enclu_handlers[(uint32_t)(reg_status->rax)])(reg_status, ptrace_regs);
+	else {
+		emusgx_gp(0, ptrace_regs);
+	}
+}
+
diff --git a/kernel/emusgx/entrance.c b/kernel/emusgx/entrance.c
new file mode 100644
index 000000000..3b1b3fa96
--- /dev/null
+++ b/kernel/emusgx/entrance.c
@@ -0,0 +1,491 @@
+#include <linux/kernel.h>
+#include <linux/semaphore.h>
+#include <linux/uaccess.h>
+#include <linux/slab.h>
+
+#include <asm/special_insns.h>
+#include <asm/processor-flags.h>
+
+#include <emusgx/emusgx.h>
+#include <emusgx/emusgx_internal.h>
+#include <emusgx/emusgx_sender.h>
+#include <emusgx/emusgx_debug.h>
+#include <emusgx/emusgx_fault.h>
+
+uint64_t _enclave_local emusgx_enter_enclave(struct sgx_tcs __user *tcs, void *aep, uint64_t pid, struct emusgx_full_regs *regs, void **pf_addr) {
+	void *tcs_pa = (void *)vsgx_vaddr_to_paddr(tcs, current->manager_entry->manager_nr);
+	struct emusgx_epcm *tcs_epcm;
+	struct emusgx_epcm *gpr_epcm;
+	struct sgx_secs *tmp_secs;
+	struct sgx_secs *tmp_secs_pa;
+	struct emusgx_user_space_manager_entry *tmp_manager_entry;
+
+	uint64_t tmp_fsbase;
+	uint64_t tmp_gsbase;
+	// uint64_t tmp_fslimit;
+	// uint64_t tmp_gslimit;
+
+	// uint64_t tmp_xsize;
+	void __user *tmp_ssa_page;
+	struct sgx_gprsgx __user *tmp_gpr;
+	void *tmp_gpr_pa;
+
+	uint64_t tmp_target;
+	struct emusgx_handle_buffer *handle_buffer;
+
+	*pf_addr = NULL;
+
+	emusgx_debug_print("EmuSGX: Now entering EENTER\n");
+
+	// Make sure DS is useable, expand up
+	// Ignored
+
+	// Check that CC,  SS, DS, ES.base is 0
+	// Ignored
+
+	if ((uint64_t)tcs % 4096 != 0) {
+		pr_info("EmuSGX: TCS align failed\n");
+		return EMUSGX_GP;
+	}
+
+	if (emusgx_check_within_epc(tcs_pa)) {
+		pr_info("EmuSGX: TCS not in EPC\n");
+		return EMUSGX_PF_RBX;
+	}
+
+	// Check AEP is canonical
+	// First 48 bits should be the same
+	// Assume same
+
+	// Check concurrency of TCS operation
+	// Ignored
+
+	// TCS verifiction
+	tcs_epcm = emusgx_get_epcm(tcs_pa);
+
+	if (tcs_epcm->valid == 0) {
+		*pf_addr = tcs;
+		pr_info("EmuSGX: TCS EPCM invalid\n");
+		return EMUSGX_PF_RCX;
+	}
+
+	if (tcs_epcm->blocked == 1) {
+		*pf_addr = tcs;
+		pr_info("EmuSGX: TCS EPCM blocked\n");
+		return EMUSGX_PF_RCX;
+	}
+
+	if (tcs_epcm->enclave_address != tcs || tcs_epcm->page_type != SGX_PT_TCS) {
+		*pf_addr = tcs;
+		pr_info("EmuSGX: TCS enclave address incorrect or page type incorrect\n");
+		return EMUSGX_PF_RCX;
+	}
+
+	if (tcs_epcm->pending == 1 || tcs_epcm->modified == 1) {
+		*pf_addr = tcs;
+		pr_info("EmuSGX: TCS EPCM pending or modified\n");
+		return EMUSGX_PF_RCX;
+	}
+
+	__uaccess_begin();
+
+	if (tcs->ossa % 4096 != 0) {
+		__uaccess_end();
+		pr_info("EmuSGX: OSSA align failed\n");
+		return EMUSGX_GP;
+	}
+
+	// Check proposed FS and GS
+	if (tcs->ofsbasgx % 4096 != 0 || tcs->ogsbasgx % 4096 != 0) {
+		__uaccess_end();
+		pr_info("EmuSGX: FS/GS base align failed\n");
+		return EMUSGX_GP;
+	}
+
+	__uaccess_end();
+
+	// Get the SECS for the enclave in which the TCS resides
+	tmp_secs_pa = (void *)tcs_epcm->enclave_secs;
+	tmp_secs = emusgx_get_epcm(tmp_secs_pa)->enclave_address;
+
+	__uaccess_begin();
+
+	// Check proposed FS/GS segments fall within DS
+	// IF (TMP_MODE64 = 0)
+	//    Ignored
+	//    No 32-bit support
+	tmp_fsbase = tcs->ofsbasgx + tmp_secs->base;
+	tmp_gsbase = tcs->ogsbasgx + tmp_secs->base;
+	// If TMP_FSBASE or TMP_GSBASE is not canonical
+	//    Ignored
+	//    Assume canonical
+
+	// Ensure that the FLAGS field in the TCS does not have any reserved bits set
+	if (tcs->flags.reserved != 0) {
+		__uaccess_end();
+		pr_info("EmuSGX: Flags reserved not 0\n");
+		return EMUSGX_GP;
+	}
+
+	__uaccess_end();
+
+	// SECS must exist and enclave must have previously been EINITted
+	if (!emusgx_is_secs_inited(tmp_secs_pa)) {
+		pr_info("EmuSGX: Non inited enclave\n");
+		return EMUSGX_GP;
+	}
+
+	// Make sure the logical processor's operating mode matches the enclave
+	// Check if MODE64 are the same
+	// No 32-bit support
+	// Ignored
+
+	// CR4.OSFXSR support
+	if (!(__read_cr4() & X86_CR4_OSFXSR)) {
+		// GP(0);
+		pr_info("EmuSGX: CR4 Failed\n");
+		return EMUSGX_GP;
+	}
+
+	// Check for legal values of SECS.ATTRIBUTES.XFRM
+	if (!(__read_cr4() & X86_CR4_OSXSAVE)) {
+		// GP(0);
+		__uaccess_begin();
+		if (tmp_secs->xfrm != 0x3) {
+			__uaccess_end();
+			pr_info("EmuSGX: SECS XFRM invalid\n");
+			return EMUSGX_GP;
+		}
+		__uaccess_end();
+	}
+	/*else {
+		We do not have AEX
+		Not necessarily to have in our implementation
+	}*/
+
+	// Compute linear address of SSA frame
+	__uaccess_begin();
+	tmp_ssa_page = (void __user *)(tcs->ossa + tmp_secs->base + 4096 * tmp_secs->ssaframesize * tcs->cssa);
+	// tmp_xsize = compute_XSAVE_frame_size;
+
+	// FOR EACH SSAPAGE check r/w accessiable
+	// We don't use XSAVE for AEX
+	// Just check GPR page being accessible is enough
+
+	// Compute address of GPR area
+	tmp_gpr = (void __user *)((uint64_t)tmp_ssa_page + 4096 * tmp_secs->ssaframesize - sizeof(struct sgx_gprsgx));
+	__uaccess_end();
+
+	tmp_gpr_pa = (void *)vsgx_vaddr_to_paddr(tmp_gpr, current->manager_entry->manager_nr);
+
+	// If GPR area valid
+	if (emusgx_check_within_epc(tmp_gpr_pa)) {
+		*pf_addr = tmp_gpr;
+		pr_info("EmuSGX: SSA GPR not in EPC\n");
+		return EMUSGX_PF_RCX;
+	}
+	gpr_epcm = emusgx_get_epcm(tmp_gpr_pa);
+	if (gpr_epcm->valid == 0) {
+		*pf_addr = tmp_gpr;
+		pr_info("EmuSGX: SSA GPR EPCM invalid\n");
+		return EMUSGX_PF_RCX;
+	}
+	if (gpr_epcm->blocked == 1) {
+		*pf_addr = tmp_gpr;
+		pr_info("EmuSGX: SSA GPR EPCM blocked\n");
+		return EMUSGX_PF_RCX;
+	}
+	if (gpr_epcm->pending == 1) {
+		*pf_addr = tmp_gpr;
+		pr_info("EmuSGX: SSA GPR EPCM pending\n");
+		return EMUSGX_PF_RCX;
+	}
+	if (gpr_epcm->modified == 1) {
+		*pf_addr = tmp_gpr;
+		pr_info("EmuSGX: SSA GPR EPCM modified\n");
+		return EMUSGX_PF_RCX;
+	}
+	if (gpr_epcm->page_type != SGX_PT_REG || gpr_epcm->enclave_secs != (uint64_t)tmp_secs_pa || gpr_epcm->R == 0 || gpr_epcm->W == 0) {
+		*pf_addr = tmp_gpr;
+		pr_info("EmuSGX: SSA GPR EPCM wrong\n");
+		return EMUSGX_PF_RCX;
+	}
+
+	__uaccess_begin();
+
+	// Validate TCS.OENTRY
+	tmp_target = tcs->oentry + tmp_secs->base;
+
+	// IF TMP_TARGET is not canonical
+	// Ignored
+
+	// Ensure the enclave is not already active and this thread is the only one using the TCS
+	if (tcs->state) {
+		__uaccess_end();
+		pr_info("EmuSGX: TCS inuse\n");
+		return EMUSGX_GP;
+	}
+
+	tcs->state = 1;
+	tcs->pid = pid;
+
+	// ENCLAVE_MODE = 1
+	// Ignored
+
+	// Save state for possible AEXs
+	tcs->aep = (uint64_t)aep;
+
+	// Save the hidden portions of FS and GS
+	// Another VM, don't care
+
+	// If XSAVE is enabled, save XCR0 and replace it with SECS.ATTRIBUTES.XFRM
+	// Ignored
+
+	// Set CR_ENCLAVE_ENTRY_IP
+	// RIP <- NRIP
+	regs->ax = tcs->cssa;
+	regs->cx = regs->ip + 3; // Address of IP following EENTER
+	// Save the outside RSP and RBP so they can be restored on interrupt or EEXIT
+	tmp_gpr->ursp = regs->sp;
+	tmp_gpr->urbp = regs->bp;
+
+	__uaccess_end();
+
+	// Do the FS/GS swap
+	// o Create new descs, and set them to FS/GS
+	// o Both of the selector needs to be 0x0B
+	// o We will drop this to the wrapper to set instead of doing it in the kernel
+	//   since we are not on the same thread with the user space wrapper
+	// o In 64-bit, all attributes except for the base is ignored
+	//   We will update FS and GS using MSR
+	handle_buffer = kmalloc(sizeof(struct emusgx_handle_buffer), GFP_KERNEL);
+	if (handle_buffer == NULL) {
+		// return GP
+		pr_info("EmuSGX: Failed to allocate handle buffer\n");
+		return EMUSGX_GP;
+	}
+
+	handle_buffer->action = EMUSGX_MGRACT_NEW_THREAD;
+	handle_buffer->fsbase = tmp_fsbase;
+	handle_buffer->gsbase = tmp_gsbase;
+	handle_buffer->rip = tmp_target;
+	handle_buffer->secs = tmp_secs;
+	handle_buffer->secs_pa = (uint64_t)tmp_secs_pa;
+	handle_buffer->pid = pid;
+	handle_buffer->tcs = tcs;
+	handle_buffer->gpr = tmp_gpr;
+	memcpy(&(handle_buffer->regs), regs, sizeof(struct emusgx_full_regs));
+
+	__uaccess_begin();
+
+	tmp_manager_entry = (struct emusgx_user_space_manager_entry *)tmp_secs->manager_entry;
+
+	__uaccess_end();
+
+	// Now wait for handling
+	// No one shall ever send signal to this process
+	down(&(tmp_manager_entry->action_avail_semaphore));
+
+	// Write 
+	tmp_manager_entry->handle_buffer = handle_buffer;
+
+	// Up the handling semaphore
+	up(&(tmp_manager_entry->action_semaphore));
+
+	emusgx_debug_print("EmuSGX: action_semaphore@0x%016llX\n", (uint64_t)&(tmp_manager_entry->action_semaphore));
+
+	// Now there should be a process running
+	// We are good to leave
+	return 0;
+}
+
+// 0 for good
+// 1 for PF on TCS
+// 2 for PF on GPR
+// 3 for GP
+uint8_t vsgx_resume_enclave(struct sgx_tcs __user *tcs, uint64_t tcs_pa, uint64_t aep, uint64_t pid, void **pf_addr) {
+	struct emusgx_epcm *tcs_epcm;
+	struct emusgx_epcm *gpr_epcm;
+	struct sgx_secs *tmp_secs;
+	struct sgx_secs *tmp_secs_pa;
+
+	//uint64_t tmp_fsbase;
+	//uint64_t tmp_gsbase;
+	// uint64_t tmp_fslimit;
+	// uint64_t tmp_gslimit;
+
+	// uint64_t tmp_xsize;
+	void __user *tmp_ssa_page;
+	struct sgx_gprsgx __user *tmp_gpr;
+	void *tmp_gpr_pa;
+
+	//uint64_t tmp_target;
+	
+	// Make sure DS is usable, expand up
+	// Context not changed. Don't care
+
+	// Check that CS, SS, DS, ES.base is 0
+	// Context not changed. Don't care
+
+	if ((uint64_t)tcs % 4096 != 0) {
+		// GP
+		return 3;
+	}
+
+	// Note that vSGX must check TCS maps to TCS_PA before calling
+	// this function
+	// Check within EPC
+	if (emusgx_check_within_epc((void *)tcs_pa)) {
+		return 1;
+	}
+
+	// TCS verification
+	tcs_epcm = emusgx_get_epcm((void *)tcs_pa);
+	if (tcs_epcm->valid == 0) {
+		return 1;
+	}
+
+	if (tcs_epcm->blocked == 1) {
+		return 1;
+	}
+
+	if (tcs_epcm->pending == 1 || tcs_epcm->modified == 1) {
+		return 1;
+	}
+
+	if (tcs_epcm->enclave_address != tcs || tcs_epcm->page_type != SGX_PT_TCS) {
+		return 1;
+	}
+
+	__uaccess_begin();
+	if (tcs->ossa % 4096 != 0) {
+		__uaccess_end();
+		return 3;
+	}
+
+	// Check proposed FS and GS
+	if (tcs->ofsbasgx % 4096 != 0 || tcs->ogsbasgx % 4096 != 0) {
+		__uaccess_end();
+		pr_info("vSGX: ERESUME: FS/GS base align failed\n");
+		return 3;
+	}
+	__uaccess_end();
+
+	tmp_secs_pa = (void *)tcs_epcm->enclave_secs;
+	tmp_secs = emusgx_get_epcm(tmp_secs_pa)->enclave_address;
+
+	__uaccess_begin();
+	// Ensure that the FLAGS field in the TCS does not have any reserved bits set
+	if (tcs->flags.reserved != 0) {
+		__uaccess_end();
+		pr_info("EmuSGX: Flags reserved not 0\n");
+		return 3;
+	}
+	__uaccess_end();
+
+	// SECS must exist and enclave must have previously been EINITted
+	if (!emusgx_is_secs_inited(tmp_secs_pa)) {
+		pr_info("EmuSGX: Non inited enclave\n");
+		return 3;
+	}
+
+	// make sure the logical processors operating mode matches the enclave 
+	// We only support 64 bit
+
+	// CR4.OSFXSR support
+	if (!(__read_cr4() & X86_CR4_OSFXSR)) {
+		// GP(0);
+		pr_info("EmuSGX: CR4 Failed\n");
+		return 3;
+	}
+
+	// Check for legal values of SECS.ATTRIBUTES.XFRM
+	if (!(__read_cr4() & X86_CR4_OSXSAVE)) {
+		// GP(0);
+		__uaccess_begin();
+		if (tmp_secs->xfrm != 0x3) {
+			__uaccess_end();
+			pr_info("EmuSGX: SECS XFRM invalid\n");
+			return 3;
+		}
+		__uaccess_end();
+	}
+
+	// Make sure the SSA contains at least one active frame
+	__uaccess_begin();
+	if (tcs->cssa == 0) {
+		__uaccess_end();
+		pr_info("vSGX: ERESUME: CSSA = 0\n");
+		return 3;
+	}
+	__uaccess_end();
+
+	__uaccess_begin();
+	tmp_ssa_page = (void __user *)(tcs->ossa + tmp_secs->base + 4096 * tmp_secs->ssaframesize * tcs->cssa);
+	// tmp_xsize = compute_XSAVE_frame_size;
+
+	// FOR EACH SSAPAGE check r/w accessiable
+	// We don't use XSAVE for AEX
+	// Just check GPR page being accessible is enough
+
+	// Compute address of GPR area
+	tmp_gpr = (void __user *)((uint64_t)tmp_ssa_page + 4096 * tmp_secs->ssaframesize - sizeof(struct sgx_gprsgx));
+	__uaccess_end();
+
+	tmp_gpr_pa = (void *)vsgx_vaddr_to_paddr(tmp_gpr, current->manager_entry->manager_nr);
+
+	// If GPR area valid
+	if (emusgx_check_within_epc(tmp_gpr_pa)) {
+		pr_info("EmuSGX: SSA GPR not in EPC\n");
+		*pf_addr = tmp_gpr;
+		return 2;
+	}
+	gpr_epcm = emusgx_get_epcm(tmp_gpr_pa);
+	if (gpr_epcm->valid == 0) {
+		pr_info("EmuSGX: SSA GPR EPCM invalid\n");
+		*pf_addr = tmp_gpr;
+		return 2;
+	}
+	if (gpr_epcm->blocked == 1) {
+		pr_info("EmuSGX: SSA GPR EPCM blocked\n");
+		*pf_addr = tmp_gpr;
+		return 2;
+	}
+	if (gpr_epcm->pending == 1) {
+		pr_info("EmuSGX: SSA GPR EPCM pending\n");
+		*pf_addr = tmp_gpr;
+		return 2;
+	}
+	if (gpr_epcm->modified == 1) {
+		pr_info("EmuSGX: SSA GPR EPCM modified\n");
+		*pf_addr = tmp_gpr;
+		return 2;
+	}
+	if (gpr_epcm->page_type != SGX_PT_REG || gpr_epcm->enclave_secs != (uint64_t)tmp_secs_pa || gpr_epcm->R == 0 || gpr_epcm->W == 0) {
+		pr_info("EmuSGX: SSA GPR EPCM wrong\n");
+		*pf_addr = tmp_gpr;
+		return 2;
+	}
+
+	current->gpr = tmp_gpr;
+
+	// The rest of restore is done by directly wake up
+
+	__uaccess_begin();
+	if (tcs->state) {
+		__uaccess_end();
+		pr_info("EmuSGX: TCS inuse\n");
+		return 3;
+	}
+
+	// Set the TCS state
+	tcs->state = 1;
+
+	// Pop the SSA stack
+	tcs->cssa -= 1;
+
+	tcs->pid = pid;
+	__uaccess_end();
+
+	return 0;
+}
\ No newline at end of file
diff --git a/kernel/emusgx/fault.c b/kernel/emusgx/fault.c
new file mode 100644
index 000000000..2bac42487
--- /dev/null
+++ b/kernel/emusgx/fault.c
@@ -0,0 +1,41 @@
+#include <linux/kernel.h>
+#include <linux/ptrace.h>
+#include <linux/sched/signal.h>
+#include <linux/uaccess.h>
+
+#include <asm/traps.h>
+#include <asm/siginfo.h>
+
+#include <emusgx/emusgx.h>
+#include <emusgx/emusgx_internal.h>
+#include <emusgx/emusgx_sender.h>
+
+struct emusgx_fault_package {
+	uint8_t instr;
+	uint8_t gp_or_pf;
+	uint64_t val;
+	uint64_t pid;
+} __attribute__((__packed__));
+
+void emusgx_gp(int code, struct pt_regs *ptrace_regs) {
+	struct emusgx_fault_package package;
+	package.instr = EMUSGX_S_FAULT;
+	package.gp_or_pf = 0;
+	package.val = code;
+	package.pid = current->emusgx_pid;
+	emusgx_send_data(&package, sizeof(struct emusgx_fault_package));
+	pr_info("vSGX: GP");
+	exc_general_protection(ptrace_regs, 0);
+}
+
+void emusgx_pf(void __user *addr, struct pt_regs *ptrace_regs) {
+	struct emusgx_fault_package package;
+	package.instr = EMUSGX_S_FAULT;
+	package.gp_or_pf = 1;
+	package.val = (uint64_t)addr;
+	package.pid = current->emusgx_pid;
+	emusgx_send_data(&package, sizeof(struct emusgx_fault_package));
+	pr_info("vSGX: Bad page fault @ 0x%016llX\n", (uint64_t)addr);
+	// A bad page fault causes an SIGSEGV
+	force_sig_fault(SIGSEGV, SEGV_MAPERR, (void __user *)addr);
+}
diff --git a/kernel/emusgx/irq.c b/kernel/emusgx/irq.c
new file mode 100644
index 000000000..36ab587e9
--- /dev/null
+++ b/kernel/emusgx/irq.c
@@ -0,0 +1,130 @@
+#include <linux/kernel.h>
+#include <linux/slab.h>
+#include <linux/interrupt.h>
+#include <linux/spinlock.h>
+#include <linux/semaphore.h>
+
+#include <asm/atomic.h>
+#include <asm/tlbflush.h>
+#include <asm/cacheflush.h>
+#include <asm/io.h>
+
+#include <emusgx/emusgx_sender.h>
+#include <emusgx/emusgx_mm.h>
+#include <emusgx/emusgx_cpuid.h>
+#include <emusgx/emusgx_debug.h>
+
+#include <linux/ktime.h>
+
+
+// Timer
+static uint64_t stamps_in[20];
+static uint64_t stamps_retrieved[20];
+static uint64_t stamps_pushed[20];
+static uint32_t stamp_i = 0;
+
+irqreturn_t emusgx_irq_handler(int irq, void *dev_id) {
+	unsigned long flags;
+	void *page;
+	struct emusgx_request_queue_node *new_node;
+	int cpuid_success;
+	uint64_t physical_page_addr;
+	uint32_t physical_page_addr_upper;
+	uint32_t physical_page_addr_lower;
+
+
+	physical_page_addr = virt_to_phys(emusgx_receive_page);
+	physical_page_addr_lower = (uint32_t)(physical_page_addr);
+	physical_page_addr_upper = (uint32_t)((uint64_t)(physical_page_addr) >> 32);
+
+	emusgx_debug_print("EmuSGX: IRQ received. Retriving data...\n");
+	if (stamp_i < 20)
+		stamps_in[stamp_i] = ktime_get_real_ns();
+
+	// First retrive data to the receive page
+	asm volatile (
+		"cpuid"
+		: "=b"(cpuid_success)
+		: "a"(KVM_CPUID_EMUSGX_ENCLAVE_RETRIVE_PAGE), "b"(physical_page_addr_upper), "c"(physical_page_addr_lower)
+		: "%rdx"
+	);
+	if (cpuid_success) {
+		pr_info("EmuSGX: CPUID failed for retriving page\n");
+	}
+
+	//wbinvd_on_all_cpus();
+	//asm volatile ("mfence" : : : "memory");
+	clflush_cache_range(emusgx_receive_page, 4096);
+	mb();
+
+	emusgx_debug_print("EmuSGX: Data retrived.\n");
+	if (stamp_i < 20)
+		stamps_retrieved[stamp_i] = ktime_get_real_ns();
+
+	// New a page
+	// CANNOT SLEEP!
+	page = kmalloc(4096, GFP_ATOMIC);
+	if (page == NULL) {
+		pr_info("EmuSGX: Failed to create page\n");
+		return IRQ_HANDLED;
+	}
+
+	// Copy the data to the page
+	memcpy(page, emusgx_receive_page, 4096);
+
+	// Create new node
+	// CANNOT SLEEP!
+	new_node = kmalloc(sizeof(struct emusgx_request_queue_node), GFP_ATOMIC);
+	if (new_node == NULL) {
+		pr_info("EmuSGX: Failed to create node\n");
+		return IRQ_HANDLED;
+	}
+
+	new_node->next = NULL;
+	new_node->page = page;
+
+	// This is a MUST NOT SLEEP context
+	emusgx_debug_print("EmuSGX: IRQ is getting the lock\n");
+	spin_lock_irqsave(&emusgx_dispatcher_queue_lock, flags);
+
+	// Add to the queue
+	if (emusgx_request_queue_tail == NULL) {
+		emusgx_request_queue = emusgx_request_queue_tail = new_node;
+	}
+	else {
+		emusgx_request_queue_tail->next = new_node;
+		emusgx_request_queue_tail = new_node;
+	}
+	
+	up(&(emusgx_dispatcher_sem));
+
+	spin_unlock_irqrestore(&emusgx_dispatcher_queue_lock, flags);
+	emusgx_debug_print("EmuSGX: IRQ released the lock\n");
+	if (stamp_i < 20) {
+		stamps_pushed[stamp_i] = ktime_get_real_ns();
+		stamp_i += 1;
+	}
+	if (stamp_i == 20) {
+		for (stamp_i = 0; stamp_i < 20; stamp_i++) {
+			pr_info("EmuSGX: In = %lld\n", stamps_in[stamp_i]);
+		}
+		for (stamp_i = 0; stamp_i < 20; stamp_i++) {
+			pr_info("EmuSGX: Retrieved = %lld\n", stamps_retrieved[stamp_i]);
+		}
+		for (stamp_i = 0; stamp_i < 20; stamp_i++) {
+			pr_info("EmuSGX: Pushed = %lld\n", stamps_pushed[stamp_i]);
+		}
+		stamp_i = 100;
+	}
+
+	// Send acknowledge
+	// We use KVM_CPUID_EMUSGX_ACK_PAGE to tell the hypervisor we are done
+	asm volatile (
+		"cpuid"
+		: "=b"(cpuid_success)
+		: "a"(KVM_CPUID_EMUSGX_ENCLAVE_ACK_PAGE)
+		: "%rcx", "%rdx"
+	);
+
+	return IRQ_HANDLED;
+}
diff --git a/kernel/emusgx/local_dispatcher.c b/kernel/emusgx/local_dispatcher.c
new file mode 100644
index 000000000..fe0a7b8a2
--- /dev/null
+++ b/kernel/emusgx/local_dispatcher.c
@@ -0,0 +1,118 @@
+#include <linux/kernel.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/semaphore.h>
+#include <linux/ktime.h>
+
+#include <emusgx/emusgx.h>
+#include <emusgx/emusgx_internal.h>
+#include <emusgx/emusgx_debug.h>
+
+static uint64_t stamps_waked[20];
+extern uint64_t stamps_dispatched[20];
+static uint64_t stamps_done[20];
+static uint32_t stamp_i = 0;
+
+void emusgx_local_dispatcher(int manager_nr) {
+	// Every manager has its own local dispatcher
+
+	// Three things to be dispatched locally
+	// o Switchless write
+	// o EENTER
+	// o ENCLS
+	// Memory fetch can be signaled directly to
+	// the waiting thread
+
+	struct emusgx_user_space_manager_entry *manager_entry = emusgx_get_manager(manager_nr);
+	struct emusgx_local_dispatch_queue_node *current_node;
+	void *package;
+	struct emusgx_eenter_package *eenter_package;
+	struct emusgx_raw_response eenter_response;
+	uint64_t response;
+	void *pf_addr;
+	uint8_t instr;
+
+	pr_info("vSGX: Local dispatcher for manager %d is running\n", manager_nr);
+
+	while (true) {
+		// Wait for the semaphore
+		if (down_interruptible(&(manager_entry->local_dispatcher_semaphore))) {
+			emusgx_debug_print("vSGX: Local dispatcher %d received signal...\n", manager_nr);
+			return;
+		}
+		if (stamp_i < 20)
+			stamps_waked[stamp_i] = ktime_get_real_ns();
+
+		// Dequeue the package
+		spin_lock(&(manager_entry->local_dispatch_queue_lock));
+
+		// Be quick
+		// Take out the first node and leave
+		current_node = manager_entry->local_dispatch_queue;
+		if (current_node == NULL) {
+			// Shit happens
+			spin_unlock(&(manager_entry->local_dispatch_queue_lock));
+			pr_err("vSGX: Unexpected error in local dispatch queue of %d\n", manager_nr);
+			continue;
+		}
+
+		package = current_node->package;
+
+		manager_entry->local_dispatch_queue = current_node->next;
+		if (manager_entry->local_dispatch_queue == NULL) {
+			// First node
+			manager_entry->local_dispatch_queue_tail = NULL;
+		}
+
+		kfree(current_node);
+
+		spin_unlock(&(manager_entry->local_dispatch_queue_lock));
+
+		// Now dispatch to the correct function
+		instr = *((uint8_t *)(package));
+		if (instr == EMUSGX_S_SWITCHLESS) {
+			emusgx_switchless_write_page((struct emusgx_page_package *)package);
+		}
+		// else if (instr == EMUSGX_S_PAGEREQ) {
+			// Should not be here
+		// }
+		// encls and enclu
+		// Have to be differentiated
+		else if (instr == EMUSGX_S_EENTER) {
+			// EENTER
+			// Executes the entrance function
+			emusgx_debug_print("vSGX: Dispatch to EENTER\n");
+			eenter_package = package;
+			response = emusgx_enter_enclave((void *)(eenter_package->tcs), (void *)(eenter_package->aep), (eenter_package->pid), &(eenter_package->regs), &pf_addr);
+
+			// We need to send a response
+			eenter_response.instr = EMUSGX_S_EENTER;
+			eenter_response.response = response;
+			eenter_response.linaddr = (uint64_t)pf_addr;
+			eenter_response.write_back = 0;
+			// Send package
+			if (emusgx_send_data(&eenter_response, sizeof(struct emusgx_raw_response))) {
+				pr_info("vSGX: Failed to response eenter\n");
+			}
+		}
+		else {
+			// encls
+			emusgx_debug_print("vSGX: Dispatcher doing ENCLS\n");
+			emusgx_do_remote_for_encls(instr, package);
+			emusgx_debug_print("vSGX: Dispatcher done ENCLS\n");
+		}
+		if (stamp_i < 20)
+			stamps_done[stamp_i] = ktime_get_real_ns();
+		stamp_i += 1;
+		if (stamp_i == 20) {
+			for (stamp_i = 0; stamp_i < 20; stamp_i++) {
+				pr_info("EmuSGX: 9 = %lld\n", stamps_dispatched[stamp_i] - stamps_waked[stamp_i]);
+			}
+			for (stamp_i = 0; stamp_i < 20; stamp_i++) {
+				pr_info("EmuSGX: 10 = %lld\n", stamps_done[stamp_i] - stamps_waked[stamp_i]);
+			}
+			stamp_i = 100;
+		}
+		kfree(package);
+	}
+}
\ No newline at end of file
diff --git a/kernel/emusgx/management.c b/kernel/emusgx/management.c
new file mode 100644
index 000000000..947d7ebba
--- /dev/null
+++ b/kernel/emusgx/management.c
@@ -0,0 +1,337 @@
+#include <linux/kernel.h>
+#include <linux/string.h>
+#include <linux/slab.h>
+#include <linux/atomic.h>
+#include <linux/sched.h>
+
+#include <linux/mm.h>
+#include <linux/mman.h>
+
+#include <emusgx/emusgx.h>
+#include <emusgx/emusgx_internal.h>
+#include <emusgx/emusgx_cpuid.h>
+
+struct emusgx_epcm *emusgx_epcm_array = NULL;
+atomic_long_t vsgx_global_eid = (atomic_long_t)ATOMIC_INIT(0);
+
+uint64_t vsgx_get_new_eid() {
+	return atomic_long_inc_return(&vsgx_global_eid);
+}
+
+int _enclave_local vsgx_check_in_elrange(void __user *addr) {
+	struct sgx_secs *secs = current->secs;
+
+	if ((uint64_t)addr < secs->base || (uint64_t)addr >= secs->base + secs->size) {
+		return 0;
+	}
+
+	return 1;
+}
+
+struct emusgx_epcm *emusgx_get_epcm(void *epc_page) {
+	uint64_t index = 0;
+	if (emusgx_epcm_array == NULL) {
+		// Should not happen
+		pr_info("EmuSGX: Non-initialized enclave\n");
+		return NULL;
+	}
+	if (((void *)emusgx_epc_start == NULL) || ((void *)emusgx_epc_end == NULL)) {
+		// Should not happen
+		pr_info("EmuSGX: Non-initialized enclave\n");
+		return NULL;
+	}
+	if ((uint64_t)epc_page >= emusgx_epc_start && (uint64_t)epc_page < emusgx_epc_end) {
+		index = ((uint64_t)epc_page - emusgx_epc_start) / 4096;
+		return &(emusgx_epcm_array[index]);
+	}
+	pr_info("EmuSGX: I cannot find the epcm for 0x%016llX\n", (uint64_t)epc_page);
+	pr_info("EmuSGX: EPC: 0x%016llX - 0x%016llX\n", (uint64_t)emusgx_epc_start, (uint64_t)emusgx_epc_end);
+	return NULL;
+}
+
+int vsgx_confirm_mapping(void __user *vaddr, void *paddr) {
+	struct emusgx_epcm *epcm;
+
+	epcm = emusgx_get_epcm(paddr);
+	if (epcm == NULL)
+		return -1;
+	
+	if (epcm->enclave_address != vaddr)
+		return -1;
+	return 0;
+}
+
+uint8_t emusgx_init_epcm(uint64_t epc_size) {
+	uint64_t epcm_size = epc_size / 4096;
+	// Double check
+	if (emusgx_epc_end - emusgx_epc_start != epc_size) {
+		pr_info("EmuSGX: EPC size provided does not match\n");
+		return -1;
+	}
+	if (epc_size % 4096 != 0) {
+		pr_info("EmuSGX: EPC size must be n * 4096\n");
+		return -1;
+	}
+	if (emusgx_epcm_array != NULL) {
+		kfree(emusgx_epcm_array);
+	}
+	emusgx_epcm_array = kmalloc(epcm_size * sizeof(struct emusgx_epcm), GFP_KERNEL);
+	if (emusgx_epcm_array == NULL) {
+		pr_info("EmuSGX: Failed to create EPCM. Is your EPC size too large?\n");
+		return -1;
+	}
+	memset(emusgx_epcm_array, 0, epcm_size * sizeof(struct emusgx_epcm));
+	return 0;
+}
+
+uint8_t emusgx_is_secs_inited(void *secs_pa) {
+	// SECS must be checked to be a *real* SECS before using this function
+	struct emusgx_epcm *epcm;
+	epcm = emusgx_get_epcm(secs_pa);
+	return epcm->secs_inited;
+}
+
+void emusgx_mark_secs_inited(struct sgx_secs *secs, uint8_t inited) {
+	// SECS must be checked to be a *real* SECS before using this function
+	struct emusgx_epcm *epcm;
+	epcm = emusgx_get_epcm(secs);
+	epcm->secs_inited = inited;
+}
+
+uint8_t emusgx_secs_has_associated_page(struct sgx_secs *secs_pa) {
+	uint64_t epcm_size = (emusgx_epc_end - emusgx_epc_start) / 4096;
+	int i, count = 0;
+
+	for (i = 0; i < epcm_size; i++) {
+		if (emusgx_epcm_array[i].valid && emusgx_epcm_array[i].enclave_secs == (uint64_t)secs_pa) {
+			count += 1;
+		}
+		if (count > 1) {
+			// Any page except for the SECS itself that is associated with the SECS?
+			return 1;
+		}
+	}
+	return 0;
+}
+
+uint8_t emusgx_check_within_epc(void __user*addr) {
+	if (emusgx_get_epcm(addr) == NULL) {
+		return -1;
+	}
+	return 0;
+}
+
+/*
+int emusgx_start_sender(void) {
+	int cpuid_success;
+
+	asm volatile (
+		"cpuid"
+		: "=b"(cpuid_success)
+		: "a"(KVM_CPUID_EMUSGX_RUN_SENDER_KTHREADS)
+		: "%rcx", "%rdx"
+	);
+	if (!cpuid_success) {
+		pr_info("EmuSGX: CPUID failed");
+		return -EINVAL;
+	}
+	return 0;
+}
+
+int emusgx_stop_sender(void) {
+	int cpuid_success;
+
+	asm volatile (
+		"cpuid"
+		: "=b"(cpuid_success)
+		: "a"(KVM_CPUID_EMUSGX_STOP_SENDER_KTHREADS)
+		: "%rcx", "%rdx"
+	);
+	if (!cpuid_success) {
+		pr_info("EmuSGX: CPUID failed");
+		return -EINVAL;
+	}
+	return 0;
+}*/
+
+int emusgx_check_sender(void) {
+	int cpuid_success;
+	int ret_val;
+
+	asm volatile (
+		"cpuid"
+		: "=b"(cpuid_success), "=c"(ret_val)
+		: "a"(KVM_CPUID_EMUSGX_CHECK_SENDER_KTHREADS)
+		: "%rdx"
+	);
+	if (!cpuid_success) {
+		pr_info("EmuSGX: CPUID failed");
+		return 1;
+	}
+	return ret_val;
+}
+
+// Just like a 64-bit page table
+struct epc_pmd {
+	uint64_t pte[512];
+};
+struct epc_pud {
+	struct epc_pmd *pmd[512];
+};
+struct epc_p4d {
+	struct epc_pud *pud[512];
+};
+struct epc_pgd {
+	struct epc_p4d *p4d[512];
+};
+
+// The mapping of each encalve
+struct epc_pgd global_pgds[EMUSGX_MAXIMUM_ENCLAVES] = { { { (void *)0 } } };
+
+void _enclave_local vsgx_map_epc_to_vaddr(void __user *vaddr, uint64_t epc_addr) {
+	int enclave_nr = current->manager_entry->manager_nr;
+	struct epc_pgd *pgd = &(global_pgds[enclave_nr]);
+	struct epc_p4d *p4d;
+	struct epc_pud *pud;
+	struct epc_pmd *pmd;
+	uint64_t vaddr_int = (uint64_t)vaddr;
+	uint16_t p4d_index = (vaddr_int >> 39) & 0x1FF;
+	uint16_t pud_index = (vaddr_int >> 30) & 0x1FF;
+	uint16_t pmd_index = (vaddr_int >> 21) & 0x1FF;
+	uint16_t pte_index = (vaddr_int >> 12) & 0x1FF;
+
+	if (pgd->p4d[p4d_index] == (void *)0) {
+		// Allocate it
+		pgd->p4d[p4d_index] = kzalloc((sizeof(struct epc_p4d)), GFP_KERNEL);
+	}
+	p4d = pgd->p4d[p4d_index];
+
+	if (p4d->pud[pud_index] == (void *)0) {
+		// Allocate it
+		p4d->pud[pud_index] = kzalloc((sizeof(struct epc_pud)), GFP_KERNEL);
+	}
+	pud = p4d->pud[pud_index];
+
+	if (pud->pmd[pmd_index] == (void *)0) {
+		// Allocate it
+		pud->pmd[pmd_index] = kzalloc((sizeof(struct epc_pmd)), GFP_KERNEL);
+	}
+	pmd = pud->pmd[pmd_index];
+
+	// Warning! Overwritable!
+	if (pmd->pte[pte_index] != 0) {
+		pr_warn("vSGX: Warning! Overwriting older mapping!\n");
+		pr_warn("vSGX: Rewrote mapping in manager %d: 0x%016llX->0x%016llX from 0x%016llX\n", enclave_nr, (uint64_t)vaddr, epc_addr, pmd->pte[pte_index]);
+	}
+	pmd->pte[pte_index] = epc_addr;
+}
+
+void _enclave_local vsgx_unmap_epc_from_vaddr(void __user *vaddr) {
+	int enclave_nr = current->manager_entry->manager_nr;
+	struct epc_pgd *pgd = &(global_pgds[enclave_nr]);
+	struct epc_p4d *p4d;
+	struct epc_pud *pud;
+	struct epc_pmd *pmd;
+	uint64_t vaddr_int = (uint64_t)vaddr;
+	uint16_t p4d_index = (vaddr_int >> 39) & 0x1FF;
+	uint16_t pud_index = (vaddr_int >> 30) & 0x1FF;
+	uint16_t pmd_index = (vaddr_int >> 21) & 0x1FF;
+	uint16_t pte_index = (vaddr_int >> 12) & 0x1FF;
+
+	if (pgd->p4d[p4d_index] == (void *)0) {
+		pr_warn("vSGX: Map not found for 0x%016llX\n", vaddr_int);
+		return;
+	}
+	p4d = pgd->p4d[p4d_index];
+
+	if (p4d->pud[pud_index] == (void *)0) {
+		pr_warn("vSGX: Map not found for 0x%016llX\n", vaddr_int);
+		return;
+	}
+	pud = p4d->pud[pud_index];
+
+	if (pud->pmd[pmd_index] == (void *)0) {
+		pr_warn("vSGX: Map not found for 0x%016llX\n", vaddr_int);
+		return;
+	}
+	pmd = pud->pmd[pmd_index];
+
+	// Warning! Overwritable!
+	if (pmd->pte[pte_index] == 0) {
+		pr_warn("vSGX: Warning! Unmapping a non mapped address for 0x%016llX\n", vaddr_int);
+	}
+	pmd->pte[pte_index] = 0;
+}
+
+uint64_t vsgx_vaddr_to_paddr(void __user *vaddr, int enclave_nr) {
+	struct epc_pgd *pgd = &(global_pgds[enclave_nr]);
+	struct epc_p4d *p4d;
+	struct epc_pud *pud;
+	struct epc_pmd *pmd;
+	uint64_t vaddr_int = (uint64_t)vaddr;
+	uint16_t p4d_index = (vaddr_int >> 39) & 0x1FF;
+	uint16_t pud_index = (vaddr_int >> 30) & 0x1FF;
+	uint16_t pmd_index = (vaddr_int >> 21) & 0x1FF;
+	uint16_t pte_index = (vaddr_int >> 12) & 0x1FF;
+
+	p4d = pgd->p4d[p4d_index];
+	if (p4d == (void *)0) {
+		return 0;
+	}
+
+	pud = p4d->pud[pud_index];
+	if (pud == (void *)0) {
+		return 0;
+	}
+
+	pmd = pud->pmd[pmd_index];
+	if (pmd == (void *)0) {
+		return 0;
+	}
+
+	return pmd->pte[pte_index];
+}
+
+void _enclave_local vsgx_check_and_unmap_page(void __user *linaddr) {
+	long munmap_error;
+	char dummy;
+
+	// First, check if the address is mapped
+	if (!copy_from_user(&dummy, linaddr, 1)) {
+		// Not mapped
+		// Do not have to unmap
+		return;
+	}
+
+	// We have to WRITE
+	// pr_info("EmuSGX: Getting write lock\n");
+	if (mmap_write_lock_killable(current->mm)) {
+		pr_info("vSGX: mmap failed to get lock\n");
+		return;
+	}
+	// pr_info("EmuSGX: Doing mmap @ 0x%016llX\n", page_addr);
+	munmap_error = do_munmap(current->mm, (uint64_t)(linaddr), 4096, NULL);
+	// pr_info("EmuSGX: mmap is done\n");
+	mmap_write_unlock(current->mm);
+}
+
+long _enclave_local vsgx_map_empty_page_to_user_space(void __user *linaddr) {
+	long mmap_error;
+	unsigned long mmap_populate;
+
+	// We have to WRITE
+	// pr_info("EmuSGX: Getting write lock\n");
+	if (mmap_write_lock_killable(current->mm)) {
+		pr_info("vSGX: mmap failed to get lock\n");
+		return -1;
+	}
+	// pr_info("EmuSGX: Doing mmap @ 0x%016llX\n", page_addr);
+	// Here we hack that allows the page to be a full-priviledged page
+	// In production, the PROT bits should be kept the same as the EPCM's restrictions
+	mmap_error = do_mmap(NULL, (unsigned long)linaddr, 4096, PROT_READ | PROT_WRITE | PROT_EXEC, 
+		MAP_FIXED_NOREPLACE | MAP_ANONYMOUS| MAP_PRIVATE, 0, &mmap_populate, NULL);
+	// pr_info("EmuSGX: mmap is done\n");
+	mmap_write_unlock(current->mm);
+
+	return mmap_error;
+}
diff --git a/kernel/emusgx/sender.c b/kernel/emusgx/sender.c
new file mode 100644
index 000000000..1926d9dc1
--- /dev/null
+++ b/kernel/emusgx/sender.c
@@ -0,0 +1,306 @@
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/mm.h>
+#include <linux/slab.h>
+#include <linux/highmem.h>
+#include <linux/string.h>
+#include <linux/spinlock.h>
+
+#include <asm/atomic.h>
+#include <asm/tlbflush.h>
+#include <asm/cacheflush.h>
+#include <asm/io.h>
+
+#include <emusgx/emusgx.h>
+#include <emusgx/emusgx_internal.h>
+#include <emusgx/emusgx_sender.h>
+#include <emusgx/emusgx_mm.h>
+#include <emusgx/emusgx_cpuid.h>
+
+static uint64_t emusgx_session = 0;
+static DEFINE_SPINLOCK(emusgx_session_number_lock);
+
+// Only first 16 bytes are used
+char *emusgx_static_aad = "EmuSGX AAD AAD AAD AAD";
+
+static uint8_t emusgx_unset_c_bit(unsigned long addr, uint8_t is_unset)
+{
+	pgd_t *pgd;
+	p4d_t *p4d;
+	pud_t *pud;
+	pmd_t *pmd;
+	pte_t *pte;
+	struct mm_struct *mm = current->mm;
+
+	// Check if the mm is NULL because a kthread will set mm as NULL
+	// If it's called from a kthread, set mm to init_mm
+	if (mm == NULL) {
+		mm = &init_mm;
+	}
+
+	pgd = pgd_offset(mm, addr);
+	if (pgd_none(*pgd) || pgd_bad(*pgd)) {
+		pr_info("EmuSGX: Page walk failed on PGD\n");
+		goto out;
+	}
+
+	p4d = p4d_offset(pgd, addr);
+	if (p4d_none(*p4d) || p4d_bad(*p4d)) {
+		pr_info("EmuSGX: Page walk failed on P4D");
+    		goto out;
+	}
+
+	pud = pud_offset(p4d, addr);
+	if (pud_none(*pud) || pud_bad(*pud)) {
+		pr_info("EmuSGX: Page walk failed on PUD at 0x%016llX\n", (uint64_t)addr);
+		goto out;
+	}
+
+	pmd = pmd_offset(pud, addr);
+	if (pmd_none(*pmd)) {
+		pr_info("EmuSGX: Page walk failed on PMD - none");
+		goto out;
+	}
+	if (pmd_bad(*pmd)) {
+		pr_info("EmuSGX: Page walk failed on PMD - bad");
+		goto out;
+	}
+
+
+	pte = pte_offset_map(pmd, addr);
+	if (!pte) {
+		pr_info("EmuSGX: Page walk failed on PTE");
+		goto out;
+	}
+	
+	// Unset the 47th "C-bit"
+	if (is_unset) {
+		//pr_info("EmuSGX: PTE mask is 0x%016llX\n", (~((uint64_t)(1) << 47)));
+		//pr_info("EmuSGX: PTE before is 0x%016lX\n", pte->pte);
+		pte->pte &= (~((uint64_t)(1) << 47));
+		pte->pte &= (~((uint64_t)(1) << 63));
+		//pr_info("EmuSGX: PTE is 0x%016lX\n", pte->pte);
+	}
+	else {
+		//pr_info("EmuSGX: PTE mask is 0x%016llX\n", ((uint64_t)(1) << 47));
+		//pr_info("EmuSGX: PTE before is 0x%016lX\n", pte->pte);
+		pte->pte |= ((uint64_t)(1) << 47);
+		pte->pte |= ((uint64_t)(1) << 63);
+		//pr_info("EmuSGX: PTE is 0x%016lX\n", pte->pte);
+	}
+	__flush_tlb_all();
+	asm volatile("clflush %0" : "+m" (*(volatile char __force *)(pte)));
+	mb();
+	rmb();
+	wmb();
+	barrier();
+	return 0;
+
+ out:
+	return -1;
+}
+
+void *emusgx_get_and_share_page(void) {
+	struct page *ret_addr_page = alloc_page(GFP_DMA);
+	void *ret_addr = page_address(ret_addr_page);
+	// uint64_t phys_addr = virt_to_phys(ret_addr);
+	// struct page *ret_valid = pfn_to_page(phys_addr >> PAGE_SHIFT);
+	// char *string_addr = ret_addr;
+	if (ret_addr == NULL) {
+		pr_info("EmuSGX: Page in high mem\n");
+		return NULL;
+	}
+	//pr_info("EmuSGX: real: 0x%016llX, get: 0x%016llX\n", (uint64_t)ret_addr_page, (uint64_t)ret_valid);
+	//memcpy(string_addr, "hello", 6);
+	//pr_info("EmuSGX: %s\n", string_addr);
+	//pr_info("EmuSGX: shared page at 0x%016llX\n", (uint64_t)ret_addr);
+	__flush_tlb_all();
+	asm volatile("clflush %0" : "+m" (*(volatile char __force *)(ret_addr)));
+	mb();
+	rmb();
+	wmb();
+	barrier();
+	// Set the page shared
+	if (emusgx_unset_c_bit((uint64_t)ret_addr, 1)) {
+		pr_info("EmuSGX: Failed to finish page walk for the sharing page\n");
+		free_page((uint64_t)ret_addr);
+		return NULL;
+	}
+	__flush_tlb_all();
+	asm volatile("clflush %0" : "+m" (*(volatile char __force *)(ret_addr)));
+	mb();
+	rmb();
+	wmb();
+	barrier();
+	//string_addr[6] = 0;
+	//pr_info("EmuSGX: %s\n", string_addr);
+	//pr_info("EmuSGX: Encrypted hello signature = 0x%08X\n", ((uint32_t *)string_addr)[0]);
+	asm volatile("clflush %0" : "+m" (*(volatile char __force *)(ret_addr)));
+	mb();
+	rmb();
+	wmb();
+	barrier();
+	return ret_addr;
+}
+
+void emusgx_init_shared_page(void) {
+	uint32_t cpuid_success;
+	uint64_t physical_page_addr;
+
+	emusgx_receive_page = emusgx_get_and_share_page();
+
+	physical_page_addr = virt_to_phys(emusgx_receive_page);
+
+	asm volatile (
+		"cpuid"
+		: "=b"(cpuid_success)
+		: "a"(KVM_CPUID_EMUSGX_ENCLAVE_SHARE_PAGE), "c"(physical_page_addr)
+		: "%rdx"
+	);
+
+	if (!cpuid_success) {
+		pr_info("EmuSGX: CPUID failed\n");
+	}
+}
+
+void emusgx_free_shared_page(void *page_addr) {
+	//pr_info("EmuSGX: Freeing page... Setting C-bit\n");
+	emusgx_unset_c_bit((uint64_t)page_addr, 0);
+	//pr_info("EmuSGX: C-bit set back\n");
+	mb();
+	rmb();
+	wmb();
+	barrier();
+	__flush_tlb_all();
+	//pr_info("EmuSGX: TLB flushed. Now freeing\n");
+	free_page((uint64_t)page_addr);
+	//pr_info("EmuSGX: Page freed\n");
+}
+
+/*
+static void emusgx_full_flush_page_cache(void *addr) {
+	int i;
+
+	for (i = 0; i < 4096; i++) {
+		asm volatile("clflush %0" : "+m" (*(volatile char __force *)(&(((char *)addr)[i]))));
+	}
+
+	mb();
+	rmb();
+	wmb();
+	barrier();
+} */
+
+static int emusgx_send_single_page(void *addr, uint64_t size) {
+	void *shared_page = NULL;
+	uint64_t physical_page_addr;
+	uint64_t rax_val;
+	uint32_t cpuid_success = 0;
+	int i;
+	
+	if (size > 4096) {
+		pr_info("EmuSGX: Data too large in pack_to_page\n");
+		return -EINVAL;
+	}
+
+	shared_page = emusgx_get_and_share_page();
+
+	for (i = 0; i < 10 && shared_page == NULL; i++) {
+		shared_page = emusgx_get_and_share_page();
+	}
+
+	if (shared_page == NULL) {
+		pr_info("EmuSGX: Failed to create shared page\n");
+		return -EINVAL;
+	}
+
+	memcpy(shared_page, addr, size);
+
+	physical_page_addr = virt_to_phys(shared_page);
+
+	// Make a full cache flush
+	// emusgx_full_flush_page_cache(shared_page);
+	clflush_cache_range(shared_page, 4096);
+	//wbinvd_on_all_cpus();
+
+	rax_val = ((uint64_t)(*((uint32_t *)(shared_page + 4092)))) << 32;
+	rax_val |= KVM_CPUID_EMUSGX_ENCLAVE_SEND_PAGE;
+		
+	// We use KVM_CPUID_EMUSGX_SEND_PAGE to send data to hypervisor
+	// The signature is the last 4 bytes of the page
+	asm volatile (
+		"cpuid"
+		: "=b"(cpuid_success)
+		: "a"(rax_val), "c"(physical_page_addr)
+		: "%rdx"
+	);
+	emusgx_free_shared_page(shared_page);
+	if (!cpuid_success) {
+		pr_info("EmuSGX: CPUID failed");
+		return -EINVAL;
+	}
+	return 0;
+}
+
+int emusgx_send_data(void *addr, uint64_t size) {
+	uint64_t turns;
+	uint64_t final;
+	uint64_t i;
+	uint64_t total_packages;
+	struct emusgx_cross_vm_package *package;
+	struct emusgx_cross_vm_package *plain_package;
+	int current_session;
+	int encrypt_ret;
+	uint64_t iv = 0;
+
+	package = kmalloc(sizeof(struct emusgx_cross_vm_package), GFP_KERNEL);
+	plain_package = kmalloc(sizeof(struct emusgx_cross_vm_package), GFP_KERNEL);
+
+	// First we get our session number
+	// Can-sleep environment
+	spin_lock(&emusgx_session_number_lock);
+	current_session = emusgx_session++;
+	if (emusgx_session >= EMUSGX_MAX_SESSION_NUMBER) {
+		// Reset session number
+		emusgx_session = 0;
+	}
+	spin_unlock(&emusgx_session_number_lock);
+
+	// Next pack data into pages and send them
+	turns = size / EMUSGX_PAYLOAD_SIZE;
+	final = size % EMUSGX_PAYLOAD_SIZE;
+	total_packages = turns + ((final != 0) ? 1 : 0);
+	for (i = 0; i < total_packages; i++) {
+		// Capsulate the data
+		plain_package->session_number = current_session;
+		plain_package->order = i;
+		plain_package->total_pages = total_packages;
+		plain_package->total_size = size;
+
+		// Copy data
+		if (i == total_packages - 1) {
+			memcpy(&(plain_package->payload), addr + (i * EMUSGX_PAYLOAD_SIZE), final);
+		}
+		else {
+			memcpy(&(plain_package->payload), addr + (i * EMUSGX_PAYLOAD_SIZE), EMUSGX_PAYLOAD_SIZE);
+		}
+
+		// Encrypt and get MAC
+		encrypt_ret = emusgx_aes_128_gcm_enc(emusgx_internal_cr_cross_vm_key, &iv, emusgx_static_aad, 16, plain_package, 4096 - 16, package, &(package->mac[0]));
+		if (encrypt_ret != 0) {
+			pr_info("EmuSGX: Unexpected encryption issue\n");
+			kfree(package);
+			kfree(plain_package);
+			return -EINVAL;
+		}
+
+		// Send single page
+		if (emusgx_send_single_page(package, 4096)) {
+			kfree(package);
+			kfree(plain_package);
+			return -EINVAL;
+		}
+	}
+	
+	return 0;
+}
diff --git a/kernel/emusgx/switchless_sync.c b/kernel/emusgx/switchless_sync.c
new file mode 100644
index 000000000..4ce2c1402
--- /dev/null
+++ b/kernel/emusgx/switchless_sync.c
@@ -0,0 +1,395 @@
+#include <linux/kernel.h>
+#include <linux/spinlock.h>
+#include <linux/uaccess.h>
+#include <linux/semaphore.h>
+#include <linux/mm.h>
+#include <linux/slab.h>
+#include <linux/sched.h>
+#include <linux/delay.h>
+
+#include <emusgx/emusgx.h>
+#include <emusgx/emusgx_internal.h>
+#include <emusgx/emusgx_sender.h>
+#include <emusgx/emusgx_debug.h>
+
+#define EMUSGX_SWITCHLESS_SLOT_FREE	0
+#define EMUSGX_SWITCHLESS_SLOT_INUSE	1
+
+struct emusgx_switchless_page_slot {
+	uint8_t status;
+	spinlock_t lock;
+	void *addr;
+	uint8_t *original_content;
+};
+
+static uint64_t emusgx_switchless_sync_index[EMUSGX_MAXIMUM_ENCLAVES] = { 0 };
+static DEFINE_SPINLOCK(emusgx_switchless_index_lock);
+
+struct emusgx_switchless_page_slot emusgx_switchless_pages[EMUSGX_MAXIMUM_ENCLAVES][EMUSGX_SWITCHLESS_SLOT_COUNT] = { [0 ... EMUSGX_MAXIMUM_ENCLAVES - 1] = { [0 ... EMUSGX_SWITCHLESS_SLOT_COUNT - 1] = { .status = EMUSGX_SWITCHLESS_SLOT_FREE, .lock = __SPIN_LOCK_UNLOCKED(emusgx_switchless_lock), .addr = NULL, .original_content = NULL } } };
+
+void emusgx_switchless_write_page(struct emusgx_page_package *package) {
+	// Make sure the page still exists
+	int i, group, bit;
+	uint64_t manager_nr = package->id;
+	for (i = 0; i < EMUSGX_SWITCHLESS_SLOT_COUNT; i++) {
+		// First grab the lock. Can sleep
+		spin_lock(&emusgx_switchless_pages[manager_nr][i].lock);
+
+		if (emusgx_switchless_pages[manager_nr][i].status != EMUSGX_SWITCHLESS_SLOT_INUSE) {
+			spin_unlock(&emusgx_switchless_pages[manager_nr][i].lock);
+			// Go ahead
+			continue;
+		}
+
+		if ((uint64_t)(emusgx_switchless_pages[manager_nr][i].addr) != package->addr) {
+			spin_unlock(&emusgx_switchless_pages[manager_nr][i].lock);
+			// Go ahead
+			continue;
+		}
+		else {
+			goto copy_data;
+			// Break with lock held so no one can free that page anyway
+			// i is the slot
+		}
+	}
+
+	// Not found
+	return;
+
+copy_data:
+	// Check dirty?
+
+	// Copy the data
+	// We just copy the page to the corresponding address on our own
+	__uaccess_begin();
+
+	for (group = 0; group < 512; group++) {
+		if (package->mask[group] == 0) // if a group is unmodified, then jump over it
+			continue;
+		for (bit = 0; bit < 8; bit++) {
+			if (((package->mask[group]) >> bit) & 1) {
+				((uint8_t *)(package->addr))[group * 8 + bit] = package->page[group * 8 + bit];
+				emusgx_switchless_pages[manager_nr][i].original_content[group * 8 + bit] = package->page[group * 8 + bit];
+			}
+		}
+	}
+
+	__uaccess_end();
+
+	// Release the lock
+	spin_unlock(&emusgx_switchless_pages[manager_nr][i].lock);
+
+	// We do not clear the dirty bit since there's no way to tell if ant
+	// threads changed our data during the writing process
+	// We leave the task of checking if the page is real dirty to
+	// emusgx_sync_on_dirty
+}
+
+// Must be called with the slot locked!
+// Only the owner manager can call this!
+int _enclave_local emusgx_sync_on_dirty(int slot) {
+	uint64_t manager_nr = current->manager_entry->manager_nr;
+	uint64_t addr = (uint64_t)emusgx_switchless_pages[manager_nr][slot].addr;
+	struct emusgx_page_package *package;
+	int group, bit, need_to_send;
+
+	// Get PTE
+	pgd_t *pgd;
+	p4d_t *p4d;
+	pud_t *pud;
+	pmd_t *pmd;
+	pte_t *pte;
+	struct mm_struct *mm = current->mm;
+
+	pgd = pgd_offset(mm, addr);
+	if (pgd_none(*pgd) || pgd_bad(*pgd)) {
+		pr_info("EmuSGX: Page walk failed on PGD\n");
+		goto out;
+	}
+
+	p4d = p4d_offset(pgd, addr);
+	if (p4d_none(*p4d) || p4d_bad(*p4d)) {
+		pr_info("EmuSGX: Page walk failed on  P4D");
+    		goto out;
+	}
+
+	pud = pud_offset(p4d, addr);
+	if (pud_none(*pud) || pud_bad(*pud)) {
+		pr_info("EmuSGX: Page walk failed on PUD at 0x%016llX when syncing on dirty\n", (uint64_t)addr);
+		goto out;
+	}
+
+	pmd = pmd_offset(pud, addr);
+	if (pmd_none(*pmd)) {
+		pr_info("EmuSGX: Page walk failed on PMD - none");
+		goto out;
+	}
+	if (pmd_bad(*pmd)) {
+		pr_info("EmuSGX: Page walk failed on PMD - bad");
+		goto out;
+	}
+
+	pte = pte_offset_map(pmd, addr);
+	if (!pte) {
+		pr_info("EmuSGX: Page walk failed on PTE");
+		goto out;
+	}
+
+	if (!pte_present(*pte)) {
+		pr_info("EmuSGX: PTE not presenting\n");
+		goto out;
+	}
+
+	// Check if the PTE is dirty
+	if (pte_dirty(*pte)) {
+		// Mark the page clear
+		set_pte(pte, pte_mkclean(*pte));
+		// Force a sync if huge
+out:
+		emusgx_debug_print("EmuSGX: Ever dirty?\n");
+		package = kmalloc(sizeof(struct emusgx_page_package), GFP_KERNEL);
+		if (package == NULL) {
+			pr_info("EmuSGX: Failed to allocate package when swapping out the sync slot\n");
+			return -1;
+		}
+
+		__uaccess_begin();
+
+		// Write the mask
+		need_to_send = 0;
+		for (group = 0; group < 512; group++) {
+			package->mask[group] = 0;
+			for (bit = 0; bit < 8; bit++) {
+				package->page[group * 8 + bit] = ((uint8_t *)(emusgx_switchless_pages[manager_nr][slot].addr))[group * 8 + bit];
+				if (emusgx_switchless_pages[manager_nr][slot].original_content[group * 8 + bit] != package->page[group * 8 + bit]) {
+					// Set bit 1
+					package->mask[group] |= ((uint64_t)1 << bit);
+					emusgx_switchless_pages[manager_nr][slot].original_content[group * 8 + bit] = package->page[group * 8 + bit];
+					need_to_send = 1;
+				}
+			}
+		}
+
+		__uaccess_end();
+
+		if (need_to_send) {
+			package->instr = EMUSGX_S_SWITCHLESS;
+			package->addr = (uint64_t)emusgx_switchless_pages[manager_nr][slot].addr;
+			package->id = manager_nr;
+			emusgx_debug_print("EmuSGX: Syncing page...\n");
+			if (emusgx_send_data(package, sizeof(struct emusgx_page_package))) {
+				pr_info("EmuSGX: Failed to send package for swapped out page\n");
+				kfree(package);
+				return -1;
+			}
+		}
+
+		kfree(package);
+
+
+		return 0;
+	}
+
+	// The page is corrupted?
+	// Which should not happen
+	return -1;
+}
+
+void emusgx_clear_dirty(void *address) {
+	uint64_t addr = (uint64_t)address;
+
+	// Get PTE
+	pgd_t *pgd;
+	p4d_t *p4d;
+	pud_t *pud;
+	pmd_t *pmd;
+	pte_t *pte;
+	struct mm_struct *mm = current->mm;
+
+	pgd = pgd_offset(mm, addr);
+	if (pgd_none(*pgd) || pgd_bad(*pgd)) {
+		pr_info("EmuSGX: Page walk failed on PGD\n");
+		goto out;
+	}
+
+	p4d = p4d_offset(pgd, addr);
+	if (p4d_none(*p4d) || p4d_bad(*p4d)) {
+		pr_info("EmuSGX: Page walk failed on  P4D");
+    		goto out;
+	}
+
+	pud = pud_offset(p4d, addr);
+	if (pud_none(*pud) || pud_bad(*pud)) {
+		pr_info("EmuSGX: Page walk failed on PUD at 0x%016llX when making clean\n", (uint64_t)addr);
+		goto out;
+	}
+
+	pmd = pmd_offset(pud, addr);
+	if (pmd_none(*pmd)) {
+		pr_info("EmuSGX: Page walk failed on PMD - none");
+		goto out;
+	}
+	if (pmd_bad(*pmd)) {
+		pr_info("EmuSGX: Page walk failed on PMD - bad");
+		goto out;
+	}
+
+	pte = pte_offset_map(pmd, addr);
+	if (!pte) {
+		pr_info("EmuSGX: Page walk failed on PTE");
+		goto out;
+	}
+
+	if (!pte_present(*pte)) {
+		pr_info("EmuSGX: PTE not presenting\n");
+		goto out;
+	}
+
+	set_pte(pte, pte_mkclean(*pte));
+out:
+	;
+}
+
+int _enclave_local emusgx_switchless_new_slot(void *addr, void *page_data) {
+	// The addr should have already been mapped and have data in it
+	// This should be the final step of a good page fault
+	// Caller must be the registering enclave manager
+
+	// When new a slot, several things are done
+	// If we are taking a free slot then just copy data into it will be good enough
+	// If we are taking an occupied slot, then we need to
+	// 1. Check if the slot is dirty, if so, sync the data 
+	//    to the guest VM
+	// 2. Unmap it from the memory
+	// 3. Update the slot
+	// We use a FIFO policy and the index will keep going up to max slot
+	// then reset
+
+	int munmap_error;
+	uint64_t index;
+	uint64_t manager_nr = current->manager_entry->manager_nr;
+
+	if (manager_nr >= EMUSGX_MAXIMUM_ENCLAVES) {
+		pr_info("vSGX: Switchless new slot for invalid manager %lld\n", manager_nr);
+		return -1;
+	}
+
+	spin_lock(&emusgx_switchless_index_lock);
+
+	index = emusgx_switchless_sync_index[manager_nr];
+	emusgx_switchless_sync_index[manager_nr] += 1;
+	if (emusgx_switchless_sync_index[manager_nr] >= EMUSGX_SWITCHLESS_SLOT_COUNT) {
+		emusgx_switchless_sync_index[manager_nr] = 0;
+	}
+
+	spin_unlock(&emusgx_switchless_index_lock);
+
+	// First, get the slot
+	// Can-sleep
+	spin_lock(&emusgx_switchless_pages[manager_nr][index].lock);
+
+	if (emusgx_switchless_pages[manager_nr][index].status != EMUSGX_SWITCHLESS_SLOT_FREE) {
+		// Sync for the last time
+		emusgx_sync_on_dirty(index);
+		// Unmap the original addr from the memory
+		if (mmap_write_lock_killable(current->mm)) {
+			pr_info("EmuSGX: mmap failed to get lock\n");
+			spin_unlock(&emusgx_switchless_pages[manager_nr][index].lock);
+			return -1;
+		}
+		munmap_error = do_munmap(current->mm, (uint64_t)emusgx_switchless_pages[manager_nr][index].addr, 4096, NULL);
+		mmap_write_unlock(current->mm);
+		if (munmap_error) {
+			pr_info("EmuSGX: Unexpected non-mapped address in an inuse sync slot\n");
+		}
+	}
+	else {
+		emusgx_switchless_pages[manager_nr][index].original_content = NULL;
+	}
+
+	// Update the slot
+	emusgx_switchless_pages[manager_nr][index].status = EMUSGX_SWITCHLESS_SLOT_INUSE;
+	emusgx_switchless_pages[manager_nr][index].addr = addr;
+	// Lazy allocate, reuse if not NULL
+	if (emusgx_switchless_pages[manager_nr][index].original_content == NULL) {
+		emusgx_switchless_pages[manager_nr][index].original_content = kmalloc(4096, GFP_KERNEL);
+	}
+	if (emusgx_switchless_pages[manager_nr][index].original_content == NULL) {
+		pr_info("EmuSGX: I cannot allocate data for the original_content\n");
+		return -1;
+	}
+	// Update the original_content field
+	// Note that the process won't be able to write to the page
+	// at this moment because this IS the process
+	memcpy(emusgx_switchless_pages[manager_nr][index].original_content, page_data, 4096);
+
+	spin_unlock(&emusgx_switchless_pages[manager_nr][index].lock);
+
+	// The slot is ready
+
+	return 0;
+}
+
+int emusgx_switchless_get_slot(void *addr, uint64_t manager_nr) {
+	// Make sure the page still exists
+	int i;
+	for (i = 0; i < EMUSGX_SWITCHLESS_SLOT_COUNT; i++) {
+		spin_lock(&emusgx_switchless_pages[manager_nr][i].lock);
+
+		if (emusgx_switchless_pages[manager_nr][i].status == EMUSGX_SWITCHLESS_SLOT_INUSE) {
+			if (emusgx_switchless_pages[manager_nr][i].addr == addr) {
+				spin_unlock(&emusgx_switchless_pages[manager_nr][i].lock);
+				return i;
+			}
+		}
+
+		spin_unlock(&emusgx_switchless_pages[manager_nr][i].lock);
+			
+	}
+	return -1;
+}
+
+int emusgx_switchless_get_and_hold_slot(void *addr, uint64_t manager_nr) {
+	// Make sure the page still exists
+	int i;
+	for (i = 0; i < EMUSGX_SWITCHLESS_SLOT_COUNT; i++) {
+		spin_lock(&emusgx_switchless_pages[manager_nr][i].lock);
+
+		if (emusgx_switchless_pages[manager_nr][i].status == EMUSGX_SWITCHLESS_SLOT_INUSE) {
+			if (emusgx_switchless_pages[manager_nr][i].addr == addr) {
+				// spin_unlock(&emusgx_switchless_pages[manager_nr][i].lock);
+				return i;
+			}
+		}
+
+		spin_unlock(&emusgx_switchless_pages[manager_nr][i].lock);
+			
+	}
+	return -1;
+}
+
+void _enclave_local emusgx_sync_all_pages(void) {
+	int i;
+	uint64_t manager_nr = current->manager_entry->manager_nr;
+	for (i = 0; i < EMUSGX_SWITCHLESS_SLOT_COUNT; i++) {
+		spin_lock(&emusgx_switchless_pages[manager_nr][i].lock);
+		if (emusgx_switchless_pages[manager_nr][i].status == EMUSGX_SWITCHLESS_SLOT_INUSE)
+			emusgx_sync_on_dirty(i);
+		spin_unlock(&emusgx_switchless_pages[manager_nr][i].lock);
+		// We don't care successful or not since we are doing lazy syncing
+	}
+}
+
+void _enclave_local emusgx_switchless_sync_worker(void) {
+	// Does not expect to return
+
+	pr_info("vSGX: Switchless sync worker for manager %lld is running\n", current->manager_entry->manager_nr);
+
+	while(true) {
+		// every 100 ms we sync all 10 slots on demand
+		emusgx_sync_all_pages();
+
+		// now wait for 100 ms
+		msleep(100);
+	}
+}
diff --git a/kernel/fork.c b/kernel/fork.c
index c675fdbd3..3dbe7d62d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1991,6 +1991,15 @@ static __latent_entropy struct task_struct *copy_process(
 	p->vfork_done = NULL;
 	spin_lock_init(&p->alloc_lock);
 
+	// An enclave thread does not fork
+	// It cannot even do syscalls
+	p->is_enclave_thread = 0;
+
+	// A forked child thread shares the same manager entry as their parent's
+	p->manager_entry = current->manager_entry;
+	p->secs = current->secs;
+	p->secs_pa = current->secs_pa;
+
 	init_sigpending(&p->pending);
 
 	p->utime = p->stime = p->gtime = 0;
-- 
2.25.1


From 5352bc92bd3d7b9f1e7ea4b63380d9de8aa54dab Mon Sep 17 00:00:00 2001
From: NSKernel <shixuan.zhao@hotmail.com>
Date: Mon, 22 Mar 2021 17:02:04 +0800
Subject: [PATCH 2/9] Preliminary support for one-VM-per-enclave

---
 arch/x86/kernel/traps.c        |  4 +--
 include/emusgx/emusgx_sender.h | 12 +++++++-
 kernel/emusgx/dispatcher.c     |  9 ++++++
 kernel/emusgx/sender.c         | 56 +++++++++++++++++++++++++++++++++-
 4 files changed, 77 insertions(+), 4 deletions(-)

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 63c6c4c37..56eae6db0 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -347,8 +347,8 @@ DEFINE_IDTENTRY_RAW(exc_invalid_op)
 						regs->ax = -1;
 					}
 
-					// Initialize the receive page
-					// and IRQ
+					// Initialize the receive page and IRQ
+					// It will also send the encalve VM ID to the guest VM
 					emusgx_init_shared_page();
 					err = request_irq(EMUSGX_IRQ, emusgx_irq_handler, 0, "emusgx_irq_response", NULL);
 					if (err < 0) {
diff --git a/include/emusgx/emusgx_sender.h b/include/emusgx/emusgx_sender.h
index cadbd98af..e101dd8db 100644
--- a/include/emusgx/emusgx_sender.h
+++ b/include/emusgx/emusgx_sender.h
@@ -3,7 +3,7 @@
 
 #include <linux/semaphore.h>
 
-#define EMUSGX_PAYLOAD_SIZE (4096 - 6 * sizeof(uint64_t))
+#define EMUSGX_PAYLOAD_SIZE (4096 - 7 * sizeof(uint64_t))
 
 #define EMUSGX_MAX_SESSION_NUMBER_D64	512
 #define EMUSGX_MAX_SESSION_NUMBER	(EMUSGX_MAX_SESSION_NUMBER_D64 * 64)
@@ -40,6 +40,8 @@
 
 #define EMUSGX_S_AEX		0x21
 
+#define EMUSGX_S_REGISTER_EVM	0x22
+
 #include <linux/kernel.h>
 #include <linux/spinlock.h>
 #include <linux/semaphore.h>
@@ -54,6 +56,7 @@ struct emusgx_cross_vm_package {
 	uint64_t order;
 	uint64_t total_pages;
 	uint64_t total_size;
+	uint64_t enclave_vm_id;
 
 	uint8_t payload[EMUSGX_PAYLOAD_SIZE];
 
@@ -152,6 +155,8 @@ extern struct emusgx_request_queue_node *emusgx_request_queue_tail;
 
 extern char *emusgx_static_aad;
 
+extern uint64_t emusgx_enclave_vm_id;
+
 struct emusgx_raw_response {
 	uint8_t instr;
 	uint64_t response;
@@ -274,6 +279,11 @@ struct emusgx_aex_package {
 	uint64_t fault_addr;
 } __attribute__((__packed__));
 
+struct emusgx_register_enclave_vm_package {
+	uint8_t instr;
+	uint64_t enclave_vm_id;
+} __attribute__((__packed__));
+
 #define EMUSGX_IRQ 				10
 
 #endif
\ No newline at end of file
diff --git a/kernel/emusgx/dispatcher.c b/kernel/emusgx/dispatcher.c
index bdb464b21..a002a56be 100644
--- a/kernel/emusgx/dispatcher.c
+++ b/kernel/emusgx/dispatcher.c
@@ -497,6 +497,15 @@ int emusgx_dispatcher(void *dummy) {
 
 		kfree(cipher_package);
 		kfree(current_node);
+		
+		// Check if the package is for the current enclave VM
+		if (plain_package->enclave_vm_id != emusgx_enclave_vm_id) {
+			// Drop the current package
+			pr_info("EmuSGX: Package not for the current enclave VM\n");
+			pr_info("EmuSGX: Possible malicious hypervisor\n");
+			kfree(plain_package);
+			continue;
+		}
 
 		// Now we have the package, to prevent replay attack,
 		// we check the session number
diff --git a/kernel/emusgx/sender.c b/kernel/emusgx/sender.c
index 1926d9dc1..c0bb73bf6 100644
--- a/kernel/emusgx/sender.c
+++ b/kernel/emusgx/sender.c
@@ -20,6 +20,9 @@
 static uint64_t emusgx_session = 0;
 static DEFINE_SPINLOCK(emusgx_session_number_lock);
 
+// Should be generated randomly
+uint64_t emusgx_enclave_vm_id = 0;
+
 // Only first 16 bytes are used
 char *emusgx_static_aad = "EmuSGX AAD AAD AAD AAD";
 
@@ -146,11 +149,58 @@ void *emusgx_get_and_share_page(void) {
 void emusgx_init_shared_page(void) {
 	uint32_t cpuid_success;
 	uint64_t physical_page_addr;
+	int current_session;
+	int encrypt_ret;
+	uint64_t iv = 0;
+
+	struct emusgx_cross_vm_package *encrypted_package;
+	struct emusgx_cross_vm_package *plain_package;
+
+	struct emusgx_register_enclave_vm_package package;
+
+	package->instr = EMUSGX_S_REGISTER_EVM;
+	package->enclave_vm_id = emusgx_enclave_vm_id;
+
+	encrypted_package = kmalloc(sizeof(struct emusgx_cross_vm_package), GFP_KERNEL);
+	plain_package = kmalloc(sizeof(struct emusgx_cross_vm_package), GFP_KERNEL);
+
+	// The very first session number
+	// Can-sleep environment
+	spin_lock(&emusgx_session_number_lock);
+	current_session = emusgx_session++;
+	if (emusgx_session >= EMUSGX_MAX_SESSION_NUMBER) {
+		// Reset session number
+		emusgx_session = 0;
+	}
+	spin_unlock(&emusgx_session_number_lock);
+
+	// Capsulate the data
+	plain_package->session_number = current_session;
+	plain_package->order = 0;
+	plain_package->total_pages = 1;
+	plain_package->total_size = sizeof(struct emusgx_register_enclave_vm_package);
+	plain_package->enclave_vm_id = emusgx_enclave_vm_id;
+
+	// Copy data
+	memcpy(&(plain_package->payload), &package, sizeof(struct emusgx_register_enclave_vm_package));
+
+	// Encrypt and get MAC
+	encrypt_ret = emusgx_aes_128_gcm_enc(emusgx_internal_cr_cross_vm_key, &iv, emusgx_static_aad, 16, plain_package, 4096 - 16, encrypted_package, &(encrypted_package->mac[0]));
+	if (encrypt_ret != 0) {
+		pr_info("EmuSGX: Unexpected encryption issue\n");
+		kfree(encrypted_package);
+		kfree(plain_package);
+		return -EINVAL;
+	}
 
 	emusgx_receive_page = emusgx_get_and_share_page();
 
+	memcpy(emusgx_receive_page, encrypted_package, 4096);
+
 	physical_page_addr = virt_to_phys(emusgx_receive_page);
 
+	clflush_cache_range(shared_page, 4096);
+
 	asm volatile (
 		"cpuid"
 		: "=b"(cpuid_success)
@@ -276,6 +326,7 @@ int emusgx_send_data(void *addr, uint64_t size) {
 		plain_package->order = i;
 		plain_package->total_pages = total_packages;
 		plain_package->total_size = size;
+		plain_package->enclave_vm_id = emusgx_enclave_vm_id;
 
 		// Copy data
 		if (i == total_packages - 1) {
@@ -301,6 +352,9 @@ int emusgx_send_data(void *addr, uint64_t size) {
 			return -EINVAL;
 		}
 	}
-	
+
+	kfree(package);
+	kfree(plain_package);
+
 	return 0;
 }
-- 
2.25.1


From 4cf880a55680b5cdcd6aaef1333685e6c4992f57 Mon Sep 17 00:00:00 2001
From: NSKernel <shixuan.zhao@hotmail.com>
Date: Wed, 24 Mar 2021 16:53:55 +0800
Subject: [PATCH 3/9] Revised EWB/EPA/ELDX for one VM per enclave

---
 include/emusgx/emusgx_internal.h |   2 +
 include/emusgx/emusgx_sender.h   |  37 +++++++-
 kernel/emusgx/cr.c               |   4 +
 kernel/emusgx/encls_cross_vm.c   | 154 ++++++++++++++++++++++++++-----
 4 files changed, 172 insertions(+), 25 deletions(-)

diff --git a/include/emusgx/emusgx_internal.h b/include/emusgx/emusgx_internal.h
index 33aa9d573..216c9353e 100644
--- a/include/emusgx/emusgx_internal.h
+++ b/include/emusgx/emusgx_internal.h
@@ -135,6 +135,8 @@ int emusgx_aes_128_gcm_enc(uint8_t *key, uint64_t *counter, void *aad, size_t aa
 
 extern uint8_t *emusgx_internal_cr_cross_vm_key;
 
+extern uint8_t *emusgx_internal_cr_va_key;
+
 struct emusgx_user_space_manager_entry *emusgx_register_manager(pid_t current_pid);
 struct emusgx_user_space_manager_entry *emusgx_get_free_manager(void);
 struct emusgx_user_space_manager_entry *emusgx_get_manager(int manager_nr);
diff --git a/include/emusgx/emusgx_sender.h b/include/emusgx/emusgx_sender.h
index e101dd8db..301ee7ee3 100644
--- a/include/emusgx/emusgx_sender.h
+++ b/include/emusgx/emusgx_sender.h
@@ -161,15 +161,41 @@ struct emusgx_raw_response {
 	uint8_t instr;
 	uint64_t response;
 	uint64_t linaddr;
-	uint8_t write_back; // Must be 0
+	uint8_t write_back;
+	uint8_t with_va;
 } __attribute__((__packed__));
 
+struct emusgx_raw_response_with_va {
+	uint8_t instr;
+	uint64_t response;
+	uint64_t linaddr;
+	uint8_t write_back;
+	uint8_t with_va;
+	uint8_t va_page[4096];
+	uint64_t va_mac[2];
+} __attribute__((__packed__));
+
+/*
 struct emusgx_raw_response_with_page {
 	uint8_t instr;
 	uint64_t response;
 	uint64_t linaddr;
-	uint8_t write_back; // Must be 1
-	uint8_t page[4096]; 
+	uint8_t write_back;
+	uint8_t with_va;
+	uint8_t page[4096];
+	struct sgx_pcmd pcmd;
+} __attribute__((__packed__));
+*/
+
+struct emusgx_raw_response_with_page_and_va {
+	uint8_t instr;
+	uint64_t response;
+	uint64_t linaddr;
+	uint8_t write_back;
+	uint8_t with_va;
+	uint8_t va_page[4096];
+	uint64_t va_mac[2];
+	uint8_t page[4096];
 	struct sgx_pcmd pcmd;
 } __attribute__((__packed__));
 
@@ -221,6 +247,8 @@ struct emusgx_eldb_eldu_package {
 	struct sgx_pcmd pcmd;
 	uint64_t linaddr;
 	uint8_t block;
+	uint8_t va_page[4096];
+	uint64_t va_mac[2];
 } __attribute__((__packed__));
 
 struct emusgx_emodpr_package {
@@ -254,8 +282,9 @@ struct emusgx_ewb_package {
 	uint64_t epc_page;
 	uint64_t vaslot;
 	uint64_t linaddr;
+	uint8_t va_page[4096];
+	uint64_t va_mac[2];
 } __attribute__((__packed__));
-
 struct emusgx_eexit_package {
 	uint8_t instr;
 	uint64_t pid;
diff --git a/kernel/emusgx/cr.c b/kernel/emusgx/cr.c
index ef05f4298..0eb3eeb77 100644
--- a/kernel/emusgx/cr.c
+++ b/kernel/emusgx/cr.c
@@ -31,6 +31,10 @@ atomic_t emusgx_cr_inited = (atomic_t)ATOMIC_INIT(0);
 // shall be set before deploy
 uint8_t *emusgx_internal_cr_cross_vm_key = "EmuSGX Cross VM";
 
+// TODO: 16 bytes key for VA encryption. In production
+// environment shall be set before deploy
+uint8_t *emusgx_internal_cr_va_key = "EmuSGX Crypt VA";
+
 struct emusgx_user_space_manager_entry emusgx_user_space_manager[EMUSGX_MAXIMUM_ENCLAVES] = { [0 ... EMUSGX_MAXIMUM_ENCLAVES - 1] = { .status = EMUSGX_MGRSTAT_EMPTY, .local_dispatch_queue = NULL, .local_dispatch_queue_tail = NULL, .local_dispatch_queue_lock = __SPIN_LOCK_UNLOCKED(emusgx_local_dispatch_queue_lock) } };
 
 struct emusgx_user_space_manager_entry *emusgx_register_manager(pid_t current_pid) {
diff --git a/kernel/emusgx/encls_cross_vm.c b/kernel/emusgx/encls_cross_vm.c
index b77be51bd..96227f370 100644
--- a/kernel/emusgx/encls_cross_vm.c
+++ b/kernel/emusgx/encls_cross_vm.c
@@ -784,12 +784,14 @@ uint64_t emusgx_validate_and_do_remote_for_einit(struct sgx_sigstruct *sigstruct
 // return 3 if pf on vaslot
 // return 4 if pf on secs
 // return 5 if gp(0)
-uint8_t emusgx_validate_and_do_remote_for_eldb_eldu(void *srcpage, struct sgx_secs *secs_pa, uint64_t *vaslot_pa, void *epc_page, struct sgx_pcmd *pcmd, uint64_t linaddr, uint8_t block, uint8_t *free_manager) {
+uint8_t emusgx_validate_and_do_remote_for_eldb_eldu(void *srcpage, struct sgx_secs *secs_pa, uint64_t *vaslot_pa, void *epc_page, struct sgx_pcmd *pcmd, uint64_t linaddr, uint8_t block, uint8_t *va_page, uint64_t *va_mac, uint8_t *free_manager) {
 	// Check concurrency of EPC and VASLOT by other Intel SGX instructions
 	// Ignored
 	struct emusgx_mac_header tmp_header;
 	uint64_t tmp_ver;
 	void *epc_page_buffer;
+	void *va_buffer;
+	void *decrypted_va_page;
 	struct sgx_secs *secs;
 	uint64_t *vaslot;
 	uint16_t page_type = pcmd->secinfo.flags.page_type;
@@ -807,11 +809,11 @@ uint8_t emusgx_validate_and_do_remote_for_eldb_eldu(void *srcpage, struct sgx_se
 		return 2;
 	}
 
-	if (emusgx_check_within_epc(vaslot_pa)) {
+	/*if (emusgx_check_within_epc(vaslot_pa)) {
 		if (page_type == SGX_PT_SECS)
 			*free_manager = 1;
 		return 3;
-	}
+	}*/
 
 	// Verify EPCM attributes of EPC page, VA and SECS
 	if (emusgx_get_epcm(epc_page)->valid == 1) {
@@ -820,13 +822,13 @@ uint8_t emusgx_validate_and_do_remote_for_eldb_eldu(void *srcpage, struct sgx_se
 		return 2;
 	}
 
-	vaslot_epcm = emusgx_get_epcm((void *)((uint64_t)vaslot_pa & ~((uint64_t)0x0FFF)));
+	// vaslot_epcm = emusgx_get_epcm((void *)((uint64_t)vaslot_pa & ~((uint64_t)0x0FFF)));
 
-	if ((vaslot_epcm->valid == 0) || (vaslot_epcm->page_type != SGX_PT_VA)) {
+	/*if ((vaslot_epcm->valid == 0) || (vaslot_epcm->page_type != SGX_PT_VA)) {
 		if (page_type == SGX_PT_SECS)
 			*free_manager = 1;
 		return 3;
-	}
+	}*/
 
 	epc_epcm = emusgx_get_epcm(epc_page);
 
@@ -893,13 +895,36 @@ uint8_t emusgx_validate_and_do_remote_for_eldb_eldu(void *srcpage, struct sgx_se
 		tmp_header.eid = 0;
 	}
 
+	// Decrypt VA page
+	va_buffer = kmalloc(4096, GFP_KERNEL);
+	decrypted_va_page = kmalloc(4096, GFP_KERNEL);
+	memcpy(va_buffer, va_page, 4096);
+	decrypt_ret = emusgx_aes_128_gcm_dec(emusgx_internal_cr_va_key, &iv, emusgx_static_aad, 16, va_buffer, 4096, decrypted_va_page, va_mac);
+	if (decrypt_ret == -EBADMSG) {
+		pr_err("EmuSGX: VA MAC mismatch\n");
+		kfree(va_buffer);
+		kfree(decrypted_va_page);
+		return EMUSGX_GP;
+	}
+	if (decrypt_ret != 0) {
+		// Shit happened
+		// Crash GP(0)
+		pr_info("EmuSGX: Unexpected decryption issue\n");
+		kfree(va_buffer);
+		kfree(decrypted_va_page);
+		return EMUSGX_GP;
+	}
+
 	// Copy 4KBytes SRCPGE to secure location
 	// memcpy(epc_page, srcpage, 4096); Just decrypt
-	vaslot = (void *)vaslot_epcm->enclave_address;
+	vaslot = &(decrypted_va_page[(vaslot_pa & PAGE_MASK)]);
 	tmp_ver = vaslot[0];
 	// Does not make sense
 	// tmp_ver = tmp_ver << 32;
 
+	kfree(va_buffer);
+	kfree(decrypted_va_page);
+
 	// Decrypt and MAC page. AES_GCM_DEC has 2 outputs, {plain text, MAC}
 	// Parameters for AES_GCM_DEC {Key, Counter, ...}
 	// Decrypted data must be copied to epc_page later
@@ -1095,9 +1120,13 @@ uint64_t emusgx_validate_and_do_remote_for_emodt(void *epc_page, uint8_t R, uint
 
 // return 0 if good
 // return 1 if pf on EPC page
-uint8_t emusgx_validate_and_do_remote_for_epa(void *epc_page) {
+// return 2 if GP 0
+uint8_t emusgx_validate_and_do_remote_for_epa(void *epc_page, void *encrypted_va_page, uint64_t *va_mac) {
 	struct emusgx_epcm *epc_epcm;
 	void *va_page;
+	int encrypt_ret;
+	void *va_buffer;
+	uint64_t iv = 0;
 
 	// Check within EPC
 	if (emusgx_check_within_epc(epc_page)) {
@@ -1119,7 +1148,25 @@ uint8_t emusgx_validate_and_do_remote_for_epa(void *epc_page) {
 	// Clears EPC page
 	va_page = kmalloc(4096, GFP_KERNEL);
 	memset(va_page, 0, 4096);
+	va_buffer = kmalloc(4096, GFP_KERNEL);
+
+	// Encrypt VA page and send back
 
+	// Encrypt and get MAC
+	encrypt_ret = emusgx_aes_128_gcm_enc(emusgx_internal_cr_va_key, &iv, emusgx_static_aad, 16, va_page, 4096, va_buffer, va_mac);
+	if (encrypt_ret != 0) {
+		pr_info("EmuSGX: Unexpected encryption issue for EPA\n");
+		kfree(va_page);
+		kfree(va_buffer);
+		return 2;
+	}
+	memcpy(encrypted_va_page, va_buffer, 4096);
+
+	kfree(va_page);
+	kfree(va_buffer);
+	return 0;
+
+	/*
 	epc_epcm->page_type = SGX_PT_VA;
 	epc_epcm->enclave_address = va_page; // Trick
 	epc_epcm->manager_entry = NULL; // VA does not have a manager_entry
@@ -1130,8 +1177,10 @@ uint8_t emusgx_validate_and_do_remote_for_epa(void *epc_page) {
 	epc_epcm->W = 0;
 	epc_epcm->X = 0;
 	epc_epcm->valid = 1;
+	
 
 	return 0;
+	*/
 }
 
 // return rax value
@@ -1185,29 +1234,33 @@ uint64_t emusgx_validate_and_do_remote_for_eremove(void *epc_page, uint8_t *free
 }
 
 // return rax value
-uint64_t emusgx_validate_and_do_remote_for_ewb(void *epc_page, void *vaslot_pa, struct sgx_pcmd *pcmd, void *srcpage, uint64_t *linaddr, uint8_t *free_manager) {
+uint64_t emusgx_validate_and_do_remote_for_ewb(void *epc_page, void *vaslot_pa, struct sgx_pcmd *pcmd, void *srcpage, uint64_t *linaddr, uint8_t *va_page, uint64_t *va_mac, uint8_t *va_page_wb, uint64_t *va_mac_wb, uint8_t *free_manager) {
 	struct emusgx_epcm *epc_epcm;
-	struct emusgx_epcm *va_epcm;
+	// struct emusgx_epcm *va_epcm;
 	struct sgx_secs __user *tmp_secs;
 	struct emusgx_mac_header tmp_header;
 	uint64_t tmp_pcmd_enclaveid = 0;
 	uint64_t tmp_ver;
 	uint64_t *vaslot;
 	int i;
+	uint64_t iv = 0;
 	int encrypt_ret;
 	void *epc_page_buffer;
+	void *va_buffer;
+	void *decrypted_va_page;
 
 	// Check within EPC
 	if (emusgx_check_within_epc(epc_page)) {
 		return EMUSGX_PF_RCX;
 	}
 
+	/*&
 	if (emusgx_check_within_epc(vaslot_pa)) {
 		return EMUSGX_PF_RDX;
-	}
+	}*/
 
 	epc_epcm = emusgx_get_epcm(epc_page);
-	va_epcm = emusgx_get_epcm((void *)((uint64_t)vaslot_pa & ~((uint64_t)0x0FFF)));
+	// va_epcm = emusgx_get_epcm((void *)((uint64_t)vaslot_pa & ~((uint64_t)0x0FFF)));
 
 	// Check for concurrent Intel SGX instruction access to the page
 	// Ignored
@@ -1220,9 +1273,9 @@ uint64_t emusgx_validate_and_do_remote_for_ewb(void *epc_page, void *vaslot_pa,
 		return EMUSGX_PF_RCX;
 	}
 
-	if (va_epcm->valid == 0 || va_epcm->page_type != SGX_PT_VA) {
+	/*if (va_epcm->valid == 0 || va_epcm->page_type != SGX_PT_VA) {
 		return EMUSGX_PF_RDX;
-	}
+	}*/
 
 	// Perform page-type-specific exception checks
 	if (epc_epcm->page_type == SGX_PT_REG || epc_epcm->page_type == SGX_PT_TCS || epc_epcm->page_type == SGX_PT_TRIM) {
@@ -1318,15 +1371,52 @@ uint64_t emusgx_validate_and_do_remote_for_ewb(void *epc_page, void *vaslot_pa,
 	pcmd->enclaveid = tmp_pcmd_enclaveid;
 	*linaddr = (uint64_t)epc_epcm->enclave_address;
 
+	// Decrypt VA page
+	va_buffer = kmalloc(4096, GFP_KERNEL);
+	decrypted_va_page = kmalloc(4096, GFP_KERNEL);
+	memcpy(va_buffer, va_page, 4096);
+	encrypt_ret = emusgx_aes_128_gcm_dec(emusgx_internal_cr_va_key, &iv, emusgx_static_aad, 16, va_buffer, 4096, decrypted_va_page, va_mac);
+	if (encrypt_ret == -EBADMSG) {
+		pr_err("EmuSGX: VA MAC mismatch\n");
+		kfree(va_buffer);
+		kfree(decrypted_va_page);
+		return EMUSGX_GP;
+	}
+	if (encrypt_ret != 0) {
+		// Shit happened
+		// Crash GP(0)
+		pr_info("EmuSGX: Unexpected decryption issue\n");
+		kfree(va_buffer);
+		kfree(decrypted_va_page);
+		return EMUSGX_GP;
+	}
+
+
 	// Check if version array slot was empty
-	vaslot = (uint64_t *)(va_epcm->enclave_address);
+	vaslot = &(decrypted_va_page[(vaslot_pa & PAGE_MASK)]);
 	if (vaslot[0]) {
+		kfree(va_buffer);
+		kfree(decrypted_va_page);
 		return EMUSGX_VA_SLOT_OCCUPIED;
 	}
 
 	// Write version to Version Array slot
 	vaslot[0] = tmp_ver;
 
+	// Encrypt again
+	encrypt_ret = emusgx_aes_128_gcm_enc(emusgx_internal_cr_va_key, &iv, emusgx_static_aad, 16, decrypted_va_page, 4096, va_buffer, va_mac_wb);
+	if (encrypt_ret != 0) {
+		// Shit happened
+		// Crash GP(0)
+		pr_info("EmuSGX: Unexpected encryption issue\n");
+		kfree(va_buffer);
+		kfree(decrypted_va_page);
+		return EMUSGX_GP;
+	}
+	memcpy(va_buffer, va_page_wb, 4096);
+	kfree(va_buffer);
+	kfree(decrypted_va_page);
+
 	// Unmap the page
 	if (epc_epcm->page_type == SGX_PT_VA || epc_epcm->page_type == SGX_PT_SECS) {
 		// Free the page
@@ -1350,12 +1440,15 @@ uint64_t emusgx_validate_and_do_remote_for_ewb(void *epc_page, void *vaslot_pa,
 
 void emusgx_do_remote_for_encls(uint8_t instr, void *package) {
 	struct emusgx_raw_response response;
-	struct emusgx_raw_response_with_page* response_with_page;
+	struct emusgx_raw_response_with_va *response_with_va;
+	struct emusgx_raw_response_with_page_and_va *response_with_page;
 	uint8_t with_page = 0;
+	uint8_t with_va = 0;
 	uint8_t free_manager = 0;
 
 	response.instr = instr;
 	response.write_back = 0;
+	response.with_va = 0;
 
 	// A huge switch on instr
 	switch(instr) {
@@ -1388,7 +1481,7 @@ void emusgx_do_remote_for_encls(uint8_t instr, void *package) {
 		break;
 	case EMUSGX_S_ELDB:
 	case EMUSGX_S_ELDU:
-		response.response = emusgx_validate_and_do_remote_for_eldb_eldu(&(((struct emusgx_eldb_eldu_package *)package)->srcpage[0]), (void *)(((struct emusgx_eldb_eldu_package *)package)->secs), (void *)(((struct emusgx_eldb_eldu_package *)package)->vaslot), (void *)(((struct emusgx_eldb_eldu_package *)package)->epc_page), &(((struct emusgx_eldb_eldu_package *)package)->pcmd), (((struct emusgx_eldb_eldu_package *)package)->linaddr), (((struct emusgx_eldb_eldu_package *)package)->block), &free_manager);
+		response.response = emusgx_validate_and_do_remote_for_eldb_eldu(&(((struct emusgx_eldb_eldu_package *)package)->srcpage[0]), (void *)(((struct emusgx_eldb_eldu_package *)package)->secs), (void *)(((struct emusgx_eldb_eldu_package *)package)->vaslot), (void *)(((struct emusgx_eldb_eldu_package *)package)->epc_page), &(((struct emusgx_eldb_eldu_package *)package)->pcmd), (((struct emusgx_eldb_eldu_package *)package)->linaddr), (((struct emusgx_eldb_eldu_package *)package)->block), (((struct emusgx_eldb_eldu_package *)package)->va_page), (((struct emusgx_eldb_eldu_package *)package)->va_mac), &free_manager);
 		if (free_manager) {
 			// Failed to load a SECS. The manager can be
 			// freed
@@ -1399,15 +1492,27 @@ void emusgx_do_remote_for_encls(uint8_t instr, void *package) {
 		response.response = emusgx_do_remote_for_eblock((void *)(((struct emusgx_eblock_package *)package)->epc_page));
 		break;
 	case EMUSGX_S_EPA:
-		response.response = emusgx_validate_and_do_remote_for_epa((void *)(((struct emusgx_epa_package *)package)->epc_page));
+		response_with_va = kmalloc(sizeof(struct emusgx_raw_response_with_va), GFP_KERNEL);
+		response_with_va.response = emusgx_validate_and_do_remote_for_epa((void *)(((struct emusgx_epa_package *)package)->epc_page), (void *)(&(response_with_va->va_page[0])), (uint64_t *)(&(response_with_va->va_mac[0])));
+		if (response_with_va->response != 0) {
+			// Use minimum response to save transfer data
+			response.response = response_with_va->response;
+			kfree(response_with_va);
+		}
+		else {
+			response_with_va->instr = instr;
+			response_with_va->write_back = 0;
+			response_with_va->with_va = 1;
+			with_va = 1;
+		}
 		break;
 	case EMUSGX_S_EWB:
-		response_with_page = kmalloc(sizeof(struct emusgx_raw_response_with_page), GFP_KERNEL);
+		response_with_page = kmalloc(sizeof(struct emusgx_raw_response_with_page_and_va), GFP_KERNEL);
 		if (response_with_page == NULL) {
 			pr_info("EmuSGX: Failed to allocate a response with page write back\n");
 			response.response = EMUSGX_GP;
 		}
-		response_with_page->response = emusgx_validate_and_do_remote_for_ewb((void *)(((struct emusgx_ewb_package *)package)->epc_page), (void *)(((struct emusgx_ewb_package *)package)->vaslot), &(response_with_page->pcmd), response_with_page->page, &(response_with_page->linaddr), &free_manager);
+		response_with_page->response = emusgx_validate_and_do_remote_for_ewb((void *)(((struct emusgx_ewb_package *)package)->epc_page), (void *)(((struct emusgx_ewb_package *)package)->vaslot), &(response_with_page->pcmd), response_with_page->page, &(response_with_page->linaddr), (void *)(((struct emusgx_ewb_package *)package)->va_page), (void *)(((struct emusgx_ewb_package *)package)->va_mac), response_with_page->va_page, response_with_page->va_mac, &free_manager);
 		if (response_with_page->response != 0) {
 			// Use minimum response to save transfer data
 			response.response = response_with_page->response;
@@ -1416,6 +1521,7 @@ void emusgx_do_remote_for_encls(uint8_t instr, void *package) {
 		else {
 			response_with_page->instr = instr;
 			response_with_page->write_back = 1;
+			response_with_page->with_va = 1;
 			with_page = 1;
 		}
 		if (free_manager) {
@@ -1441,11 +1547,17 @@ void emusgx_do_remote_for_encls(uint8_t instr, void *package) {
 
 
 	if (with_page) {
-		if (emusgx_send_data(response_with_page, sizeof(struct emusgx_raw_response_with_page))) {
+		if (emusgx_send_data(response_with_page, sizeof(struct emusgx_raw_response_with_page_and_va))) {
 			pr_info("EmuSGX: Failed to response with page\n");
 		}
 		kfree(response_with_page);
 	}
+	else if (with_va) {
+		if (emusgx_send_data(response_with_va, sizeof(struct emusgx_raw_response_with_va))) {
+			pr_info("EmuSGX: Failed to response with VA\n");
+		}
+		kfree(response_with_va);
+	}
 	else {
 		if (emusgx_send_data(&response, sizeof(struct emusgx_raw_response))) {
 			pr_info("EmuSGX: Failed to response\n");
-- 
2.25.1


From b06196b04e649a26fe375fd36fa70baff3778701 Mon Sep 17 00:00:00 2001
From: NSKernel <zhao.3289@osu.edu>
Date: Tue, 13 Apr 2021 21:54:39 -0400
Subject: [PATCH 4/9] Bug fixes

---
 kernel/emusgx/dispatcher.c       |  4 +++-
 kernel/emusgx/encls_cross_vm.c   | 15 ++++++++-------
 kernel/emusgx/local_dispatcher.c |  3 ++-
 kernel/emusgx/sender.c           |  8 ++++----
 4 files changed, 17 insertions(+), 13 deletions(-)

diff --git a/kernel/emusgx/dispatcher.c b/kernel/emusgx/dispatcher.c
index a002a56be..d79e6d778 100644
--- a/kernel/emusgx/dispatcher.c
+++ b/kernel/emusgx/dispatcher.c
@@ -272,6 +272,7 @@ uint8_t emusgx_wait_for_aex_request(uint64_t tcs_pa, struct pt_regs *regs) {
 	eresume_response.instr = EMUSGX_S_ERESUME;
 	eresume_response.response = resume_ret;
 	eresume_response.write_back = 0;
+	eresume_response.with_va = 0;
 	if (resume_ret == 1) {
 		// PF on TCS
 		eresume_response.linaddr = (uint64_t)package->tcs;
@@ -812,6 +813,7 @@ int emusgx_dispatcher(void *dummy) {
 						pr_info("vSGX: dispatcher: No more free manager\n");
 						response.instr = instr;
 						response.write_back = 0;
+						response.with_va = 0;
 
 						if (instr == EMUSGX_S_ECREATE) {
 							response.response = 2; // GP for ECREATE
@@ -872,4 +874,4 @@ int emusgx_dispatcher(void *dummy) {
 	
 	}
 	return -1;
-}
\ No newline at end of file
+}
diff --git a/kernel/emusgx/encls_cross_vm.c b/kernel/emusgx/encls_cross_vm.c
index 96227f370..2d396c882 100644
--- a/kernel/emusgx/encls_cross_vm.c
+++ b/kernel/emusgx/encls_cross_vm.c
@@ -795,9 +795,10 @@ uint8_t emusgx_validate_and_do_remote_for_eldb_eldu(void *srcpage, struct sgx_se
 	struct sgx_secs *secs;
 	uint64_t *vaslot;
 	uint16_t page_type = pcmd->secinfo.flags.page_type;
+	uint64_t iv = 0;
 
 	struct emusgx_epcm *epc_epcm;
-	struct emusgx_epcm *vaslot_epcm;
+	// struct emusgx_epcm *vaslot_epcm;
 	int decrypt_ret;
 
 	*free_manager = 0;
@@ -904,7 +905,7 @@ uint8_t emusgx_validate_and_do_remote_for_eldb_eldu(void *srcpage, struct sgx_se
 		pr_err("EmuSGX: VA MAC mismatch\n");
 		kfree(va_buffer);
 		kfree(decrypted_va_page);
-		return EMUSGX_GP;
+		return 5;
 	}
 	if (decrypt_ret != 0) {
 		// Shit happened
@@ -912,12 +913,12 @@ uint8_t emusgx_validate_and_do_remote_for_eldb_eldu(void *srcpage, struct sgx_se
 		pr_info("EmuSGX: Unexpected decryption issue\n");
 		kfree(va_buffer);
 		kfree(decrypted_va_page);
-		return EMUSGX_GP;
+		return 5;
 	}
 
 	// Copy 4KBytes SRCPGE to secure location
 	// memcpy(epc_page, srcpage, 4096); Just decrypt
-	vaslot = &(decrypted_va_page[(vaslot_pa & PAGE_MASK)]);
+	vaslot = decrypted_va_page + (((uint64_t)vaslot_pa) & ((uint64_t)PAGE_MASK));
 	tmp_ver = vaslot[0];
 	// Does not make sense
 	// tmp_ver = tmp_ver << 32;
@@ -1393,7 +1394,7 @@ uint64_t emusgx_validate_and_do_remote_for_ewb(void *epc_page, void *vaslot_pa,
 
 
 	// Check if version array slot was empty
-	vaslot = &(decrypted_va_page[(vaslot_pa & PAGE_MASK)]);
+	vaslot = decrypted_va_page + (((uint64_t)vaslot_pa) & ((uint64_t)PAGE_MASK));
 	if (vaslot[0]) {
 		kfree(va_buffer);
 		kfree(decrypted_va_page);
@@ -1493,7 +1494,7 @@ void emusgx_do_remote_for_encls(uint8_t instr, void *package) {
 		break;
 	case EMUSGX_S_EPA:
 		response_with_va = kmalloc(sizeof(struct emusgx_raw_response_with_va), GFP_KERNEL);
-		response_with_va.response = emusgx_validate_and_do_remote_for_epa((void *)(((struct emusgx_epa_package *)package)->epc_page), (void *)(&(response_with_va->va_page[0])), (uint64_t *)(&(response_with_va->va_mac[0])));
+		response_with_va->response = emusgx_validate_and_do_remote_for_epa((void *)(((struct emusgx_epa_package *)package)->epc_page), (void *)(&(response_with_va->va_page[0])), (uint64_t *)(&(response_with_va->va_mac[0])));
 		if (response_with_va->response != 0) {
 			// Use minimum response to save transfer data
 			response.response = response_with_va->response;
@@ -1563,4 +1564,4 @@ void emusgx_do_remote_for_encls(uint8_t instr, void *package) {
 			pr_info("EmuSGX: Failed to response\n");
 		}
 	}
-}
\ No newline at end of file
+}
diff --git a/kernel/emusgx/local_dispatcher.c b/kernel/emusgx/local_dispatcher.c
index fe0a7b8a2..302f9769f 100644
--- a/kernel/emusgx/local_dispatcher.c
+++ b/kernel/emusgx/local_dispatcher.c
@@ -90,6 +90,7 @@ void emusgx_local_dispatcher(int manager_nr) {
 			eenter_response.response = response;
 			eenter_response.linaddr = (uint64_t)pf_addr;
 			eenter_response.write_back = 0;
+			eenter_response.with_va = 0;
 			// Send package
 			if (emusgx_send_data(&eenter_response, sizeof(struct emusgx_raw_response))) {
 				pr_info("vSGX: Failed to response eenter\n");
@@ -115,4 +116,4 @@ void emusgx_local_dispatcher(int manager_nr) {
 		}
 		kfree(package);
 	}
-}
\ No newline at end of file
+}
diff --git a/kernel/emusgx/sender.c b/kernel/emusgx/sender.c
index c0bb73bf6..3b8ced2a1 100644
--- a/kernel/emusgx/sender.c
+++ b/kernel/emusgx/sender.c
@@ -158,8 +158,8 @@ void emusgx_init_shared_page(void) {
 
 	struct emusgx_register_enclave_vm_package package;
 
-	package->instr = EMUSGX_S_REGISTER_EVM;
-	package->enclave_vm_id = emusgx_enclave_vm_id;
+	package.instr = EMUSGX_S_REGISTER_EVM;
+	package.enclave_vm_id = emusgx_enclave_vm_id;
 
 	encrypted_package = kmalloc(sizeof(struct emusgx_cross_vm_package), GFP_KERNEL);
 	plain_package = kmalloc(sizeof(struct emusgx_cross_vm_package), GFP_KERNEL);
@@ -190,7 +190,7 @@ void emusgx_init_shared_page(void) {
 		pr_info("EmuSGX: Unexpected encryption issue\n");
 		kfree(encrypted_package);
 		kfree(plain_package);
-		return -EINVAL;
+		return;
 	}
 
 	emusgx_receive_page = emusgx_get_and_share_page();
@@ -199,7 +199,7 @@ void emusgx_init_shared_page(void) {
 
 	physical_page_addr = virt_to_phys(emusgx_receive_page);
 
-	clflush_cache_range(shared_page, 4096);
+	clflush_cache_range(emusgx_receive_page, 4096);
 
 	asm volatile (
 		"cpuid"
-- 
2.25.1


From 31cdfd5b0c059ad1ffe716488f0b0ca41547758d Mon Sep 17 00:00:00 2001
From: NSKernel <zhao.3289@osu.edu>
Date: Wed, 28 Apr 2021 07:45:43 -0400
Subject: [PATCH 5/9] Removed local dispatching

---
 arch/x86/kernel/traps.c          |  48 ++++-----
 include/emusgx/emusgx_internal.h |  24 ++---
 include/emusgx/emusgx_sender.h   |   3 -
 kernel/emusgx/Makefile           |   2 +-
 kernel/emusgx/aex.c              |   1 -
 kernel/emusgx/cr.c               | 174 ++++---------------------------
 kernel/emusgx/dispatcher.c       | 138 +++++++++++++-----------
 kernel/emusgx/emusgx_fetch.c     |   2 -
 kernel/emusgx/encls_cross_vm.c   |  25 ++---
 kernel/emusgx/enclu.c            |  24 ++---
 kernel/emusgx/entrance.c         |   6 +-
 kernel/emusgx/management.c       |  57 +++-------
 kernel/emusgx/switchless_sync.c  | 103 +++++++++---------
 13 files changed, 215 insertions(+), 392 deletions(-)

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 56eae6db0..312e584ad 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -39,6 +39,7 @@
 #include <linux/io.h>
 #include <linux/hardirq.h>
 #include <linux/atomic.h>
+#include <linux/random.h>
 
 #include <asm/stacktrace.h>
 #include <asm/processor.h>
@@ -254,8 +255,6 @@ static noinstr bool handle_bug(struct pt_regs *regs)
 #include <emusgx/emusgx_cpuid.h>
 #include <emusgx/emusgx_debug.h>
 
-struct task_struct *emusgx_dispatcher_task = NULL;
-
 static void vsgx_enclave_debug_print(char *msg) {
 	pr_info("vSGX Enclave: %s\n", msg);
 }
@@ -340,15 +339,10 @@ DEFINE_IDTENTRY_RAW(exc_invalid_op)
 				if (atomic_xchg(&emusgx_cr_inited, 1) == 0) {
 					// Not inited, do the initialization
 
-					// Get dispatcher online
-					emusgx_dispatcher_task = kthread_run(emusgx_dispatcher, (void *)0, "emusgx_dispatcher_task");
-					if (IS_ERR(emusgx_dispatcher_task)) {
-						pr_err("EmuSGX: Failed to create enclave sender task\n");
-						regs->ax = -1;
-					}
-
 					// Initialize the receive page and IRQ
+					// First generate the enclave VM ID
 					// It will also send the encalve VM ID to the guest VM
+					get_random_bytes(&emusgx_enclave_vm_id, sizeof(emusgx_enclave_vm_id));
 					emusgx_init_shared_page();
 					err = request_irq(EMUSGX_IRQ, emusgx_irq_handler, 0, "emusgx_irq_response", NULL);
 					if (err < 0) {
@@ -362,29 +356,36 @@ DEFINE_IDTENTRY_RAW(exc_invalid_op)
 			else if (regs->ax == EMUSGX_MGROPS_SET_EPC) {
 				// RBX: Start
 				// RCX: Size
-				if (regs->bx % 4096 != 0) {
-					regs->ax = -1;
+				// Only init once. Since the enclave VM is disposable,
+				// no second initialization is allowed
+				if (atomic_xchg(&emusgx_cr_epcm_inited, 1) == 0) {
+					if (regs->bx % 4096 != 0) {
+						regs->ax = -1;
+						return;
+					}
+					if (regs->cx % 4096 != 0 || regs->bx + regs->cx <= regs->bx) {
+						regs->ax = -1;
+						return;
+					}
+					emusgx_epc_start = regs->bx;
+					emusgx_epc_end = regs->bx + regs->cx;
+					emusgx_init_epcm(regs->cx);
+					regs->ip += 3;
+					regs->ax = 0;
 					return;
 				}
-				if (regs->cx % 4096 != 0 || regs->bx + regs->cx <= regs->bx) {
+				else {
+					// VM fucked. Failed
 					regs->ax = -1;
 					return;
 				}
-				emusgx_epc_start = regs->bx;
-				emusgx_epc_end = regs->bx + regs->cx;
-				emusgx_init_epcm(regs->cx);
-				regs->ip += 3;
-				regs->ax = 0;
-				return;
 			}
-
 			else if (regs->ax == EMUSGX_MGROPS_GET_EPC) {
 				regs->bx = emusgx_epc_start;
 				regs->cx = emusgx_epc_end;
 				regs->ip += 3;
 				return;
 			}
-
 			else if (regs->ax == EMUSGX_MGROPS_WAIT_FOR_ACTION) {
 				emusgx_debug_print("EmuSGX: action_semaphore@0x%016llX\n", (uint64_t)&(((struct emusgx_user_space_manager_entry *)(current->manager_entry))->action_semaphore));
 				if (down_interruptible(&(((struct emusgx_user_space_manager_entry *)(current->manager_entry))->action_semaphore))) {
@@ -525,11 +526,10 @@ DEFINE_IDTENTRY_RAW(exc_invalid_op)
 		}
 		if (opcode_secondary == 0xED) { // esgxes: EmuSGX Dispatcher
 			// DOES NOT EXPECT TO RETURN
+			
+			emusgx_dispatcher();
 
-			emusgx_local_dispatcher(current->manager_entry->manager_nr);
-
-			// Only possible to be here when SIGKILL is issued
-			regs->ax = -1; 
+			regs->ax = -1;
 			regs->ip += 3;
 			return;
 		}
diff --git a/include/emusgx/emusgx_internal.h b/include/emusgx/emusgx_internal.h
index 216c9353e..ea252c640 100644
--- a/include/emusgx/emusgx_internal.h
+++ b/include/emusgx/emusgx_internal.h
@@ -34,6 +34,7 @@ struct emusgx_tmp_key_dependencies {
 #define EMUSGX_MGRSTAT_EMPTY		0
 #define EMUSGX_MGRSTAT_FREE		1
 #define EMUSGX_MGRSTAT_OCCUPIED		2
+#define EMUSGX_MGRSTAT_DEAD		3
 
 #define EMUSGX_MGRACT_NEW_THREAD	1
 
@@ -57,7 +58,6 @@ struct emusgx_local_dispatch_queue_node {
 
 struct emusgx_user_space_manager_entry {
 	uint8_t status;
-	uint64_t manager_nr;
 	pid_t pid;
 	struct emusgx_handle_buffer *handle_buffer;
 	struct semaphore action_semaphore;
@@ -83,6 +83,7 @@ struct emusgx_user_space_manager_entry {
 extern uint64_t emusgx_epc_start;
 extern uint64_t emusgx_epc_end;
 extern atomic_t emusgx_cr_inited;
+extern atomic_t emusgx_cr_epcm_inited;
 uint8_t emusgx_check_within_epc(void *addr);
 
 // Returns a 128-bit secret key
@@ -101,7 +102,7 @@ void emusgx_sha256final(uint32_t *original, uint64_t counter);
 uint32_t *emusgx_sha256(uint64_t *data, size_t len);
 
 // EID
-uint64_t vsgx_get_new_eid(void);
+uint64_t vsgx_get_eid(void);
 
 // ELRANGE
 int _enclave_local vsgx_check_in_elrange(void __user *addr);
@@ -116,7 +117,7 @@ int emusgx_compare_cpusvn(const uint64_t *cpusvn1, const uint64_t *cpusvn2);
 int vsgx_confirm_mapping(void __user *vaddr, void *paddr);
 void _enclave_local vsgx_map_epc_to_vaddr(void __user *vaddr, uint64_t epc_addr);
 void _enclave_local vsgx_unmap_epc_from_vaddr(void __user *vaddr);
-uint64_t vsgx_vaddr_to_paddr(void __user *vaddr, int enclave_nr);
+uint64_t vsgx_vaddr_to_paddr(void __user *vaddr);
 void _enclave_local vsgx_check_and_unmap_page(void __user *linaddr);
 long _enclave_local vsgx_map_empty_page_to_user_space(void __user *linaddr);
 
@@ -134,16 +135,16 @@ int emusgx_aes_128_gcm_enc(uint8_t *key, uint64_t *counter, void *aad, size_t aa
 				void *plain_text, size_t plain_size, void *cipher_text, uint64_t *mac);
 
 extern uint8_t *emusgx_internal_cr_cross_vm_key;
+extern uint64_t emusgx_enclave_vm_id;
 
 extern uint8_t *emusgx_internal_cr_va_key;
 
 struct emusgx_user_space_manager_entry *emusgx_register_manager(pid_t current_pid);
-struct emusgx_user_space_manager_entry *emusgx_get_free_manager(void);
-struct emusgx_user_space_manager_entry *emusgx_get_manager(int manager_nr);
+struct emusgx_user_space_manager_entry *emusgx_occupy_free_manager(void);
+struct emusgx_user_space_manager_entry *emusgx_get_manager(void);
 void emusgx_free_current_manager(void);
+void emusgx_mark_manager_dead(void);
 int vsgx_need_new_manager(void *package);
-int emusgx_find_enclave_with_package(void *package);
-void emusgx_queue_local_dispatch_request(void *package, uint64_t manager_nr);
 
 struct emusgx_page_fault_queue_node _enclave_local *emusgx_register_pf_request(void *addr, uint8_t *is_main);
 
@@ -151,8 +152,8 @@ int emusgx_send_data(void *addr, uint64_t size);
 
 void _enclave_local emusgx_switchless_write_page(struct emusgx_page_package *package);
 int _enclave_local emusgx_switchless_new_slot(void *addr, void *page_data);
-int emusgx_switchless_get_slot(void *addr, uint64_t manager_nr);
-int emusgx_switchless_get_and_hold_slot(void *addr, uint64_t manager_nr);
+int emusgx_switchless_get_slot(void *addr);
+int emusgx_switchless_get_and_hold_slot(void *addr);
 void _enclave_local emusgx_sync_all_pages(void);
 void _enclave_local emusgx_switchless_sync_worker(void);
 
@@ -163,8 +164,7 @@ void emusgx_do_remote_for_encls(uint8_t instr, void *package);
 
 irqreturn_t emusgx_irq_handler(int irq, void *dev_id);
 
-int emusgx_dispatcher(void *dummy);
-void _enclave_local emusgx_local_dispatcher(int manager_nr);
+int _enclave_local emusgx_dispatcher(void);
 
 /*int emusgx_start_sender(void);
 int emusgx_stop_sender(void);*/
@@ -172,4 +172,4 @@ int emusgx_check_sender(void);
 
 void _enclave_local vsgx_aex_on_current_thread(struct pt_regs *ptrace_regs, uint8_t exception_code, uint32_t error_code, uint64_t cr2);
 
-#endif
\ No newline at end of file
+#endif
diff --git a/include/emusgx/emusgx_sender.h b/include/emusgx/emusgx_sender.h
index 301ee7ee3..8c41935fc 100644
--- a/include/emusgx/emusgx_sender.h
+++ b/include/emusgx/emusgx_sender.h
@@ -136,7 +136,6 @@ struct emusgx_page_fault_queue_node {
 	struct semaphore *node_semaphore;
 	void *addr;
 	atomic_t waiting_nr;
-	uint64_t manager_nr;
 	struct emusgx_page_fault_queue_node *next;
 	struct semaphore other_waiting_semaphore;
 };
@@ -155,8 +154,6 @@ extern struct emusgx_request_queue_node *emusgx_request_queue_tail;
 
 extern char *emusgx_static_aad;
 
-extern uint64_t emusgx_enclave_vm_id;
-
 struct emusgx_raw_response {
 	uint8_t instr;
 	uint64_t response;
diff --git a/kernel/emusgx/Makefile b/kernel/emusgx/Makefile
index 56985e542..d498f7740 100644
--- a/kernel/emusgx/Makefile
+++ b/kernel/emusgx/Makefile
@@ -1,3 +1,3 @@
 obj-y += enclu.o fault.o cpusvn.o crypto.o encls_cross_vm.o \
 	irq.o management.o cr.o dispatcher.o emusgx_fetch.o encls_cross_vm.o \
-	entrance.o sender.o switchless_sync.o local_dispatcher.o aex.o
\ No newline at end of file
+	entrance.o sender.o switchless_sync.o aex.o
\ No newline at end of file
diff --git a/kernel/emusgx/aex.c b/kernel/emusgx/aex.c
index 01ea67063..5a8cd3f10 100644
--- a/kernel/emusgx/aex.c
+++ b/kernel/emusgx/aex.c
@@ -146,7 +146,6 @@ void _enclave_local vsgx_aex_on_current_thread(struct pt_regs *ptrace_regs, uint
 	package->fault_addr = cr2 & 0xFFFFFFFFFFFFF000;
 
 	package->pid = current->emusgx_pid;
-	//package->tcs_pa = vsgx_vaddr_to_paddr(current->tcs, current->manager_entry->manager_nr);
 	package->exception_code = exception_code;
 	package->error_code = error_code;
 
diff --git a/kernel/emusgx/cr.c b/kernel/emusgx/cr.c
index 0eb3eeb77..f85810411 100644
--- a/kernel/emusgx/cr.c
+++ b/kernel/emusgx/cr.c
@@ -26,6 +26,7 @@ uint64_t emusgx_epc_start = 0;
 uint64_t emusgx_epc_end = 0;
 
 atomic_t emusgx_cr_inited = (atomic_t)ATOMIC_INIT(0);
+atomic_t emusgx_cr_epcm_inited = (atomic_t)ATOMIC_INIT(0);
 
 // TODO: The 16 bytes key. In production environment
 // shall be set before deploy
@@ -35,45 +36,30 @@ uint8_t *emusgx_internal_cr_cross_vm_key = "EmuSGX Cross VM";
 // environment shall be set before deploy
 uint8_t *emusgx_internal_cr_va_key = "EmuSGX Crypt VA";
 
-struct emusgx_user_space_manager_entry emusgx_user_space_manager[EMUSGX_MAXIMUM_ENCLAVES] = { [0 ... EMUSGX_MAXIMUM_ENCLAVES - 1] = { .status = EMUSGX_MGRSTAT_EMPTY, .local_dispatch_queue = NULL, .local_dispatch_queue_tail = NULL, .local_dispatch_queue_lock = __SPIN_LOCK_UNLOCKED(emusgx_local_dispatch_queue_lock) } };
+struct emusgx_user_space_manager_entry emusgx_user_space_manager = { .status = EMUSGX_MGRSTAT_EMPTY };
 
 struct emusgx_user_space_manager_entry *emusgx_register_manager(pid_t current_pid) {
-	int i;
 	spin_lock(&emusgx_user_manager_lock);
-	for (i = 0; i < EMUSGX_MAXIMUM_ENCLAVES; i++) {
-		if (emusgx_user_space_manager[i].status == EMUSGX_MGRSTAT_EMPTY) {
-			emusgx_user_space_manager[i].status = EMUSGX_MGRSTAT_FREE;
-			break;
-		}
+	if (emusgx_user_space_manager.status == EMUSGX_MGRSTAT_EMPTY) {
+		emusgx_user_space_manager.status = EMUSGX_MGRSTAT_FREE;
 	}
 	spin_unlock(&emusgx_user_manager_lock);
 
-	if (i == EMUSGX_MAXIMUM_ENCLAVES) {
-		return NULL;
-	}
-
-	emusgx_user_space_manager[i].manager_nr = i;
 	emusgx_debug_print("vSGX: Manager number is %d\n", i);
-	emusgx_user_space_manager[i].pid = current_pid;
+	emusgx_user_space_manager.pid = current_pid;
 
-	emusgx_user_space_manager[i].action_semaphore = (struct semaphore)__SEMAPHORE_INITIALIZER(emusgx_user_space_manager[i].action_semaphore, 0);
-	emusgx_user_space_manager[i].action_avail_semaphore = (struct semaphore)__SEMAPHORE_INITIALIZER(emusgx_user_space_manager[i].action_avail_semaphore, 1);
-	emusgx_user_space_manager[i].local_dispatcher_semaphore = (struct semaphore)__SEMAPHORE_INITIALIZER(emusgx_user_space_manager[i].local_dispatcher_semaphore, 0);
-	return &(emusgx_user_space_manager[i]);
+	emusgx_user_space_manager.action_semaphore = (struct semaphore)__SEMAPHORE_INITIALIZER(emusgx_user_space_manager.action_semaphore, 0);
+	emusgx_user_space_manager.action_avail_semaphore = (struct semaphore)__SEMAPHORE_INITIALIZER(emusgx_user_space_manager.action_avail_semaphore, 1);
+	return &emusgx_user_space_manager;
 }
 
-struct emusgx_user_space_manager_entry *emusgx_get_free_manager() {
-	int i;
-	struct emusgx_user_space_manager_entry *ret_val = NULL;
+struct emusgx_user_space_manager_entry *emusgx_occupy_free_manager() {
 	spin_lock(&emusgx_user_manager_lock);
-	for (i = 0; i < EMUSGX_MAXIMUM_ENCLAVES; i++) {
-		if (emusgx_user_space_manager[i].status == EMUSGX_MGRSTAT_FREE) {
-			emusgx_user_space_manager[i].status = EMUSGX_MGRSTAT_OCCUPIED;
-			ret_val = &(emusgx_user_space_manager[i]);
-			spin_unlock(&emusgx_user_manager_lock);
-			pr_info("vSGX: Manager %d is successfully registered\n", i);
-			return ret_val;
-		}
+	if (emusgx_user_space_manager.status == EMUSGX_MGRSTAT_FREE) {
+		emusgx_user_space_manager.status = EMUSGX_MGRSTAT_OCCUPIED;
+		spin_unlock(&emusgx_user_manager_lock);
+		pr_info("vSGX: Manager is successfully registered\n");
+		return &emusgx_user_space_manager;
 	}
 	spin_unlock(&emusgx_user_manager_lock);
 
@@ -82,32 +68,14 @@ struct emusgx_user_space_manager_entry *emusgx_get_free_manager() {
 
 void emusgx_free_current_manager() {
 	spin_lock(&emusgx_user_manager_lock);
-	current->manager_entry->status = EMUSGX_MGRSTAT_FREE;
+	emusgx_user_space_manager.status = EMUSGX_MGRSTAT_FREE;
 	spin_unlock(&emusgx_user_manager_lock);
 }
 
-static int emusgx_find_enclave_with_page(void *epc_page) {
-	int manager_nr;
-
-	if (emusgx_get_epcm(epc_page) == NULL) {
-		// Not in range. Just let manager 0 handle it
-		return 0;
-	}
-	if (emusgx_get_epcm(epc_page)->valid == 0) {
-		// Not assigned to any manager. Just let manager 0 handle it
-		return 0;
-	}
-
-	if (emusgx_get_epcm(epc_page)->manager_entry == NULL) {
-		// Doing jobs on a verison array page
-		// Let manager 0 handle it
-		return 0;
-	}
-
-	manager_nr = (int)(emusgx_get_epcm(epc_page)->manager_entry->manager_nr);
-
-	return manager_nr;
-	
+void emusgx_mark_manager_dead() {
+	spin_lock(&emusgx_user_manager_lock);
+	emusgx_user_space_manager.status = EMUSGX_MGRSTAT_DEAD;
+	spin_unlock(&emusgx_user_manager_lock);
 }
 
 int vsgx_need_new_manager(void *package) {
@@ -127,106 +95,6 @@ int vsgx_need_new_manager(void *package) {
 	return 0;
 }
 
-int emusgx_find_enclave_with_package(void *package) {
-	// Find 
-	uint8_t instr = *((uint8_t *)package);
-	struct emusgx_page_package *switchless_package;
-	int manager_nr;
-
-	switch (instr) {
-	case EMUSGX_S_SWITCHLESS:
-		// Check who this page was assigned
-		switchless_package = package;
-		manager_nr = (int)switchless_package->id;
-		if (manager_nr < 0 || manager_nr >= EMUSGX_MAXIMUM_ENCLAVES) {
-			pr_info("vSGX: Switchless received a page whose manager_nr is invalid\n");
-			return -1;
-		}
-		if (emusgx_user_space_manager[manager_nr].status == EMUSGX_MGRSTAT_EMPTY) {
-			pr_info("vSGX: Switchless received a page whose manager does not exist yet\n");
-			return -1;
-		}
-
-		return manager_nr;
-	case EMUSGX_S_EENTER:
-		// Depends on TCS's SECS
-		// We have to use physical address
-		// We should also check if the TCS's vaddr actually maps to paddr
-		if (vsgx_confirm_mapping((void __user *)(((struct emusgx_eenter_package *)package)->tcs), (void *)(((struct emusgx_eenter_package *)package)->tcs_pa))) {
-			// Mapping inconsistent
-			// Reject
-			pr_info("vSGX: EENTER failed to find enclave due to inconsistent mapping of TCS to TCS's PA\n");
-			// Find a random enclave and let it complain it
-			return 0;
-		}
-		return emusgx_find_enclave_with_page((void *)(((struct emusgx_eenter_package *)package)->tcs_pa));
-	case EMUSGX_S_EADD:
-		return emusgx_find_enclave_with_page((void *)(((struct emusgx_eadd_package *)package)->secs));
-	case EMUSGX_S_EINIT:
-		return emusgx_find_enclave_with_page((void *)(((struct emusgx_einit_package *)package)->secs));
-	case EMUSGX_S_EREMOVE:
-		return emusgx_find_enclave_with_page((void *)(((struct emusgx_eremove_package *)package)->epc_page));
-	case EMUSGX_S_EEXTEND:
-		return emusgx_find_enclave_with_page((void *)(((struct emusgx_eextend_package *)package)->addr));
-	case EMUSGX_S_ELDB:
-	case EMUSGX_S_ELDU:
-		return emusgx_find_enclave_with_page((void *)(((struct emusgx_eldb_eldu_package *)package)->secs));
-	case EMUSGX_S_EBLOCK:
-		return emusgx_find_enclave_with_page((void *)(((struct emusgx_eblock_package *)package)->epc_page));
-	case EMUSGX_S_EPA:
-		// EPA adds an EPC page to the system rather than a specific process
-		// Just let a random manager 0  handle it
-		return 0;
-	case EMUSGX_S_EWB:
-		return emusgx_find_enclave_with_page((void *)(((struct emusgx_ewb_package *)package)->epc_page));
-	case EMUSGX_S_EAUG:
-		return emusgx_find_enclave_with_page((void *)(((struct emusgx_eaug_package *)package)->secs));
-	case EMUSGX_S_EMODPR:
-		return emusgx_find_enclave_with_page((void *)(((struct emusgx_emodpr_package *)package)->epc_page));
-	case EMUSGX_S_EMODT:
-		return emusgx_find_enclave_with_page((void *)(((struct emusgx_emodt_package *)package)->epc_page));
-	default:
-		// False package
-		pr_info("vSGX: Uknown package type %d\n", instr);
-		return -1;
-	}
-	
-}
-
-struct emusgx_user_space_manager_entry *emusgx_get_manager(int manager_nr) {
-	return &(emusgx_user_space_manager[manager_nr]);
+struct emusgx_user_space_manager_entry *emusgx_get_manager() {
+	return &emusgx_user_space_manager;
 }
-
-void emusgx_queue_local_dispatch_request(void *package, uint64_t manager_nr) {
-	// This will queue the package to the corresponding manager's
-	// local dispatcher
-
-	struct emusgx_user_space_manager_entry *manager_entry = &(emusgx_user_space_manager[manager_nr]);
-	struct emusgx_local_dispatch_queue_node *new_node = kmalloc(sizeof(struct emusgx_local_dispatch_queue_node), GFP_KERNEL);
-
-	if (new_node == NULL) {
-		// Unexpected
-		pr_info("vSGX: Failed to create new dispatch request node\n");
-		// Free the package since no later processing will be done
-		kfree(package);
-		return;
-	}
-
-	new_node->package = package;
-	new_node->next = NULL;
-
-	spin_lock(&(manager_entry->local_dispatch_queue_lock));
-
-	// Be quick
-	if (manager_entry->local_dispatch_queue == NULL) {
-		manager_entry->local_dispatch_queue = new_node;
-	}
-	else {
-		manager_entry->local_dispatch_queue_tail->next = new_node;
-	}
-	manager_entry->local_dispatch_queue_tail = new_node;
-
-	spin_unlock(&(manager_entry->local_dispatch_queue_lock));
-
-	up(&(manager_entry->local_dispatcher_semaphore));
-}
\ No newline at end of file
diff --git a/kernel/emusgx/dispatcher.c b/kernel/emusgx/dispatcher.c
index d79e6d778..95a7e62d4 100644
--- a/kernel/emusgx/dispatcher.c
+++ b/kernel/emusgx/dispatcher.c
@@ -24,7 +24,7 @@ struct emusgx_aex_queue_node *emusgx_aex_queue = NULL;
 struct emusgx_aex_queue_node *emusgx_aex_queue_tail = NULL;
 DEFINE_SPINLOCK(emusgx_aex_queue_lock);
 
-struct emusgx_dispatch_slot emusgx_switchless_sync_slot[EMUSGX_MAXIMUM_ENCLAVES][EMUSGX_SWITCHLESS_SLOT_COUNT] = { [0 ... EMUSGX_MAXIMUM_ENCLAVES - 1] = { [0 ... EMUSGX_SWITCHLESS_SLOT_COUNT - 1] = {.status = EMUSGX_DISPATCH_SLOT_FREE} } };
+struct emusgx_dispatch_slot emusgx_switchless_sync_slot[EMUSGX_SWITCHLESS_SLOT_COUNT] = { [0 ... EMUSGX_SWITCHLESS_SLOT_COUNT - 1] = {.status = EMUSGX_DISPATCH_SLOT_FREE} };
 
 struct emusgx_dispatch_slot emusgx_instruction_emulation_slot = {.status = EMUSGX_DISPATCH_SLOT_FREE};
 
@@ -79,14 +79,13 @@ void emusgx_remove_pf_node(struct emusgx_page_fault_queue_node *node) {
 
 struct emusgx_page_fault_queue_node _enclave_local *emusgx_register_pf_request(void *addr, uint8_t *is_main) {
 	struct emusgx_page_fault_queue_node *current_node;
-	uint64_t manager_nr = current->manager_entry->manager_nr;
 
 	// Sleepable
 	spin_lock(&emusgx_pf_queue_lock);
 
 	current_node = emusgx_pf_queue;
 	while (current_node != NULL) {
-		if (current_node->addr == addr && current_node->manager_nr == manager_nr) {
+		if (current_node->addr == addr) {
 			break;
 		}
 		current_node = current_node->next;
@@ -104,7 +103,6 @@ struct emusgx_page_fault_queue_node _enclave_local *emusgx_register_pf_request(v
 		*(current_node->node_semaphore) = (struct semaphore)__SEMAPHORE_INITIALIZER(*(current_node->node_semaphore), 0);
 		current_node->addr = addr;
 		current_node->waiting_nr = (atomic_t)ATOMIC_INIT(0);
-		current_node->manager_nr = manager_nr;
 		current_node->slot->data = kmalloc(sizeof(struct emusgx_page_package), GFP_KERNEL);
 		current_node->other_waiting_semaphore = (struct semaphore)__SEMAPHORE_INITIALIZER(current_node->other_waiting_semaphore, 0);
 		current_node->next = NULL;
@@ -339,7 +337,7 @@ struct emusgx_aex_queue_node *emusgx_find_aex_node_with_session_number(uint64_t
 struct emusgx_dispatch_slot *emusgx_get_slot_with_session_number(uint64_t session_number) {
 	struct emusgx_page_fault_queue_node *pf_node;
 	struct emusgx_aex_queue_node *aex_node;
-	int i, j;
+	int i;
 
 	// First, the encls slot
 	if (emusgx_instruction_emulation_slot.status == EMUSGX_DISPATCH_SLOT_INUSE) {
@@ -365,13 +363,11 @@ struct emusgx_dispatch_slot *emusgx_get_slot_with_session_number(uint64_t sessio
 	}
 
 	// Finally we check switchless sync list
-	for (j = 0; j < EMUSGX_MAXIMUM_ENCLAVES; j++) {
-		for (i = 0; i < EMUSGX_SWITCHLESS_SLOT_COUNT; i++) {
-			if (emusgx_switchless_sync_slot[j][i].session_number == session_number) {
-				// Must be INUSE state
-				if (emusgx_switchless_sync_slot[j][i].status == EMUSGX_DISPATCH_SLOT_INUSE)
-					return &(emusgx_switchless_sync_slot[j][i]);
-			}
+	for (i = 0; i < EMUSGX_SWITCHLESS_SLOT_COUNT; i++) {
+		if (emusgx_switchless_sync_slot[i].session_number == session_number) {
+			// Must be INUSE state
+			if (emusgx_switchless_sync_slot[i].status == EMUSGX_DISPATCH_SLOT_INUSE)
+				return &(emusgx_switchless_sync_slot[i]);
 		}
 	}
 
@@ -420,7 +416,7 @@ uint8_t emusgx_register_session_number(uint64_t session_number) {
 	return 0;
 }
 
-int emusgx_dispatcher(void *dummy) {
+int _enclave_local emusgx_dispatcher(void) {
 	int decrypt_ret;
 	int slot_index;
 	uint8_t instr;
@@ -429,9 +425,11 @@ int emusgx_dispatcher(void *dummy) {
 	struct emusgx_aex_queue_node *aex_node;
 	struct emusgx_dispatch_slot *slot;
 	struct emusgx_user_space_manager_entry *manager_entry;
+	struct emusgx_eenter_package *eenter_package;
+	struct emusgx_raw_response response_package;
+	uint64_t response;
+	void *pf_addr;
 	unsigned long flags;
-	uint64_t manager_nr;
-	struct emusgx_raw_response response;
 
 
 	uint64_t iv = 0;
@@ -558,7 +556,7 @@ int emusgx_dispatcher(void *dummy) {
 			if (instr == EMUSGX_S_SWITCHLESS) {
 				// Switchless 
 				// Find the corresponding slot
-				slot_index = emusgx_switchless_get_slot((void *)((struct emusgx_page_package *)(plain_package->payload))->addr, ((struct emusgx_page_package *)(plain_package->payload))->id);
+				slot_index = emusgx_switchless_get_slot((void *)((struct emusgx_page_package *)(plain_package->payload))->addr);
 				if (slot_index == -1) {
 					// This page is swapped out of the sync list
 					// Nothing is done. It's cool
@@ -567,29 +565,27 @@ int emusgx_dispatcher(void *dummy) {
 					continue;
 				}
 
-				manager_nr = ((struct emusgx_page_package *)(plain_package->payload))->id;
-
 				// Put the page into the slot
 				// Will there be a chance that a finished transmiting page gets lost?
 				// No because as the last page comes in, the dispatcher will dispatch that page 
 				// and set the slot free in this thread so there won't be a collision
 				// Thus no check is needed! But we do need to take care of the data field
-				if (emusgx_switchless_sync_slot[manager_nr][slot_index].status == EMUSGX_DISPATCH_SLOT_INUSE) {
+				if (emusgx_switchless_sync_slot[slot_index].status == EMUSGX_DISPATCH_SLOT_INUSE) {
 					// Should not happen. If happend, drop the current slot
-					pr_info("vSGX: Unexpected overwriting an inuse switchless slot @ mgr %lld, slot %d\n", manager_nr, slot_index);
+					pr_info("vSGX: Unexpected overwriting an inuse switchless slot @ slot %d\n", slot_index);
 					
 					// Yet still, we overwrite the last change
 					// Because the data size should be sizeof(struct emusgx_page_package)
 					// Every time the data size should be the same
-					if (emusgx_switchless_sync_slot[manager_nr][slot_index].total_size != plain_package->total_size) {
+					if (emusgx_switchless_sync_slot[slot_index].total_size != plain_package->total_size) {
 						// Should not happen
 						pr_info("EmuSGX: Data size mismatch in sync slot\n");
 						// Reallocate the correct size
-						kfree(emusgx_switchless_sync_slot[manager_nr][slot_index].data);
-						emusgx_switchless_sync_slot[manager_nr][slot_index].data = kmalloc(plain_package->total_size, GFP_KERNEL);
-						if (emusgx_switchless_sync_slot[manager_nr][slot_index].data == NULL) {
+						kfree(emusgx_switchless_sync_slot[slot_index].data);
+						emusgx_switchless_sync_slot[slot_index].data = kmalloc(plain_package->total_size, GFP_KERNEL);
+						if (emusgx_switchless_sync_slot[slot_index].data == NULL) {
 							pr_info("EmuSGX: Failed to allocate data for sync slot\n");
-							emusgx_switchless_sync_slot[manager_nr][slot_index].status = EMUSGX_DISPATCH_SLOT_FREE;
+							emusgx_switchless_sync_slot[slot_index].status = EMUSGX_DISPATCH_SLOT_FREE;
 							// Drop the package
 							kfree(plain_package);
 							continue;
@@ -599,25 +595,25 @@ int emusgx_dispatcher(void *dummy) {
 				}
 				else {
 					// If a free slot, just allocate the data field
-					emusgx_switchless_sync_slot[manager_nr][slot_index].data = kmalloc(plain_package->total_size, GFP_KERNEL);
-					if (emusgx_switchless_sync_slot[manager_nr][slot_index].data == NULL) {
+					emusgx_switchless_sync_slot[slot_index].data = kmalloc(plain_package->total_size, GFP_KERNEL);
+					if (emusgx_switchless_sync_slot[slot_index].data == NULL) {
 						pr_info("EmuSGX: Failed to allocate data for sync slot\n");
-						emusgx_switchless_sync_slot[manager_nr][slot_index].status = EMUSGX_DISPATCH_SLOT_FREE;
+						emusgx_switchless_sync_slot[slot_index].status = EMUSGX_DISPATCH_SLOT_FREE;
 						// Drop the package
 						kfree(plain_package);
 						continue;
 					}
 				}
 
-				emusgx_switchless_sync_slot[manager_nr][slot_index].status = EMUSGX_DISPATCH_SLOT_INUSE;
+				emusgx_switchless_sync_slot[slot_index].status = EMUSGX_DISPATCH_SLOT_INUSE;
 
-				emusgx_switchless_sync_slot[manager_nr][slot_index].session_number = plain_package->session_number;
-				emusgx_switchless_sync_slot[manager_nr][slot_index].total_pages = plain_package->total_pages;
-				emusgx_switchless_sync_slot[manager_nr][slot_index].current_order = plain_package->order; // Here must be zero
-				emusgx_switchless_sync_slot[manager_nr][slot_index].total_size = plain_package->total_size;
+				emusgx_switchless_sync_slot[slot_index].session_number = plain_package->session_number;
+				emusgx_switchless_sync_slot[slot_index].total_pages = plain_package->total_pages;
+				emusgx_switchless_sync_slot[slot_index].current_order = plain_package->order; // Here must be zero
+				emusgx_switchless_sync_slot[slot_index].total_size = plain_package->total_size;
 
 				// Data is allocated
-				slot = &(emusgx_switchless_sync_slot[manager_nr][slot_index]);
+				slot = &(emusgx_switchless_sync_slot[slot_index]);
 			}
 			else if (instr == EMUSGX_S_PAGEREQ) {
 				// First we find the page request
@@ -770,9 +766,15 @@ int emusgx_dispatcher(void *dummy) {
 			stamp_i+= 1;
 			// Check the instr and dispatch
 			instr = *((uint8_t *)(slot->data));
+			if (instr == EMUSGX_S_SWITCHLESS) {
+				emusgx_switchless_write_page((struct emusgx_page_package *)(slot->data));
+				
+				kfree(slot->data);
+				slot->status = EMUSGX_DISPATCH_SLOT_FREE;
+			}
 			// PAGEREQ can be handled directly here because it will wake up the waiting
 			// user thread
-			if (instr == EMUSGX_S_PAGEREQ) {
+			else if (instr == EMUSGX_S_PAGEREQ) {
 				// We just wake up the semaphore and that's done
 				// But wait! Multiple threads could be waiting on this page
 				pf_node = emusgx_find_pf_node((void *)(((struct emusgx_page_package *)(slot->data))->id));
@@ -804,25 +806,48 @@ int emusgx_dispatcher(void *dummy) {
 				// The node will be freed by the woken-up thread
 				// in emusgx_wait_for_eexit_request
 			}
-			else {
+			// encls and enclu
+			// Have to be differentiated
+			else if (instr == EMUSGX_S_EENTER) {
+				// EENTER
+				// Executes the entrance function
+				emusgx_debug_print("vSGX: Dispatch to EENTER\n");
+				eenter_package = (void *)(slot->data);
+				response = emusgx_enter_enclave((void *)(eenter_package->tcs), (void *)(eenter_package->aep), (eenter_package->pid), &(eenter_package->regs), &pf_addr);
+
+				// We need to send a response
+				response_package.instr = EMUSGX_S_EENTER;
+				response_package.response = response;
+				response_package.linaddr = (uint64_t)pf_addr;
+				response_package.write_back = 0;
+				response_package.with_va = 0;
+				// Send package
+				if (emusgx_send_data(&response_package, sizeof(struct emusgx_raw_response))) {
+					pr_info("vSGX: Failed to response eenter\n");
+				}
+				kfree(slot->data);
+				slot->status = EMUSGX_DISPATCH_SLOT_FREE;
+			}
+			else { // encls
 				if (vsgx_need_new_manager(slot->data)) {
 					// Find a new manager
-					manager_entry = emusgx_get_free_manager();
+					manager_entry = emusgx_occupy_free_manager();
 					if (manager_entry == NULL) {
 						// We send a GP back
 						pr_info("vSGX: dispatcher: No more free manager\n");
-						response.instr = instr;
-						response.write_back = 0;
-						response.with_va = 0;
+						pr_info("vSGX: Why's this package sent to me? I'm already inuse!");
+						response_package.instr = instr;
+						response_package.write_back = 0;
+						response_package.with_va = 0;
 
 						if (instr == EMUSGX_S_ECREATE) {
-							response.response = 2; // GP for ECREATE
+							response_package.response = 2; // GP for ECREATE
 						}
 						else {
 							// ELDB/ELDU
-							response.response = 5; // GP for ELDB/ELDU
+							response_package.response = 5; // GP for ELDB/ELDU
 						}
-						if (emusgx_send_data(&response, sizeof(struct emusgx_raw_response))) {
+						if (emusgx_send_data(&response_package, sizeof(struct emusgx_raw_response))) {
 							pr_info("vSGX: Failed to response\n");
 						}
 						// Free the slot and continue
@@ -830,35 +855,22 @@ int emusgx_dispatcher(void *dummy) {
 						slot->status = EMUSGX_DISPATCH_SLOT_FREE;
 						continue;
 					}
-					manager_nr = manager_entry->manager_nr;
-					// Note the local dispatcher does not need to have the manager
-					// entry because it can get the entry with current->manager_entry
-					// The current->manager_entry is set by the manager during its
-					// registration
 
-					emusgx_debug_print("vSGX: New manager at %lld\n", manager_nr);
+					emusgx_debug_print("vSGX: New manager occupied\n");
 
 					// We also do not have to free the manager if ECREATE fails here 
 					// because the ENCLS handler will do this for us
 				}
 				else {
-					manager_nr = emusgx_find_enclave_with_package(slot->data);
-				}
-				if (manager_nr == -1) {
-					pr_info("vSGX: Failed to retrive package's manager. This should not happen.\n");
-					// Only SWITCHLESS or a false package can go here.
-					// Note that a hack is done by dispatching the non-contained
-					// package for ENCLS and ENCLU to the first manager.
-					// This is because that Intel has specified faults for this situation
-					// thus we can randomly pick a manager to report such fault.
-					slot->status = EMUSGX_DISPATCH_SLOT_FREE;
-					continue;
+					manager_entry = emusgx_get_manager();
 				}
 				
-				// Dispatch to loacl dispatchers
-				// Freeing the data is the responsibility of the local dispatcher
-				emusgx_queue_local_dispatch_request(slot->data, manager_nr);
+				// Dispatch to EENTER and 
+				emusgx_debug_print("vSGX: Dispatcher doing ENCLS\n");
+				emusgx_do_remote_for_encls(instr, (void *)(slot->data));
+				emusgx_debug_print("vSGX: Dispatcher done ENCLS\n");
 
+				kfree(slot->data);
 				slot->status = EMUSGX_DISPATCH_SLOT_FREE;
 			}
 		}
diff --git a/kernel/emusgx/emusgx_fetch.c b/kernel/emusgx/emusgx_fetch.c
index de952514a..edbcd4c16 100644
--- a/kernel/emusgx/emusgx_fetch.c
+++ b/kernel/emusgx/emusgx_fetch.c
@@ -11,7 +11,6 @@ void *emusgx_receive_page = NULL;
 struct emusgx_page_request_package {
 	uint8_t instr;
 	uint64_t addr;
-	uint64_t manager_nr;
 	uint64_t semaphore_addr;
 } __attribute__((__packed__));
 
@@ -26,7 +25,6 @@ uint8_t emusgx_get_guest_page(uint64_t addr, void **page_data_addr, struct semap
 
 	request.instr = EMUSGX_S_PAGEREQ;
 	request.addr = addr;
-	request.manager_nr = current->manager_entry->manager_nr;
 
 	// Register a page request
 	pf_request_node = emusgx_register_pf_request((void *)addr, &is_main);
diff --git a/kernel/emusgx/encls_cross_vm.c b/kernel/emusgx/encls_cross_vm.c
index 2d396c882..8a962d458 100644
--- a/kernel/emusgx/encls_cross_vm.c
+++ b/kernel/emusgx/encls_cross_vm.c
@@ -145,7 +145,6 @@ uint8_t _enclave_local emusgx_validate_and_do_remote_for_eadd(struct sgx_secs *s
 
 	// Associate the EPCPAGE with the SECS by storing the SECS identifier of TMP_SECS
 	epc_epcm->enclave_secs = (uint64_t)secs_pa; // Here the encalve_secs refers to physical address
-	epc_epcm->manager_entry = current->manager_entry;
 
 	// Set EPCM entry fields
 	epc_epcm->blocked = 0;
@@ -232,7 +231,6 @@ uint8_t emusgx_validate_and_do_remote_for_eaug(struct sgx_secs *secs_pa, void __
 
 	// Associate the EPCPAGE with the SECS by storing the SECS identifier of SECS
 	epc_epcm->enclave_secs = (uint64_t)secs_pa;
-	epc_epcm->manager_entry = current->manager_entry;
 
 	// Set EPCM valid fields
 	epc_epcm->valid = 1;
@@ -448,13 +446,12 @@ uint8_t emusgx_validate_and_do_remote_for_ecreate(void *srcpage, void *epc_page)
 	emusgx_debug_print("EmuSGX: hash after extend is 0x%016llX\n", *((uint64_t *)(tmp_secs->mrenclave)));
 
 	// Set manager
-	tmp_manager_entry = current->manager_entry; // current->manager_entry is set by when the manager initialized itself
-						    // The manager is already occupied during the global dispatch procedure
-	tmp_manager_entry->secs_pa = epc_page;      // Use physical address of SECS
+	tmp_manager_entry = emusgx_get_manager();
+	tmp_manager_entry->secs_pa = epc_page;     // Use physical address of SECS
 	tmp_secs->manager_entry = (uint64_t)tmp_manager_entry;
 
 	// Set EID
-	tmp_secs->eid = vsgx_get_new_eid(); // Get a new EID
+	tmp_secs->eid = vsgx_get_eid(); // Get a new EID
 
 	// Set the EPCM entry, first create SECS identifier and store the identifier in EPCM
 	epc_epcm->page_type = SGX_PT_SECS;
@@ -465,7 +462,6 @@ uint8_t emusgx_validate_and_do_remote_for_ecreate(void *srcpage, void *epc_page)
 
 	// Added SECS to achieve dispatching
 	epc_epcm->enclave_secs = (uint64_t)epc_page;
-	epc_epcm->manager_entry = current->manager_entry;
 
 	// Set EPCM entry fields
 	epc_epcm->blocked = 0;
@@ -1003,18 +999,11 @@ uint8_t emusgx_validate_and_do_remote_for_eldb_eldu(void *srcpage, struct sgx_se
 	}
 
 	if (tmp_header.secinfo.flags.page_type == SGX_PT_SECS) {
-		current->manager_entry->secs_pa = epc_page; // Use physical address of SECS
-		((struct sgx_secs *)epc_page_buffer)->manager_entry = (uint64_t)current->manager_entry;
+		emusgx_get_manager()->secs_pa = epc_page; // Use physical address of SECS
+		((struct sgx_secs *)epc_page_buffer)->manager_entry = (uint64_t)emusgx_get_manager();
 
 		// Don't change EID
 	}
-	if (tmp_header.secinfo.flags.page_type == SGX_PT_VA) {
-		// VA does not have a manager_entry
-		epc_epcm->manager_entry = NULL;
-	}
-	else {
-		epc_epcm->manager_entry = current->manager_entry;
-	}
 
 	epc_epcm->valid = 1;
 
@@ -1474,7 +1463,7 @@ void emusgx_do_remote_for_encls(uint8_t instr, void *package) {
 		if (free_manager) {
 			// SECS freed. The enclave is terminated, can be
 			// freed
-			emusgx_free_current_manager();
+			emusgx_mark_manager_dead();
 		}
 		break;
 	case EMUSGX_S_EEXTEND:
@@ -1528,7 +1517,7 @@ void emusgx_do_remote_for_encls(uint8_t instr, void *package) {
 		if (free_manager) {
 			// SECS freed. The enclave is terminated, can be
 			// freed
-			emusgx_free_current_manager();
+			emusgx_mark_manager_dead();
 		}
 		break;
 	case EMUSGX_S_EAUG:
diff --git a/kernel/emusgx/enclu.c b/kernel/emusgx/enclu.c
index 5823a07fb..273493ec9 100644
--- a/kernel/emusgx/enclu.c
+++ b/kernel/emusgx/enclu.c
@@ -15,12 +15,12 @@ void emusgx_ereport(struct emusgx_regs *reg_status, struct pt_regs *ptrace_regs)
 	// RCX address of report data 
 	// RDX output data
 	struct sgx_targetinfo __user *targetinfo = (struct sgx_targetinfo *)(reg_status->rbx);
-	void *targetinfo_pa = (void *)vsgx_vaddr_to_paddr(targetinfo, current->manager_entry->manager_nr);
+	void *targetinfo_pa = (void *)vsgx_vaddr_to_paddr(targetinfo);
 	uint8_t __user *reportdata = (uint8_t *)(reg_status->rcx);
-	void *reportdata_pa = (void *)vsgx_vaddr_to_paddr(reportdata, current->manager_entry->manager_nr);
+	void *reportdata_pa = (void *)vsgx_vaddr_to_paddr(reportdata);
 	struct sgx_report *tmp_report;
 	struct sgx_report *outputdata = (struct sgx_report*)(reg_status->rdx);
-	void *outputdata_pa = (void *)vsgx_vaddr_to_paddr(outputdata, current->manager_entry->manager_nr);
+	void *outputdata_pa = (void *)vsgx_vaddr_to_paddr(outputdata);
 	uint64_t *tmp_reportkey;
 	uint64_t *tmp_mac;
 	struct emusgx_tmp_key_dependencies *tmp_keydependencies;
@@ -220,9 +220,9 @@ void emusgx_ereport(struct emusgx_regs *reg_status, struct pt_regs *ptrace_regs)
 
 void emusgx_egetkey(struct emusgx_regs *reg_status, struct pt_regs *ptrace_regs) {
 	struct sgx_keyrequest *keyrequest = (struct sgx_keyrequest *)(reg_status->rbx);
-	void *keyrequest_pa = (void *)vsgx_vaddr_to_paddr(keyrequest, current->manager_entry->manager_nr);
+	void *keyrequest_pa = (void *)vsgx_vaddr_to_paddr(keyrequest);
 	void *outputdata = (void *)(reg_status->rcx);
-	void *outputdata_pa = (void *)vsgx_vaddr_to_paddr(outputdata, current->manager_entry->manager_nr);
+	void *outputdata_pa = (void *)vsgx_vaddr_to_paddr(outputdata);
 	uint64_t *derived_key;
 	uint64_t tmp_attributes[2];
 	uint32_t tmp_micselect;
@@ -589,10 +589,10 @@ void emusgx_eresume(struct emusgx_regs *reg_status, struct pt_regs *ptrace_regs)
 
 void emusgx_eaccept(struct emusgx_regs *reg_status, struct pt_regs *ptrace_regs) {
 	struct sgx_secinfo *scratch_secinfo = (struct sgx_secinfo *)(reg_status->rbx);
-	void *secinfo_pa = (void *)vsgx_vaddr_to_paddr(scratch_secinfo, current->manager_entry->manager_nr);
+	void *secinfo_pa = (void *)vsgx_vaddr_to_paddr(scratch_secinfo);
 	struct sgx_tcs *tmp_tcs;
 	void *epc_page_va = (void *)(reg_status->rcx);
-	void *epc_page_pa = (void *)vsgx_vaddr_to_paddr(epc_page_va, current->manager_entry->manager_nr);
+	void *epc_page_pa = (void *)vsgx_vaddr_to_paddr(epc_page_va);
 	int i;
 
 	struct emusgx_epcm *secinfo_epcm;
@@ -735,9 +735,9 @@ void emusgx_eaccept(struct emusgx_regs *reg_status, struct pt_regs *ptrace_regs)
 
 void emusgx_emodpe(struct emusgx_regs *reg_status, struct pt_regs *ptrace_regs) {
 	struct sgx_secinfo *scratch_secinfo = (struct sgx_secinfo *)(reg_status->rbx);
-	void *secinfo_pa = (void *)vsgx_vaddr_to_paddr(scratch_secinfo, current->manager_entry->manager_nr);
+	void *secinfo_pa = (void *)vsgx_vaddr_to_paddr(scratch_secinfo);
 	void *epc_page_va = (void *)(reg_status->rcx);
-	void *epc_page_pa = (void *)vsgx_vaddr_to_paddr(epc_page_va, current->manager_entry->manager_nr);
+	void *epc_page_pa = (void *)vsgx_vaddr_to_paddr(epc_page_va);
 
 	struct emusgx_epcm *secinfo_epcm;
 	struct emusgx_epcm *epc_epcm;
@@ -823,11 +823,11 @@ void emusgx_emodpe(struct emusgx_regs *reg_status, struct pt_regs *ptrace_regs)
 
 void emusgx_eacceptcopy(struct emusgx_regs *reg_status, struct pt_regs *ptrace_regs) {
 	struct sgx_secinfo *scratch_secinfo = (struct sgx_secinfo *)(reg_status->rbx);
-	void *secinfo_pa = (void *)vsgx_vaddr_to_paddr(scratch_secinfo, current->manager_entry->manager_nr);
+	void *secinfo_pa = (void *)vsgx_vaddr_to_paddr(scratch_secinfo);
 	void *epc_dest_page_va = (void *)(reg_status->rcx);
-	void *epc_dest_page_pa = (void *)vsgx_vaddr_to_paddr(epc_dest_page_va, current->manager_entry->manager_nr);
+	void *epc_dest_page_pa = (void *)vsgx_vaddr_to_paddr(epc_dest_page_va);
 	void *epc_src_page_va = (void *)(reg_status->rdx);
-	void *epc_src_page_pa = (void *)vsgx_vaddr_to_paddr(epc_src_page_va, current->manager_entry->manager_nr);
+	void *epc_src_page_pa = (void *)vsgx_vaddr_to_paddr(epc_src_page_va);
 
 	struct emusgx_epcm *secinfo_epcm;
 	struct emusgx_epcm *epc_dest_epcm;
diff --git a/kernel/emusgx/entrance.c b/kernel/emusgx/entrance.c
index 3b1b3fa96..1c1e22c65 100644
--- a/kernel/emusgx/entrance.c
+++ b/kernel/emusgx/entrance.c
@@ -13,7 +13,7 @@
 #include <emusgx/emusgx_fault.h>
 
 uint64_t _enclave_local emusgx_enter_enclave(struct sgx_tcs __user *tcs, void *aep, uint64_t pid, struct emusgx_full_regs *regs, void **pf_addr) {
-	void *tcs_pa = (void *)vsgx_vaddr_to_paddr(tcs, current->manager_entry->manager_nr);
+	void *tcs_pa = (void *)vsgx_vaddr_to_paddr(tcs);
 	struct emusgx_epcm *tcs_epcm;
 	struct emusgx_epcm *gpr_epcm;
 	struct sgx_secs *tmp_secs;
@@ -176,7 +176,7 @@ uint64_t _enclave_local emusgx_enter_enclave(struct sgx_tcs __user *tcs, void *a
 	tmp_gpr = (void __user *)((uint64_t)tmp_ssa_page + 4096 * tmp_secs->ssaframesize - sizeof(struct sgx_gprsgx));
 	__uaccess_end();
 
-	tmp_gpr_pa = (void *)vsgx_vaddr_to_paddr(tmp_gpr, current->manager_entry->manager_nr);
+	tmp_gpr_pa = (void *)vsgx_vaddr_to_paddr(tmp_gpr);
 
 	// If GPR area valid
 	if (emusgx_check_within_epc(tmp_gpr_pa)) {
@@ -432,7 +432,7 @@ uint8_t vsgx_resume_enclave(struct sgx_tcs __user *tcs, uint64_t tcs_pa, uint64_
 	tmp_gpr = (void __user *)((uint64_t)tmp_ssa_page + 4096 * tmp_secs->ssaframesize - sizeof(struct sgx_gprsgx));
 	__uaccess_end();
 
-	tmp_gpr_pa = (void *)vsgx_vaddr_to_paddr(tmp_gpr, current->manager_entry->manager_nr);
+	tmp_gpr_pa = (void *)vsgx_vaddr_to_paddr(tmp_gpr);
 
 	// If GPR area valid
 	if (emusgx_check_within_epc(tmp_gpr_pa)) {
diff --git a/kernel/emusgx/management.c b/kernel/emusgx/management.c
index 947d7ebba..e48e8ac08 100644
--- a/kernel/emusgx/management.c
+++ b/kernel/emusgx/management.c
@@ -3,6 +3,7 @@
 #include <linux/slab.h>
 #include <linux/atomic.h>
 #include <linux/sched.h>
+#include <linux/random.h>
 
 #include <linux/mm.h>
 #include <linux/mman.h>
@@ -12,10 +13,12 @@
 #include <emusgx/emusgx_cpuid.h>
 
 struct emusgx_epcm *emusgx_epcm_array = NULL;
-atomic_long_t vsgx_global_eid = (atomic_long_t)ATOMIC_INIT(0);
 
-uint64_t vsgx_get_new_eid() {
-	return atomic_long_inc_return(&vsgx_global_eid);
+uint64_t vsgx_get_eid() {
+	// Just generate randomly
+	uint64_t ret_val;
+	get_random_bytes(&ret_val, sizeof(ret_val));
+	return ret_val;
 }
 
 int _enclave_local vsgx_check_in_elrange(void __user *addr) {
@@ -121,39 +124,6 @@ uint8_t emusgx_check_within_epc(void __user*addr) {
 	return 0;
 }
 
-/*
-int emusgx_start_sender(void) {
-	int cpuid_success;
-
-	asm volatile (
-		"cpuid"
-		: "=b"(cpuid_success)
-		: "a"(KVM_CPUID_EMUSGX_RUN_SENDER_KTHREADS)
-		: "%rcx", "%rdx"
-	);
-	if (!cpuid_success) {
-		pr_info("EmuSGX: CPUID failed");
-		return -EINVAL;
-	}
-	return 0;
-}
-
-int emusgx_stop_sender(void) {
-	int cpuid_success;
-
-	asm volatile (
-		"cpuid"
-		: "=b"(cpuid_success)
-		: "a"(KVM_CPUID_EMUSGX_STOP_SENDER_KTHREADS)
-		: "%rcx", "%rdx"
-	);
-	if (!cpuid_success) {
-		pr_info("EmuSGX: CPUID failed");
-		return -EINVAL;
-	}
-	return 0;
-}*/
-
 int emusgx_check_sender(void) {
 	int cpuid_success;
 	int ret_val;
@@ -171,6 +141,7 @@ int emusgx_check_sender(void) {
 	return ret_val;
 }
 
+
 // Just like a 64-bit page table
 struct epc_pmd {
 	uint64_t pte[512];
@@ -186,11 +157,10 @@ struct epc_pgd {
 };
 
 // The mapping of each encalve
-struct epc_pgd global_pgds[EMUSGX_MAXIMUM_ENCLAVES] = { { { (void *)0 } } };
+static struct epc_pgd global_pgd = { .p4d = { [0 ... 511] = NULL } };
 
 void _enclave_local vsgx_map_epc_to_vaddr(void __user *vaddr, uint64_t epc_addr) {
-	int enclave_nr = current->manager_entry->manager_nr;
-	struct epc_pgd *pgd = &(global_pgds[enclave_nr]);
+	struct epc_pgd *pgd = &global_pgd;
 	struct epc_p4d *p4d;
 	struct epc_pud *pud;
 	struct epc_pmd *pmd;
@@ -221,14 +191,13 @@ void _enclave_local vsgx_map_epc_to_vaddr(void __user *vaddr, uint64_t epc_addr)
 	// Warning! Overwritable!
 	if (pmd->pte[pte_index] != 0) {
 		pr_warn("vSGX: Warning! Overwriting older mapping!\n");
-		pr_warn("vSGX: Rewrote mapping in manager %d: 0x%016llX->0x%016llX from 0x%016llX\n", enclave_nr, (uint64_t)vaddr, epc_addr, pmd->pte[pte_index]);
+		pr_warn("vSGX: Rewrote mapping in manager: 0x%016llX->0x%016llX from 0x%016llX\n", (uint64_t)vaddr, epc_addr, pmd->pte[pte_index]);
 	}
 	pmd->pte[pte_index] = epc_addr;
 }
 
 void _enclave_local vsgx_unmap_epc_from_vaddr(void __user *vaddr) {
-	int enclave_nr = current->manager_entry->manager_nr;
-	struct epc_pgd *pgd = &(global_pgds[enclave_nr]);
+	struct epc_pgd *pgd = &global_pgd;
 	struct epc_p4d *p4d;
 	struct epc_pud *pud;
 	struct epc_pmd *pmd;
@@ -263,8 +232,8 @@ void _enclave_local vsgx_unmap_epc_from_vaddr(void __user *vaddr) {
 	pmd->pte[pte_index] = 0;
 }
 
-uint64_t vsgx_vaddr_to_paddr(void __user *vaddr, int enclave_nr) {
-	struct epc_pgd *pgd = &(global_pgds[enclave_nr]);
+uint64_t vsgx_vaddr_to_paddr(void __user *vaddr) {
+	struct epc_pgd *pgd = &global_pgd;
 	struct epc_p4d *p4d;
 	struct epc_pud *pud;
 	struct epc_pmd *pmd;
diff --git a/kernel/emusgx/switchless_sync.c b/kernel/emusgx/switchless_sync.c
index 4ce2c1402..da09e78f4 100644
--- a/kernel/emusgx/switchless_sync.c
+++ b/kernel/emusgx/switchless_sync.c
@@ -22,27 +22,26 @@ struct emusgx_switchless_page_slot {
 	uint8_t *original_content;
 };
 
-static uint64_t emusgx_switchless_sync_index[EMUSGX_MAXIMUM_ENCLAVES] = { 0 };
+static uint64_t emusgx_switchless_sync_index = 0;
 static DEFINE_SPINLOCK(emusgx_switchless_index_lock);
 
-struct emusgx_switchless_page_slot emusgx_switchless_pages[EMUSGX_MAXIMUM_ENCLAVES][EMUSGX_SWITCHLESS_SLOT_COUNT] = { [0 ... EMUSGX_MAXIMUM_ENCLAVES - 1] = { [0 ... EMUSGX_SWITCHLESS_SLOT_COUNT - 1] = { .status = EMUSGX_SWITCHLESS_SLOT_FREE, .lock = __SPIN_LOCK_UNLOCKED(emusgx_switchless_lock), .addr = NULL, .original_content = NULL } } };
+struct emusgx_switchless_page_slot emusgx_switchless_pages[EMUSGX_SWITCHLESS_SLOT_COUNT] = { [0 ... EMUSGX_SWITCHLESS_SLOT_COUNT - 1] = { .status = EMUSGX_SWITCHLESS_SLOT_FREE, .lock = __SPIN_LOCK_UNLOCKED(emusgx_switchless_lock), .addr = NULL, .original_content = NULL } };
 
 void emusgx_switchless_write_page(struct emusgx_page_package *package) {
 	// Make sure the page still exists
 	int i, group, bit;
-	uint64_t manager_nr = package->id;
 	for (i = 0; i < EMUSGX_SWITCHLESS_SLOT_COUNT; i++) {
 		// First grab the lock. Can sleep
-		spin_lock(&emusgx_switchless_pages[manager_nr][i].lock);
+		spin_lock(&emusgx_switchless_pages[i].lock);
 
-		if (emusgx_switchless_pages[manager_nr][i].status != EMUSGX_SWITCHLESS_SLOT_INUSE) {
-			spin_unlock(&emusgx_switchless_pages[manager_nr][i].lock);
+		if (emusgx_switchless_pages[i].status != EMUSGX_SWITCHLESS_SLOT_INUSE) {
+			spin_unlock(&emusgx_switchless_pages[i].lock);
 			// Go ahead
 			continue;
 		}
 
-		if ((uint64_t)(emusgx_switchless_pages[manager_nr][i].addr) != package->addr) {
-			spin_unlock(&emusgx_switchless_pages[manager_nr][i].lock);
+		if ((uint64_t)(emusgx_switchless_pages[i].addr) != package->addr) {
+			spin_unlock(&emusgx_switchless_pages[i].lock);
 			// Go ahead
 			continue;
 		}
@@ -69,7 +68,7 @@ void emusgx_switchless_write_page(struct emusgx_page_package *package) {
 		for (bit = 0; bit < 8; bit++) {
 			if (((package->mask[group]) >> bit) & 1) {
 				((uint8_t *)(package->addr))[group * 8 + bit] = package->page[group * 8 + bit];
-				emusgx_switchless_pages[manager_nr][i].original_content[group * 8 + bit] = package->page[group * 8 + bit];
+				emusgx_switchless_pages[i].original_content[group * 8 + bit] = package->page[group * 8 + bit];
 			}
 		}
 	}
@@ -77,7 +76,7 @@ void emusgx_switchless_write_page(struct emusgx_page_package *package) {
 	__uaccess_end();
 
 	// Release the lock
-	spin_unlock(&emusgx_switchless_pages[manager_nr][i].lock);
+	spin_unlock(&emusgx_switchless_pages[i].lock);
 
 	// We do not clear the dirty bit since there's no way to tell if ant
 	// threads changed our data during the writing process
@@ -88,8 +87,7 @@ void emusgx_switchless_write_page(struct emusgx_page_package *package) {
 // Must be called with the slot locked!
 // Only the owner manager can call this!
 int _enclave_local emusgx_sync_on_dirty(int slot) {
-	uint64_t manager_nr = current->manager_entry->manager_nr;
-	uint64_t addr = (uint64_t)emusgx_switchless_pages[manager_nr][slot].addr;
+	uint64_t addr = (uint64_t)emusgx_switchless_pages[slot].addr;
 	struct emusgx_page_package *package;
 	int group, bit, need_to_send;
 
@@ -160,11 +158,11 @@ int _enclave_local emusgx_sync_on_dirty(int slot) {
 		for (group = 0; group < 512; group++) {
 			package->mask[group] = 0;
 			for (bit = 0; bit < 8; bit++) {
-				package->page[group * 8 + bit] = ((uint8_t *)(emusgx_switchless_pages[manager_nr][slot].addr))[group * 8 + bit];
-				if (emusgx_switchless_pages[manager_nr][slot].original_content[group * 8 + bit] != package->page[group * 8 + bit]) {
+				package->page[group * 8 + bit] = ((uint8_t *)(emusgx_switchless_pages[slot].addr))[group * 8 + bit];
+				if (emusgx_switchless_pages[slot].original_content[group * 8 + bit] != package->page[group * 8 + bit]) {
 					// Set bit 1
 					package->mask[group] |= ((uint64_t)1 << bit);
-					emusgx_switchless_pages[manager_nr][slot].original_content[group * 8 + bit] = package->page[group * 8 + bit];
+					emusgx_switchless_pages[slot].original_content[group * 8 + bit] = package->page[group * 8 + bit];
 					need_to_send = 1;
 				}
 			}
@@ -174,8 +172,8 @@ int _enclave_local emusgx_sync_on_dirty(int slot) {
 
 		if (need_to_send) {
 			package->instr = EMUSGX_S_SWITCHLESS;
-			package->addr = (uint64_t)emusgx_switchless_pages[manager_nr][slot].addr;
-			package->id = manager_nr;
+			package->addr = (uint64_t)emusgx_switchless_pages[slot].addr;
+			package->id = 0; // Not used
 			emusgx_debug_print("EmuSGX: Syncing page...\n");
 			if (emusgx_send_data(package, sizeof(struct emusgx_page_package))) {
 				pr_info("EmuSGX: Failed to send package for swapped out page\n");
@@ -267,102 +265,96 @@ int _enclave_local emusgx_switchless_new_slot(void *addr, void *page_data) {
 
 	int munmap_error;
 	uint64_t index;
-	uint64_t manager_nr = current->manager_entry->manager_nr;
-
-	if (manager_nr >= EMUSGX_MAXIMUM_ENCLAVES) {
-		pr_info("vSGX: Switchless new slot for invalid manager %lld\n", manager_nr);
-		return -1;
-	}
 
 	spin_lock(&emusgx_switchless_index_lock);
 
-	index = emusgx_switchless_sync_index[manager_nr];
-	emusgx_switchless_sync_index[manager_nr] += 1;
-	if (emusgx_switchless_sync_index[manager_nr] >= EMUSGX_SWITCHLESS_SLOT_COUNT) {
-		emusgx_switchless_sync_index[manager_nr] = 0;
+	index = emusgx_switchless_sync_index;
+	emusgx_switchless_sync_index += 1;
+	if (emusgx_switchless_sync_index >= EMUSGX_SWITCHLESS_SLOT_COUNT) {
+		emusgx_switchless_sync_index = 0;
 	}
 
 	spin_unlock(&emusgx_switchless_index_lock);
 
 	// First, get the slot
 	// Can-sleep
-	spin_lock(&emusgx_switchless_pages[manager_nr][index].lock);
+	spin_lock(&emusgx_switchless_pages[index].lock);
 
-	if (emusgx_switchless_pages[manager_nr][index].status != EMUSGX_SWITCHLESS_SLOT_FREE) {
+	if (emusgx_switchless_pages[index].status != EMUSGX_SWITCHLESS_SLOT_FREE) {
 		// Sync for the last time
 		emusgx_sync_on_dirty(index);
 		// Unmap the original addr from the memory
 		if (mmap_write_lock_killable(current->mm)) {
 			pr_info("EmuSGX: mmap failed to get lock\n");
-			spin_unlock(&emusgx_switchless_pages[manager_nr][index].lock);
+			spin_unlock(&emusgx_switchless_pages[index].lock);
 			return -1;
 		}
-		munmap_error = do_munmap(current->mm, (uint64_t)emusgx_switchless_pages[manager_nr][index].addr, 4096, NULL);
+		munmap_error = do_munmap(current->mm, (uint64_t)emusgx_switchless_pages[index].addr, 4096, NULL);
 		mmap_write_unlock(current->mm);
 		if (munmap_error) {
 			pr_info("EmuSGX: Unexpected non-mapped address in an inuse sync slot\n");
 		}
 	}
 	else {
-		emusgx_switchless_pages[manager_nr][index].original_content = NULL;
+		emusgx_switchless_pages[index].original_content = NULL;
 	}
 
 	// Update the slot
-	emusgx_switchless_pages[manager_nr][index].status = EMUSGX_SWITCHLESS_SLOT_INUSE;
-	emusgx_switchless_pages[manager_nr][index].addr = addr;
+	emusgx_switchless_pages[index].status = EMUSGX_SWITCHLESS_SLOT_INUSE;
+	emusgx_switchless_pages[index].addr = addr;
 	// Lazy allocate, reuse if not NULL
-	if (emusgx_switchless_pages[manager_nr][index].original_content == NULL) {
-		emusgx_switchless_pages[manager_nr][index].original_content = kmalloc(4096, GFP_KERNEL);
+	if (emusgx_switchless_pages[index].original_content == NULL) {
+		emusgx_switchless_pages[index].original_content = kmalloc(4096, GFP_KERNEL);
 	}
-	if (emusgx_switchless_pages[manager_nr][index].original_content == NULL) {
+	if (emusgx_switchless_pages[index].original_content == NULL) {
 		pr_info("EmuSGX: I cannot allocate data for the original_content\n");
 		return -1;
 	}
 	// Update the original_content field
 	// Note that the process won't be able to write to the page
 	// at this moment because this IS the process
-	memcpy(emusgx_switchless_pages[manager_nr][index].original_content, page_data, 4096);
+	memcpy(emusgx_switchless_pages[index].original_content, page_data, 4096);
 
-	spin_unlock(&emusgx_switchless_pages[manager_nr][index].lock);
+	spin_unlock(&emusgx_switchless_pages[index].lock);
 
 	// The slot is ready
 
 	return 0;
 }
 
-int emusgx_switchless_get_slot(void *addr, uint64_t manager_nr) {
+int emusgx_switchless_get_slot(void *addr) {
 	// Make sure the page still exists
 	int i;
 	for (i = 0; i < EMUSGX_SWITCHLESS_SLOT_COUNT; i++) {
-		spin_lock(&emusgx_switchless_pages[manager_nr][i].lock);
+		spin_lock(&emusgx_switchless_pages[i].lock);
 
-		if (emusgx_switchless_pages[manager_nr][i].status == EMUSGX_SWITCHLESS_SLOT_INUSE) {
-			if (emusgx_switchless_pages[manager_nr][i].addr == addr) {
-				spin_unlock(&emusgx_switchless_pages[manager_nr][i].lock);
+		if (emusgx_switchless_pages[i].status == EMUSGX_SWITCHLESS_SLOT_INUSE) {
+			if (emusgx_switchless_pages[i].addr == addr) {
+				spin_unlock(&emusgx_switchless_pages[i].lock);
 				return i;
 			}
 		}
 
-		spin_unlock(&emusgx_switchless_pages[manager_nr][i].lock);
+		spin_unlock(&emusgx_switchless_pages[i].lock);
 			
 	}
 	return -1;
 }
 
-int emusgx_switchless_get_and_hold_slot(void *addr, uint64_t manager_nr) {
+int emusgx_switchless_get_and_hold_slot(void *addr) {
 	// Make sure the page still exists
 	int i;
 	for (i = 0; i < EMUSGX_SWITCHLESS_SLOT_COUNT; i++) {
-		spin_lock(&emusgx_switchless_pages[manager_nr][i].lock);
+		spin_lock(&emusgx_switchless_pages[i].lock);
 
-		if (emusgx_switchless_pages[manager_nr][i].status == EMUSGX_SWITCHLESS_SLOT_INUSE) {
-			if (emusgx_switchless_pages[manager_nr][i].addr == addr) {
-				// spin_unlock(&emusgx_switchless_pages[manager_nr][i].lock);
+		if (emusgx_switchless_pages[i].status == EMUSGX_SWITCHLESS_SLOT_INUSE) {
+			if (emusgx_switchless_pages[i].addr == addr) {
+				// spin_unlock(&emusgx_switchless_pages[i].lock);
 				return i;
 			}
 		}
 
-		spin_unlock(&emusgx_switchless_pages[manager_nr][i].lock);
+		spin_unlock(&emusgx_switchless_pages[i].lock);
 			
 	}
 	return -1;
@@ -370,12 +362,11 @@ int emusgx_switchless_get_and_hold_slot(void *addr, uint64_t manager_nr) {
 
 void _enclave_local emusgx_sync_all_pages(void) {
 	int i;
-	uint64_t manager_nr = current->manager_entry->manager_nr;
 	for (i = 0; i < EMUSGX_SWITCHLESS_SLOT_COUNT; i++) {
-		spin_lock(&emusgx_switchless_pages[manager_nr][i].lock);
-		if (emusgx_switchless_pages[manager_nr][i].status == EMUSGX_SWITCHLESS_SLOT_INUSE)
+		spin_lock(&emusgx_switchless_pages[i].lock);
+		if (emusgx_switchless_pages[i].status == EMUSGX_SWITCHLESS_SLOT_INUSE)
 			emusgx_sync_on_dirty(i);
-		spin_unlock(&emusgx_switchless_pages[manager_nr][i].lock);
+		spin_unlock(&emusgx_switchless_pages[i].lock);
 		// We don't care successful or not since we are doing lazy syncing
 	}
 }
@@ -383,7 +374,7 @@ void _enclave_local emusgx_sync_all_pages(void) {
 void _enclave_local emusgx_switchless_sync_worker(void) {
 	// Does not expect to return
 
-	pr_info("vSGX: Switchless sync worker for manager %lld is running\n", current->manager_entry->manager_nr);
+	pr_info("vSGX: Switchless sync worker for manager is running\n");
 
 	while(true) {
 		// every 100 ms we sync all 10 slots on demand
-- 
2.25.1


From ce2e935002c03bfff3353332f320f0b5d7cb0ebf Mon Sep 17 00:00:00 2001
From: NSKernel <zhao.3289@osu.edu>
Date: Sat, 15 May 2021 11:57:50 -0400
Subject: [PATCH 6/9] Temporarily working one

---
 arch/x86/kernel/traps.c          |  28 +-
 include/emusgx/emusgx_internal.h |   6 +
 kernel/emusgx/Makefile           |   2 +-
 kernel/emusgx/cr.c               |  37 +-
 kernel/emusgx/dispatcher.c       |  82 ++--
 kernel/emusgx/emusgx_internal.h  | 181 ++++++++
 kernel/emusgx/local_dispatcher.c | 119 ------
 kernel/emusgx/mprotect.c         | 704 +++++++++++++++++++++++++++++++
 kernel/emusgx/sender.c           |  18 +-
 kernel/emusgx/switchless_sync.c  | 105 +++--
 kernel/sched/Makefile            |   2 +-
 kernel/sched/enclave_context.c   |  83 ++++
 mm/mmap.c                        |  69 +++
 mm/mprotect.c                    |   4 +
 14 files changed, 1239 insertions(+), 201 deletions(-)
 create mode 100644 kernel/emusgx/emusgx_internal.h
 delete mode 100644 kernel/emusgx/local_dispatcher.c
 create mode 100644 kernel/emusgx/mprotect.c
 create mode 100644 kernel/sched/enclave_context.c

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 312e584ad..7792dcc0a 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -281,6 +281,7 @@ DEFINE_IDTENTRY_RAW(exc_invalid_op)
 	if (opcode_prefix == 0x0F &&
 		opcode_primary == 0x01) {
 		if (opcode_secondary == 0xD7) {
+			// MUST BE IN ENCLAVE ADDRESS SPACE
 			reg_status.rax = (uint32_t)regs->ax;
 			reg_status.rbx = regs->bx;
 			reg_status.rcx = regs->cx;
@@ -291,8 +292,11 @@ DEFINE_IDTENTRY_RAW(exc_invalid_op)
 
 			// If EEXIT, restore regs to previous state
 			if ((uint32_t)(regs->ax) == EMUSGX_EEXIT) {
+				// Swap back to the manager space
+				vsgx_swap_to_manager_context();
 				// Not enclave anymore
 				current->is_enclave_thread = 0;
+
 				// Restore reg status
 				memcpy(regs, current->backup_regs_before_eenter, sizeof(struct pt_regs));
 				// So when EEXIT, it's just like nothing happened
@@ -322,6 +326,7 @@ DEFINE_IDTENTRY_RAW(exc_invalid_op)
 			return;
 		}
 		if (opcode_secondary == 0xEB) { // esgxmgr: EmuSGX User Manager Handlers
+			// MUST BE IN MANAGER ADDRESS SPACE
 			if (regs->ax == EMUSGX_MGROPS_REG_SELF) {
 				current->manager_entry = emusgx_register_manager(current->pid);
 				if (current->manager_entry == NULL) {
@@ -344,11 +349,16 @@ DEFINE_IDTENTRY_RAW(exc_invalid_op)
 					// It will also send the encalve VM ID to the guest VM
 					get_random_bytes(&emusgx_enclave_vm_id, sizeof(emusgx_enclave_vm_id));
 					emusgx_init_shared_page();
+					vsgx_switchless_init_locks();
 					err = request_irq(EMUSGX_IRQ, emusgx_irq_handler, 0, "emusgx_irq_response", NULL);
 					if (err < 0) {
 						pr_info("EmuSGX: Failed to register IRQ handler, err = %d\n", err);
 						regs->ax = -1;
 					}
+					if (vsgx_init_enclave_context()) {
+						pr_info("vSGX: Failed to initialize enclave context\n");
+						regs->ax = -1;
+					}
 				}
 				regs->ip += 3;
 				return;
@@ -437,6 +447,11 @@ DEFINE_IDTENTRY_RAW(exc_invalid_op)
 				handle_buffer = (struct emusgx_handle_buffer *)regs->bx;
 
 				emusgx_debug_print("EmuSGX: Now setting up thread\n");
+
+				// We are now enclave
+				current->is_enclave_thread = 1;
+				// Swap to the enclave context
+				vsgx_swap_to_enclave_context();
 				// Setup FS/GS base
 				// o Save first
 				// o FS/GS selectors are set to 0x0B
@@ -458,7 +473,6 @@ DEFINE_IDTENTRY_RAW(exc_invalid_op)
 				current->secs_pa = handle_buffer->secs_pa;
 				current->tcs = handle_buffer->tcs;
 				current->gpr = handle_buffer->gpr;
-				current->is_enclave_thread = 1;
 				current->emusgx_pid = handle_buffer->pid;
 				current->backup_regs_before_eenter = kmalloc(sizeof(struct pt_regs), GFP_KERNEL);
 				// Backup regs, do not have to +3 because 
@@ -517,8 +531,14 @@ DEFINE_IDTENTRY_RAW(exc_invalid_op)
 		if (opcode_secondary == 0xEC) { // esgxsl: EmuSGX Switchless Page Syncing
 			// DOES NOT EXPECT TO RETURN
 
+			vsgx_swap_to_enclave_context();
+
 			// The switchless page syncing process
 			emusgx_switchless_sync_worker();
+
+			// Swap back on unexpected return
+			// Should not happen
+			vsgx_swap_to_manager_context();
 			
 			regs->ax = -1;
 			regs->ip += 3;
@@ -527,8 +547,14 @@ DEFINE_IDTENTRY_RAW(exc_invalid_op)
 		if (opcode_secondary == 0xED) { // esgxes: EmuSGX Dispatcher
 			// DOES NOT EXPECT TO RETURN
 			
+			vsgx_swap_to_enclave_context();
+			
 			emusgx_dispatcher();
 
+			// Swap back on unexpected return
+			// Should not happen
+			vsgx_swap_to_manager_context();
+
 			regs->ax = -1;
 			regs->ip += 3;
 			return;
diff --git a/include/emusgx/emusgx_internal.h b/include/emusgx/emusgx_internal.h
index ea252c640..231cec6d1 100644
--- a/include/emusgx/emusgx_internal.h
+++ b/include/emusgx/emusgx_internal.h
@@ -150,6 +150,7 @@ struct emusgx_page_fault_queue_node _enclave_local *emusgx_register_pf_request(v
 
 int emusgx_send_data(void *addr, uint64_t size);
 
+void vsgx_switchless_init_locks(void);
 void _enclave_local emusgx_switchless_write_page(struct emusgx_page_package *package);
 int _enclave_local emusgx_switchless_new_slot(void *addr, void *page_data);
 int emusgx_switchless_get_slot(void *addr);
@@ -172,4 +173,9 @@ int emusgx_check_sender(void);
 
 void _enclave_local vsgx_aex_on_current_thread(struct pt_regs *ptrace_regs, uint8_t exception_code, uint32_t error_code, uint64_t cr2);
 
+// Enclave address space
+int vsgx_init_enclave_context(void);
+void vsgx_swap_to_enclave_context(void);
+void vsgx_swap_to_manager_context(void);
+
 #endif
diff --git a/kernel/emusgx/Makefile b/kernel/emusgx/Makefile
index d498f7740..f17030e39 100644
--- a/kernel/emusgx/Makefile
+++ b/kernel/emusgx/Makefile
@@ -1,3 +1,3 @@
 obj-y += enclu.o fault.o cpusvn.o crypto.o encls_cross_vm.o \
 	irq.o management.o cr.o dispatcher.o emusgx_fetch.o encls_cross_vm.o \
-	entrance.o sender.o switchless_sync.o aex.o
\ No newline at end of file
+	entrance.o sender.o switchless_sync.o aex.o
diff --git a/kernel/emusgx/cr.c b/kernel/emusgx/cr.c
index f85810411..eb5ac6c66 100644
--- a/kernel/emusgx/cr.c
+++ b/kernel/emusgx/cr.c
@@ -2,7 +2,7 @@
 #include <linux/types.h>
 #include <linux/slab.h>
 #include <linux/sched.h>
-#include <linux/spinlock.h>
+#include <linux/mutex.h>
 #include <linux/semaphore.h>
 
 #include <asm/ldt.h>
@@ -13,7 +13,7 @@
 #include <emusgx/emusgx_internal.h>
 #include <emusgx/emusgx_debug.h>
 
-static DEFINE_SPINLOCK(emusgx_user_manager_lock);
+static DEFINE_MUTEX(emusgx_user_manager_lock);
 
 uint64_t emusgx_csr_owner_epoch[2];
 uint64_t emusgx_cr_seal_fuses[2];
@@ -39,11 +39,15 @@ uint8_t *emusgx_internal_cr_va_key = "EmuSGX Crypt VA";
 struct emusgx_user_space_manager_entry emusgx_user_space_manager = { .status = EMUSGX_MGRSTAT_EMPTY };
 
 struct emusgx_user_space_manager_entry *emusgx_register_manager(pid_t current_pid) {
-	spin_lock(&emusgx_user_manager_lock);
+	if (mutex_lock_killable(&emusgx_user_manager_lock)) {
+		pr_info("vSGX: Register manager killed\n");
+		return NULL;
+	}
+
 	if (emusgx_user_space_manager.status == EMUSGX_MGRSTAT_EMPTY) {
 		emusgx_user_space_manager.status = EMUSGX_MGRSTAT_FREE;
 	}
-	spin_unlock(&emusgx_user_manager_lock);
+	mutex_unlock(&emusgx_user_manager_lock);
 
 	emusgx_debug_print("vSGX: Manager number is %d\n", i);
 	emusgx_user_space_manager.pid = current_pid;
@@ -54,28 +58,39 @@ struct emusgx_user_space_manager_entry *emusgx_register_manager(pid_t current_pi
 }
 
 struct emusgx_user_space_manager_entry *emusgx_occupy_free_manager() {
-	spin_lock(&emusgx_user_manager_lock);
+	if (mutex_lock_killable(&emusgx_user_manager_lock)) {
+		pr_info("vSGX: Occupy free manager killed\n");
+		return NULL;
+	}
+
 	if (emusgx_user_space_manager.status == EMUSGX_MGRSTAT_FREE) {
 		emusgx_user_space_manager.status = EMUSGX_MGRSTAT_OCCUPIED;
-		spin_unlock(&emusgx_user_manager_lock);
+		mutex_unlock(&emusgx_user_manager_lock);
 		pr_info("vSGX: Manager is successfully registered\n");
 		return &emusgx_user_space_manager;
 	}
-	spin_unlock(&emusgx_user_manager_lock);
+	mutex_unlock(&emusgx_user_manager_lock);
 
 	return NULL;
 }
 
 void emusgx_free_current_manager() {
-	spin_lock(&emusgx_user_manager_lock);
+	if (mutex_lock_killable(&emusgx_user_manager_lock)) {
+		pr_info("vSGX: Free current manager killed\n");
+		return ;
+	}
+
 	emusgx_user_space_manager.status = EMUSGX_MGRSTAT_FREE;
-	spin_unlock(&emusgx_user_manager_lock);
+	mutex_unlock(&emusgx_user_manager_lock);
 }
 
 void emusgx_mark_manager_dead() {
-	spin_lock(&emusgx_user_manager_lock);
+	if (mutex_lock_killable(&emusgx_user_manager_lock)) {
+		pr_info("vSGX: Mark manager dead killed\n");
+		return ;
+	}
 	emusgx_user_space_manager.status = EMUSGX_MGRSTAT_DEAD;
-	spin_unlock(&emusgx_user_manager_lock);
+	mutex_unlock(&emusgx_user_manager_lock);
 }
 
 int vsgx_need_new_manager(void *package) {
diff --git a/kernel/emusgx/dispatcher.c b/kernel/emusgx/dispatcher.c
index 95a7e62d4..646e461d9 100644
--- a/kernel/emusgx/dispatcher.c
+++ b/kernel/emusgx/dispatcher.c
@@ -1,6 +1,7 @@
 #include <linux/kernel.h>
 #include <linux/interrupt.h>
 #include <linux/spinlock.h>
+#include <linux/mutex.h>
 #include <linux/semaphore.h>
 #include <linux/slab.h>
 #include <linux/ktime.h>
@@ -18,11 +19,11 @@ struct semaphore emusgx_dispatcher_sem = __SEMAPHORE_INITIALIZER(emusgx_dispatch
 
 struct emusgx_page_fault_queue_node *emusgx_pf_queue = NULL;
 struct emusgx_page_fault_queue_node *emusgx_pf_queue_tail = NULL;
-DEFINE_SPINLOCK(emusgx_pf_queue_lock);
+DEFINE_MUTEX(emusgx_pf_queue_lock);
 
 struct emusgx_aex_queue_node *emusgx_aex_queue = NULL;
 struct emusgx_aex_queue_node *emusgx_aex_queue_tail = NULL;
-DEFINE_SPINLOCK(emusgx_aex_queue_lock);
+DEFINE_MUTEX(emusgx_aex_queue_lock);
 
 struct emusgx_dispatch_slot emusgx_switchless_sync_slot[EMUSGX_SWITCHLESS_SLOT_COUNT] = { [0 ... EMUSGX_SWITCHLESS_SLOT_COUNT - 1] = {.status = EMUSGX_DISPATCH_SLOT_FREE} };
 
@@ -48,7 +49,10 @@ void emusgx_remove_pf_node(struct emusgx_page_fault_queue_node *node) {
 	struct emusgx_page_fault_queue_node *prev_node = NULL;
 
 	// Sleepable
-	spin_lock(&emusgx_pf_queue_lock);
+	if (mutex_lock_killable(&emusgx_pf_queue_lock)) {
+		pr_info("vSGX: Remove PF node killed\n");
+		return;
+	}
 
 	current_node = emusgx_pf_queue;
 	while (current_node != NULL) {
@@ -67,21 +71,24 @@ void emusgx_remove_pf_node(struct emusgx_page_fault_queue_node *node) {
 					emusgx_pf_queue_tail = prev_node;
 				}
 			}
-			spin_unlock(&emusgx_pf_queue_lock);
+			mutex_unlock(&emusgx_pf_queue_lock);
 			return;
 		}
 		prev_node = current_node;
 		current_node = current_node->next;
 	}
 
-	spin_unlock(&emusgx_pf_queue_lock);
+	mutex_unlock(&emusgx_pf_queue_lock);
 }
 
 struct emusgx_page_fault_queue_node _enclave_local *emusgx_register_pf_request(void *addr, uint8_t *is_main) {
 	struct emusgx_page_fault_queue_node *current_node;
 
 	// Sleepable
-	spin_lock(&emusgx_pf_queue_lock);
+	if (mutex_lock_killable(&emusgx_pf_queue_lock)) {
+		pr_info("vSGX: Register PF request killed\n");
+		return NULL;
+	}
 
 	current_node = emusgx_pf_queue;
 	while (current_node != NULL) {
@@ -116,7 +123,7 @@ struct emusgx_page_fault_queue_node _enclave_local *emusgx_register_pf_request(v
 		*is_main = 1;
 	}
 
-	spin_unlock(&emusgx_pf_queue_lock);
+	mutex_unlock(&emusgx_pf_queue_lock);
 
 	return current_node;
 }
@@ -125,18 +132,21 @@ struct emusgx_page_fault_queue_node *emusgx_find_pf_node_with_session_number(uin
 	struct emusgx_page_fault_queue_node *current_node;
 
 	// Sleepable
-	spin_lock(&emusgx_pf_queue_lock);
+	if (mutex_lock_killable(&emusgx_pf_queue_lock)) {
+		pr_info("vSGX: Find PF node with session number killed\n");
+		return NULL;
+	}
 
 	current_node = emusgx_pf_queue;
 	while (current_node != NULL) {
 		if (current_node->slot->session_number == session_number) {
-			spin_unlock(&emusgx_pf_queue_lock);
+			mutex_unlock(&emusgx_pf_queue_lock);
 			return current_node;
 		}
 		current_node = current_node->next;
 	}
 
-	spin_unlock(&emusgx_pf_queue_lock);
+	mutex_unlock(&emusgx_pf_queue_lock);
 
 	return NULL;
 }
@@ -145,18 +155,21 @@ struct emusgx_page_fault_queue_node *emusgx_find_pf_node(struct semaphore *sem)
 	struct emusgx_page_fault_queue_node *current_node;
 
 	// Sleepable
-	spin_lock(&emusgx_pf_queue_lock);
+	if (mutex_lock_killable(&emusgx_pf_queue_lock)) {
+		pr_info("vSGX: Find PF node killed\n");
+		return NULL;
+	}
 
 	current_node = emusgx_pf_queue;
 	while (current_node != NULL) {
 		if (current_node->node_semaphore == sem) {
-			spin_unlock(&emusgx_pf_queue_lock);
+			mutex_unlock(&emusgx_pf_queue_lock);
 			return current_node;
 		}
 		current_node = current_node->next;
 	}
 
-	spin_unlock(&emusgx_pf_queue_lock);
+	mutex_unlock(&emusgx_pf_queue_lock);
 
 	return NULL;
 }
@@ -165,7 +178,10 @@ uint8_t emusgx_register_aex_request(uint64_t tcs_pa) {
 	struct emusgx_aex_queue_node *current_node;
 
 	// Sleepable
-	spin_lock(&emusgx_aex_queue_lock);
+	if (mutex_lock_killable(&emusgx_aex_queue_lock)) {
+		pr_info("vSGX: Register AEX request killed\n");
+		return -1;
+	}
 
 	// We don't have to check previously saved AEX requests since in Intel's
 	// reference there's no documentation suggesting that you cannot reenter
@@ -191,7 +207,7 @@ uint8_t emusgx_register_aex_request(uint64_t tcs_pa) {
 	}
 	
 
-	spin_unlock(&emusgx_aex_queue_lock);
+	mutex_unlock(&emusgx_aex_queue_lock);
 
 	return 0;
 }
@@ -206,7 +222,10 @@ uint8_t emusgx_wait_for_aex_request(uint64_t tcs_pa, struct pt_regs *regs) {
 	uint8_t resume_ret;
 
 	// Sleepable
-	spin_lock(&emusgx_aex_queue_lock);
+	if (mutex_lock_killable(&emusgx_aex_queue_lock)) {
+		pr_info("vSGX: Wait for AEX request killed before the request is found\n");
+		return -1;
+	}
 
 	// Search for node
 	// Search and pop from the front
@@ -221,12 +240,12 @@ uint8_t emusgx_wait_for_aex_request(uint64_t tcs_pa, struct pt_regs *regs) {
 
 	// If not found, unexpected
 	if (current_node == NULL) {
-		spin_unlock(&emusgx_aex_queue_lock);
+		mutex_unlock(&emusgx_aex_queue_lock);
 		pr_info("EmuSGX: Unexpected. You have not been registered. TCS_PA = %lld\n", tcs_pa);
 		return -1;
 	}
 
-	spin_unlock(&emusgx_aex_queue_lock);
+	mutex_unlock(&emusgx_aex_queue_lock);
 
 	// Now wait for the response
 	if (down_interruptible(&current_node->semaphore)) {
@@ -240,7 +259,10 @@ uint8_t emusgx_wait_for_aex_request(uint64_t tcs_pa, struct pt_regs *regs) {
 	// Must be ERESUME
 	// Dequeue and handle
 
-	spin_lock(&emusgx_aex_queue_lock);
+	if (mutex_lock_killable(&emusgx_aex_queue_lock)) {
+		pr_info("vSGX: Register AEX request killed after the response is received\n");
+		return -1;
+	}
 
 	// Clear up everything
 
@@ -289,7 +311,7 @@ uint8_t emusgx_wait_for_aex_request(uint64_t tcs_pa, struct pt_regs *regs) {
 	kfree(current_node->slot);
 	kfree(current_node);
 
-	spin_unlock(&emusgx_aex_queue_lock);
+	mutex_unlock(&emusgx_aex_queue_lock);
 
 	return 0;
 }
@@ -298,18 +320,21 @@ struct emusgx_aex_queue_node *emusgx_find_aex_node(uint64_t tcs_pa) {
 	struct emusgx_aex_queue_node *current_node;
 
 	// Sleepable
-	spin_lock(&emusgx_aex_queue_lock);
+	if (mutex_lock_killable(&emusgx_aex_queue_lock)) {
+		pr_info("vSGX: Find AEX node killed\n");
+		return NULL;
+	}
 
 	current_node = emusgx_aex_queue;
 	while (current_node != NULL) {
 		if (current_node->tcs_pa == tcs_pa) {
-			spin_unlock(&emusgx_aex_queue_lock);
+			mutex_unlock(&emusgx_aex_queue_lock);
 			return current_node;
 		}
 		current_node = current_node->next;
 	}
 
-	spin_unlock(&emusgx_aex_queue_lock);
+	mutex_unlock(&emusgx_aex_queue_lock);
 
 	return NULL;
 }
@@ -318,18 +343,21 @@ struct emusgx_aex_queue_node *emusgx_find_aex_node_with_session_number(uint64_t
 	struct emusgx_aex_queue_node *current_node;
 
 	// Sleepable
-	spin_lock(&emusgx_aex_queue_lock);
+	if (mutex_lock_killable(&emusgx_aex_queue_lock)) {
+		pr_info("vSGX: Find AEX node with session number killed\n");
+		return NULL;
+	}
 
 	current_node = emusgx_aex_queue;
 	while (current_node != NULL) {
 		if (current_node->slot->session_number == session_number) {
-			spin_unlock(&emusgx_aex_queue_lock);
+			mutex_unlock(&emusgx_aex_queue_lock);
 			return current_node;
 		}
 		current_node = current_node->next;
 	}
 
-	spin_unlock(&emusgx_aex_queue_lock);
+	mutex_unlock(&emusgx_aex_queue_lock);
 
 	return NULL;
 }
@@ -462,8 +490,8 @@ int _enclave_local emusgx_dispatcher(void) {
 
 		if (current_node == NULL) {
 			// Shit happens
-			pr_err("EmuSGX: Unexpected error in dispatch queue\n");
 			spin_unlock_irqrestore(&emusgx_dispatcher_queue_lock, flags);
+			pr_err("EmuSGX: Unexpected error in dispatch queue\n");
 			continue;
 		}
 
diff --git a/kernel/emusgx/emusgx_internal.h b/kernel/emusgx/emusgx_internal.h
new file mode 100644
index 000000000..231cec6d1
--- /dev/null
+++ b/kernel/emusgx/emusgx_internal.h
@@ -0,0 +1,181 @@
+#ifndef EMUSGX_INTERNAL_H
+#define EMUSGX_INTERNAL_H
+
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/interrupt.h>
+#include <linux/spinlock.h>
+#include <linux/semaphore.h>
+
+#include <emusgx/emusgx_sender.h>
+
+#define EMUSGX_SWITCHLESS_SLOT_COUNT	10
+#define EMUSGX_MAXIMUM_ENCLAVES 	10
+
+#define _enclave_local
+
+struct emusgx_tmp_key_dependencies {
+	uint16_t keyname;
+	uint16_t isvprodid;
+	uint16_t isvsvn;
+	uint64_t ownerepoch[2];
+	uint64_t attributes[2];
+	uint64_t attributesmask[2];
+	uint32_t mrenclave[8];
+	uint32_t mrsigner[8];
+	uint64_t keyid[4];
+	uint64_t seal_key_fuses[2];
+	uint64_t cpusvn[2];
+	uint8_t	padding[352];
+	uint32_t miscselect;
+	uint32_t miscmask;
+};
+
+#define EMUSGX_MGRSTAT_EMPTY		0
+#define EMUSGX_MGRSTAT_FREE		1
+#define EMUSGX_MGRSTAT_OCCUPIED		2
+#define EMUSGX_MGRSTAT_DEAD		3
+
+#define EMUSGX_MGRACT_NEW_THREAD	1
+
+struct emusgx_handle_buffer {
+	uint8_t action;
+	struct emusgx_full_regs regs;
+	uint64_t fsbase;
+	uint64_t gsbase;
+	uint64_t pid;
+	struct sgx_secs *secs;
+	uint64_t secs_pa;
+	uint64_t rip;
+	struct sgx_tcs *tcs;
+	struct sgx_gprsgx *gpr;
+} __attribute__((__packed__));
+
+struct emusgx_local_dispatch_queue_node {
+	void *package;
+	struct emusgx_local_dispatch_queue_node *next;
+};
+
+struct emusgx_user_space_manager_entry {
+	uint8_t status;
+	pid_t pid;
+	struct emusgx_handle_buffer *handle_buffer;
+	struct semaphore action_semaphore;
+	struct semaphore action_avail_semaphore;
+	struct semaphore local_dispatcher_semaphore;
+	spinlock_t local_dispatch_queue_lock;
+	struct emusgx_local_dispatch_queue_node *local_dispatch_queue;
+	struct emusgx_local_dispatch_queue_node *local_dispatch_queue_tail;
+	void *secs_pa;
+};
+
+#define EMUSGX_MGROPS_REG_SELF		0
+#define EMUSGX_MGROPS_SET_EPC		1
+#define EMUSGX_MGROPS_GET_EPC		2
+#define EMUSGX_MGROPS_SETUP_THREAD	3
+#define EMUSGX_MGROPS_WAIT_FOR_ACTION	4
+#define EMUSGX_MGROPS_START_SENDER	5
+#define EMUSGX_MGROPS_STOP_SENDER	6
+#define EMUSGX_MGROPS_CHECK_SENDER	7
+#define EMUSGX_MGROPS_INIT_SYS		8
+#define EMUSGX_MGROPS_DBGPRINT		9
+
+extern uint64_t emusgx_epc_start;
+extern uint64_t emusgx_epc_end;
+extern atomic_t emusgx_cr_inited;
+extern atomic_t emusgx_cr_epcm_inited;
+uint8_t emusgx_check_within_epc(void *addr);
+
+// Returns a 128-bit secret key
+// The user is resiponsible for freeing the key
+uint64_t *emusgx_derive_key(const struct emusgx_tmp_key_dependencies *key_dependencies);
+
+// Returns a 128-bit MAC
+// The user is resiponsible for freeing the MAC
+uint64_t *emusgx_cmac(const uint64_t *key, const void *data, const size_t size);
+
+// SHA256
+void emusgx_sha256init(uint32_t *original);
+void emusgx_sha256update(uint32_t *original, uint64_t *data);
+void emusgx_sha256final(uint32_t *original, uint64_t counter);
+// The user is resiponsible for freeing the hash 
+uint32_t *emusgx_sha256(uint64_t *data, size_t len);
+
+// EID
+uint64_t vsgx_get_eid(void);
+
+// ELRANGE
+int _enclave_local vsgx_check_in_elrange(void __user *addr);
+
+// EPCM
+uint8_t emusgx_init_epcm(uint64_t epc_size);
+struct emusgx_epcm *emusgx_get_epcm(void *epc_page);
+uint64_t *emusgx_get_cpusvn(void);
+int emusgx_compare_cpusvn(const uint64_t *cpusvn1, const uint64_t *cpusvn2);
+
+// VMA
+int vsgx_confirm_mapping(void __user *vaddr, void *paddr);
+void _enclave_local vsgx_map_epc_to_vaddr(void __user *vaddr, uint64_t epc_addr);
+void _enclave_local vsgx_unmap_epc_from_vaddr(void __user *vaddr);
+uint64_t vsgx_vaddr_to_paddr(void __user *vaddr);
+void _enclave_local vsgx_check_and_unmap_page(void __user *linaddr);
+long _enclave_local vsgx_map_empty_page_to_user_space(void __user *linaddr);
+
+uint8_t emusgx_is_secs_inited(void *secs_pa);
+void emusgx_mark_secs_inited(struct sgx_secs *secs, uint8_t inited);
+
+uint8_t emusgx_verify_sigstruct(struct sgx_sigstruct *sig);
+
+uint8_t emusgx_secs_has_associated_page(struct sgx_secs *secs_pa);
+
+int emusgx_aes_128_gcm_dec(uint8_t *key, uint64_t *counter, void *aad, size_t aad_size, 
+				void *cipher_text, size_t cipher_size, void *plain_text, uint64_t *mac);
+
+int emusgx_aes_128_gcm_enc(uint8_t *key, uint64_t *counter, void *aad, size_t aad_size, 
+				void *plain_text, size_t plain_size, void *cipher_text, uint64_t *mac);
+
+extern uint8_t *emusgx_internal_cr_cross_vm_key;
+extern uint64_t emusgx_enclave_vm_id;
+
+extern uint8_t *emusgx_internal_cr_va_key;
+
+struct emusgx_user_space_manager_entry *emusgx_register_manager(pid_t current_pid);
+struct emusgx_user_space_manager_entry *emusgx_occupy_free_manager(void);
+struct emusgx_user_space_manager_entry *emusgx_get_manager(void);
+void emusgx_free_current_manager(void);
+void emusgx_mark_manager_dead(void);
+int vsgx_need_new_manager(void *package);
+
+struct emusgx_page_fault_queue_node _enclave_local *emusgx_register_pf_request(void *addr, uint8_t *is_main);
+
+int emusgx_send_data(void *addr, uint64_t size);
+
+void vsgx_switchless_init_locks(void);
+void _enclave_local emusgx_switchless_write_page(struct emusgx_page_package *package);
+int _enclave_local emusgx_switchless_new_slot(void *addr, void *page_data);
+int emusgx_switchless_get_slot(void *addr);
+int emusgx_switchless_get_and_hold_slot(void *addr);
+void _enclave_local emusgx_sync_all_pages(void);
+void _enclave_local emusgx_switchless_sync_worker(void);
+
+uint64_t emusgx_enter_enclave(struct sgx_tcs *tcs, void *aep, uint64_t pid, struct emusgx_full_regs *regs, void **pf_addr);
+uint8_t vsgx_resume_enclave(struct sgx_tcs __user *tcs, uint64_t tcs_pa, uint64_t aep, uint64_t pid, void **pf_addr);
+
+void emusgx_do_remote_for_encls(uint8_t instr, void *package);
+
+irqreturn_t emusgx_irq_handler(int irq, void *dev_id);
+
+int _enclave_local emusgx_dispatcher(void);
+
+/*int emusgx_start_sender(void);
+int emusgx_stop_sender(void);*/
+int emusgx_check_sender(void);
+
+void _enclave_local vsgx_aex_on_current_thread(struct pt_regs *ptrace_regs, uint8_t exception_code, uint32_t error_code, uint64_t cr2);
+
+// Enclave address space
+int vsgx_init_enclave_context(void);
+void vsgx_swap_to_enclave_context(void);
+void vsgx_swap_to_manager_context(void);
+
+#endif
diff --git a/kernel/emusgx/local_dispatcher.c b/kernel/emusgx/local_dispatcher.c
deleted file mode 100644
index 302f9769f..000000000
--- a/kernel/emusgx/local_dispatcher.c
+++ /dev/null
@@ -1,119 +0,0 @@
-#include <linux/kernel.h>
-#include <linux/slab.h>
-#include <linux/spinlock.h>
-#include <linux/semaphore.h>
-#include <linux/ktime.h>
-
-#include <emusgx/emusgx.h>
-#include <emusgx/emusgx_internal.h>
-#include <emusgx/emusgx_debug.h>
-
-static uint64_t stamps_waked[20];
-extern uint64_t stamps_dispatched[20];
-static uint64_t stamps_done[20];
-static uint32_t stamp_i = 0;
-
-void emusgx_local_dispatcher(int manager_nr) {
-	// Every manager has its own local dispatcher
-
-	// Three things to be dispatched locally
-	// o Switchless write
-	// o EENTER
-	// o ENCLS
-	// Memory fetch can be signaled directly to
-	// the waiting thread
-
-	struct emusgx_user_space_manager_entry *manager_entry = emusgx_get_manager(manager_nr);
-	struct emusgx_local_dispatch_queue_node *current_node;
-	void *package;
-	struct emusgx_eenter_package *eenter_package;
-	struct emusgx_raw_response eenter_response;
-	uint64_t response;
-	void *pf_addr;
-	uint8_t instr;
-
-	pr_info("vSGX: Local dispatcher for manager %d is running\n", manager_nr);
-
-	while (true) {
-		// Wait for the semaphore
-		if (down_interruptible(&(manager_entry->local_dispatcher_semaphore))) {
-			emusgx_debug_print("vSGX: Local dispatcher %d received signal...\n", manager_nr);
-			return;
-		}
-		if (stamp_i < 20)
-			stamps_waked[stamp_i] = ktime_get_real_ns();
-
-		// Dequeue the package
-		spin_lock(&(manager_entry->local_dispatch_queue_lock));
-
-		// Be quick
-		// Take out the first node and leave
-		current_node = manager_entry->local_dispatch_queue;
-		if (current_node == NULL) {
-			// Shit happens
-			spin_unlock(&(manager_entry->local_dispatch_queue_lock));
-			pr_err("vSGX: Unexpected error in local dispatch queue of %d\n", manager_nr);
-			continue;
-		}
-
-		package = current_node->package;
-
-		manager_entry->local_dispatch_queue = current_node->next;
-		if (manager_entry->local_dispatch_queue == NULL) {
-			// First node
-			manager_entry->local_dispatch_queue_tail = NULL;
-		}
-
-		kfree(current_node);
-
-		spin_unlock(&(manager_entry->local_dispatch_queue_lock));
-
-		// Now dispatch to the correct function
-		instr = *((uint8_t *)(package));
-		if (instr == EMUSGX_S_SWITCHLESS) {
-			emusgx_switchless_write_page((struct emusgx_page_package *)package);
-		}
-		// else if (instr == EMUSGX_S_PAGEREQ) {
-			// Should not be here
-		// }
-		// encls and enclu
-		// Have to be differentiated
-		else if (instr == EMUSGX_S_EENTER) {
-			// EENTER
-			// Executes the entrance function
-			emusgx_debug_print("vSGX: Dispatch to EENTER\n");
-			eenter_package = package;
-			response = emusgx_enter_enclave((void *)(eenter_package->tcs), (void *)(eenter_package->aep), (eenter_package->pid), &(eenter_package->regs), &pf_addr);
-
-			// We need to send a response
-			eenter_response.instr = EMUSGX_S_EENTER;
-			eenter_response.response = response;
-			eenter_response.linaddr = (uint64_t)pf_addr;
-			eenter_response.write_back = 0;
-			eenter_response.with_va = 0;
-			// Send package
-			if (emusgx_send_data(&eenter_response, sizeof(struct emusgx_raw_response))) {
-				pr_info("vSGX: Failed to response eenter\n");
-			}
-		}
-		else {
-			// encls
-			emusgx_debug_print("vSGX: Dispatcher doing ENCLS\n");
-			emusgx_do_remote_for_encls(instr, package);
-			emusgx_debug_print("vSGX: Dispatcher done ENCLS\n");
-		}
-		if (stamp_i < 20)
-			stamps_done[stamp_i] = ktime_get_real_ns();
-		stamp_i += 1;
-		if (stamp_i == 20) {
-			for (stamp_i = 0; stamp_i < 20; stamp_i++) {
-				pr_info("EmuSGX: 9 = %lld\n", stamps_dispatched[stamp_i] - stamps_waked[stamp_i]);
-			}
-			for (stamp_i = 0; stamp_i < 20; stamp_i++) {
-				pr_info("EmuSGX: 10 = %lld\n", stamps_done[stamp_i] - stamps_waked[stamp_i]);
-			}
-			stamp_i = 100;
-		}
-		kfree(package);
-	}
-}
diff --git a/kernel/emusgx/mprotect.c b/kernel/emusgx/mprotect.c
new file mode 100644
index 000000000..dd70c70ff
--- /dev/null
+++ b/kernel/emusgx/mprotect.c
@@ -0,0 +1,704 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ *  mm/mprotect.c
+ *
+ *  (C) Copyright 1994 Linus Torvalds
+ *  (C) Copyright 2002 Christoph Hellwig
+ *
+ *  Address space accounting code	<alan@lxorguk.ukuu.org.uk>
+ *  (C) Copyright 2002 Red Hat Inc, All Rights Reserved
+ */
+
+#include <linux/pagewalk.h>
+#include <linux/hugetlb.h>
+#include <linux/shm.h>
+#include <linux/mman.h>
+#include <linux/fs.h>
+#include <linux/highmem.h>
+#include <linux/security.h>
+#include <linux/mempolicy.h>
+#include <linux/personality.h>
+#include <linux/syscalls.h>
+#include <linux/swap.h>
+#include <linux/swapops.h>
+#include <linux/mmu_notifier.h>
+#include <linux/migrate.h>
+#include <linux/perf_event.h>
+#include <linux/pkeys.h>
+#include <linux/ksm.h>
+#include <linux/uaccess.h>
+#include <linux/mm_inline.h>
+#include <linux/pgtable.h>
+#include <asm/cacheflush.h>
+#include <asm/mmu_context.h>
+#include <asm/tlbflush.h>
+
+#include "internal.h"
+
+static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
+		unsigned long addr, unsigned long end, pgprot_t newprot,
+		unsigned long cp_flags)
+{
+	pte_t *pte, oldpte;
+	spinlock_t *ptl;
+	unsigned long pages = 0;
+	int target_node = NUMA_NO_NODE;
+	bool dirty_accountable = cp_flags & MM_CP_DIRTY_ACCT;
+	bool prot_numa = cp_flags & MM_CP_PROT_NUMA;
+	bool uffd_wp = cp_flags & MM_CP_UFFD_WP;
+	bool uffd_wp_resolve = cp_flags & MM_CP_UFFD_WP_RESOLVE;
+
+	/*
+	 * Can be called with only the mmap_lock for reading by
+	 * prot_numa so we must check the pmd isn't constantly
+	 * changing from under us from pmd_none to pmd_trans_huge
+	 * and/or the other way around.
+	 */
+	if (pmd_trans_unstable(pmd))
+		return 0;
+
+	/*
+	 * The pmd points to a regular pte so the pmd can't change
+	 * from under us even if the mmap_lock is only hold for
+	 * reading.
+	 */
+	pte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);
+
+	/* Get target node for single threaded private VMAs */
+	if (prot_numa && !(vma->vm_flags & VM_SHARED) &&
+	    atomic_read(&vma->vm_mm->mm_users) == 1)
+		target_node = numa_node_id();
+
+	flush_tlb_batched_pending(vma->vm_mm);
+	arch_enter_lazy_mmu_mode();
+	do {
+		oldpte = *pte;
+		if (pte_present(oldpte)) {
+			pte_t ptent;
+			bool preserve_write = prot_numa && pte_write(oldpte);
+
+			/*
+			 * Avoid trapping faults against the zero or KSM
+			 * pages. See similar comment in change_huge_pmd.
+			 */
+			if (prot_numa) {
+				struct page *page;
+
+				/* Avoid TLB flush if possible */
+				if (pte_protnone(oldpte))
+					continue;
+
+				page = vm_normal_page(vma, addr, oldpte);
+				if (!page || PageKsm(page))
+					continue;
+
+				/* Also skip shared copy-on-write pages */
+				if (is_cow_mapping(vma->vm_flags) &&
+				    page_mapcount(page) != 1)
+					continue;
+
+				/*
+				 * While migration can move some dirty pages,
+				 * it cannot move them all from MIGRATE_ASYNC
+				 * context.
+				 */
+				if (page_is_file_lru(page) && PageDirty(page))
+					continue;
+
+				/*
+				 * Don't mess with PTEs if page is already on the node
+				 * a single-threaded process is running on.
+				 */
+				if (target_node == page_to_nid(page))
+					continue;
+			}
+
+			oldpte = ptep_modify_prot_start(vma, addr, pte);
+			ptent = pte_modify(oldpte, newprot);
+			if (preserve_write)
+				ptent = pte_mk_savedwrite(ptent);
+
+			if (uffd_wp) {
+				ptent = pte_wrprotect(ptent);
+				ptent = pte_mkuffd_wp(ptent);
+			} else if (uffd_wp_resolve) {
+				/*
+				 * Leave the write bit to be handled
+				 * by PF interrupt handler, then
+				 * things like COW could be properly
+				 * handled.
+				 */
+				ptent = pte_clear_uffd_wp(ptent);
+			}
+
+			/* Avoid taking write faults for known dirty pages */
+			if (dirty_accountable && pte_dirty(ptent) &&
+					(pte_soft_dirty(ptent) ||
+					 !(vma->vm_flags & VM_SOFTDIRTY))) {
+				ptent = pte_mkwrite(ptent);
+			}
+			ptep_modify_prot_commit(vma, addr, pte, oldpte, ptent);
+			pages++;
+		} else if (is_swap_pte(oldpte)) {
+			swp_entry_t entry = pte_to_swp_entry(oldpte);
+			pte_t newpte;
+
+			if (is_write_migration_entry(entry)) {
+				/*
+				 * A protection check is difficult so
+				 * just be safe and disable write
+				 */
+				make_migration_entry_read(&entry);
+				newpte = swp_entry_to_pte(entry);
+				if (pte_swp_soft_dirty(oldpte))
+					newpte = pte_swp_mksoft_dirty(newpte);
+				if (pte_swp_uffd_wp(oldpte))
+					newpte = pte_swp_mkuffd_wp(newpte);
+			} else if (is_write_device_private_entry(entry)) {
+				/*
+				 * We do not preserve soft-dirtiness. See
+				 * copy_one_pte() for explanation.
+				 */
+				make_device_private_entry_read(&entry);
+				newpte = swp_entry_to_pte(entry);
+				if (pte_swp_uffd_wp(oldpte))
+					newpte = pte_swp_mkuffd_wp(newpte);
+			} else {
+				newpte = oldpte;
+			}
+
+			if (uffd_wp)
+				newpte = pte_swp_mkuffd_wp(newpte);
+			else if (uffd_wp_resolve)
+				newpte = pte_swp_clear_uffd_wp(newpte);
+
+			if (!pte_same(oldpte, newpte)) {
+				set_pte_at(vma->vm_mm, addr, pte, newpte);
+				pages++;
+			}
+		}
+	} while (pte++, addr += PAGE_SIZE, addr != end);
+	arch_leave_lazy_mmu_mode();
+	pte_unmap_unlock(pte - 1, ptl);
+
+	return pages;
+}
+
+/*
+ * Used when setting automatic NUMA hinting protection where it is
+ * critical that a numa hinting PMD is not confused with a bad PMD.
+ */
+static inline int pmd_none_or_clear_bad_unless_trans_huge(pmd_t *pmd)
+{
+	pmd_t pmdval = pmd_read_atomic(pmd);
+
+	/* See pmd_none_or_trans_huge_or_clear_bad for info on barrier */
+#ifdef CONFIG_TRANSPARENT_HUGEPAGE
+	barrier();
+#endif
+
+	if (pmd_none(pmdval))
+		return 1;
+	if (pmd_trans_huge(pmdval))
+		return 0;
+	if (unlikely(pmd_bad(pmdval))) {
+		pmd_clear_bad(pmd);
+		return 1;
+	}
+
+	return 0;
+}
+
+static inline unsigned long change_pmd_range(struct vm_area_struct *vma,
+		pud_t *pud, unsigned long addr, unsigned long end,
+		pgprot_t newprot, unsigned long cp_flags)
+{
+	pmd_t *pmd;
+	unsigned long next;
+	unsigned long pages = 0;
+	unsigned long nr_huge_updates = 0;
+	struct mmu_notifier_range range;
+
+	range.start = 0;
+
+	pmd = pmd_offset(pud, addr);
+	do {
+		unsigned long this_pages;
+
+		next = pmd_addr_end(addr, end);
+
+		/*
+		 * Automatic NUMA balancing walks the tables with mmap_lock
+		 * held for read. It's possible a parallel update to occur
+		 * between pmd_trans_huge() and a pmd_none_or_clear_bad()
+		 * check leading to a false positive and clearing.
+		 * Hence, it's necessary to atomically read the PMD value
+		 * for all the checks.
+		 */
+		if (!is_swap_pmd(*pmd) && !pmd_devmap(*pmd) &&
+		     pmd_none_or_clear_bad_unless_trans_huge(pmd))
+			goto next;
+
+		/* invoke the mmu notifier if the pmd is populated */
+		if (!range.start) {
+			mmu_notifier_range_init(&range,
+				MMU_NOTIFY_PROTECTION_VMA, 0,
+				vma, vma->vm_mm, addr, end);
+			mmu_notifier_invalidate_range_start(&range);
+		}
+
+		if (is_swap_pmd(*pmd) || pmd_trans_huge(*pmd) || pmd_devmap(*pmd)) {
+			if (next - addr != HPAGE_PMD_SIZE) {
+				__split_huge_pmd(vma, pmd, addr, false, NULL);
+			} else {
+				int nr_ptes = change_huge_pmd(vma, pmd, addr,
+							      newprot, cp_flags);
+
+				if (nr_ptes) {
+					if (nr_ptes == HPAGE_PMD_NR) {
+						pages += HPAGE_PMD_NR;
+						nr_huge_updates++;
+					}
+
+					/* huge pmd was handled */
+					goto next;
+				}
+			}
+			/* fall through, the trans huge pmd just split */
+		}
+		this_pages = change_pte_range(vma, pmd, addr, next, newprot,
+					      cp_flags);
+		pages += this_pages;
+next:
+		cond_resched();
+	} while (pmd++, addr = next, addr != end);
+
+	if (range.start)
+		mmu_notifier_invalidate_range_end(&range);
+
+	if (nr_huge_updates)
+		count_vm_numa_events(NUMA_HUGE_PTE_UPDATES, nr_huge_updates);
+	return pages;
+}
+
+static inline unsigned long change_pud_range(struct vm_area_struct *vma,
+		p4d_t *p4d, unsigned long addr, unsigned long end,
+		pgprot_t newprot, unsigned long cp_flags)
+{
+	pud_t *pud;
+	unsigned long next;
+	unsigned long pages = 0;
+
+	pud = pud_offset(p4d, addr);
+	do {
+		next = pud_addr_end(addr, end);
+		if (pud_none_or_clear_bad(pud))
+			continue;
+		pages += change_pmd_range(vma, pud, addr, next, newprot,
+					  cp_flags);
+	} while (pud++, addr = next, addr != end);
+
+	return pages;
+}
+
+static inline unsigned long change_p4d_range(struct vm_area_struct *vma,
+		pgd_t *pgd, unsigned long addr, unsigned long end,
+		pgprot_t newprot, unsigned long cp_flags)
+{
+	p4d_t *p4d;
+	unsigned long next;
+	unsigned long pages = 0;
+
+	p4d = p4d_offset(pgd, addr);
+	do {
+		next = p4d_addr_end(addr, end);
+		if (p4d_none_or_clear_bad(p4d))
+			continue;
+		pages += change_pud_range(vma, p4d, addr, next, newprot,
+					  cp_flags);
+	} while (p4d++, addr = next, addr != end);
+
+	return pages;
+}
+
+static unsigned long change_protection_range(struct vm_area_struct *vma,
+		unsigned long addr, unsigned long end, pgprot_t newprot,
+		unsigned long cp_flags)
+{
+	struct mm_struct *mm = vma->vm_mm;
+	pgd_t *pgd;
+	unsigned long next;
+	unsigned long start = addr;
+	unsigned long pages = 0;
+
+	BUG_ON(addr >= end);
+	pgd = pgd_offset(mm, addr);
+	flush_cache_range(vma, addr, end);
+	inc_tlb_flush_pending(mm);
+	do {
+		next = pgd_addr_end(addr, end);
+		if (pgd_none_or_clear_bad(pgd))
+			continue;
+		pages += change_p4d_range(vma, pgd, addr, next, newprot,
+					  cp_flags);
+	} while (pgd++, addr = next, addr != end);
+
+	/* Only flush the TLB if we actually modified any entries: */
+	if (pages)
+		flush_tlb_range(vma, start, end);
+	dec_tlb_flush_pending(mm);
+
+	return pages;
+}
+
+unsigned long change_protection(struct vm_area_struct *vma, unsigned long start,
+		       unsigned long end, pgprot_t newprot,
+		       unsigned long cp_flags)
+{
+	unsigned long pages;
+
+	BUG_ON((cp_flags & MM_CP_UFFD_WP_ALL) == MM_CP_UFFD_WP_ALL);
+
+	if (is_vm_hugetlb_page(vma))
+		pages = hugetlb_change_protection(vma, start, end, newprot);
+	else
+		pages = change_protection_range(vma, start, end, newprot,
+						cp_flags);
+
+	return pages;
+}
+
+static int prot_none_pte_entry(pte_t *pte, unsigned long addr,
+			       unsigned long next, struct mm_walk *walk)
+{
+	return pfn_modify_allowed(pte_pfn(*pte), *(pgprot_t *)(walk->private)) ?
+		0 : -EACCES;
+}
+
+static int prot_none_hugetlb_entry(pte_t *pte, unsigned long hmask,
+				   unsigned long addr, unsigned long next,
+				   struct mm_walk *walk)
+{
+	return pfn_modify_allowed(pte_pfn(*pte), *(pgprot_t *)(walk->private)) ?
+		0 : -EACCES;
+}
+
+static int prot_none_test(unsigned long addr, unsigned long next,
+			  struct mm_walk *walk)
+{
+	return 0;
+}
+
+static const struct mm_walk_ops prot_none_walk_ops = {
+	.pte_entry		= prot_none_pte_entry,
+	.hugetlb_entry		= prot_none_hugetlb_entry,
+	.test_walk		= prot_none_test,
+};
+
+int
+mprotect_fixup(struct vm_area_struct *vma, struct vm_area_struct **pprev,
+	unsigned long start, unsigned long end, unsigned long newflags)
+{
+	struct mm_struct *mm = vma->vm_mm;
+	unsigned long oldflags = vma->vm_flags;
+	long nrpages = (end - start) >> PAGE_SHIFT;
+	unsigned long charged = 0;
+	pgoff_t pgoff;
+	int error;
+	int dirty_accountable = 0;
+
+	if (newflags == oldflags) {
+		*pprev = vma;
+		return 0;
+	}
+
+	/*
+	 * Do PROT_NONE PFN permission checks here when we can still
+	 * bail out without undoing a lot of state. This is a rather
+	 * uncommon case, so doesn't need to be very optimized.
+	 */
+	if (arch_has_pfn_modify_check() &&
+	    (vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP)) &&
+	    (newflags & VM_ACCESS_FLAGS) == 0) {
+		pgprot_t new_pgprot = vm_get_page_prot(newflags);
+
+		error = walk_page_range(current->mm, start, end,
+				&prot_none_walk_ops, &new_pgprot);
+		if (error)
+			return error;
+	}
+
+	/*
+	 * If we make a private mapping writable we increase our commit;
+	 * but (without finer accounting) cannot reduce our commit if we
+	 * make it unwritable again. hugetlb mapping were accounted for
+	 * even if read-only so there is no need to account for them here
+	 */
+	if (newflags & VM_WRITE) {
+		/* Check space limits when area turns into data. */
+		if (!may_expand_vm(mm, newflags, nrpages) &&
+				may_expand_vm(mm, oldflags, nrpages))
+			return -ENOMEM;
+		if (!(oldflags & (VM_ACCOUNT|VM_WRITE|VM_HUGETLB|
+						VM_SHARED|VM_NORESERVE))) {
+			charged = nrpages;
+			if (security_vm_enough_memory_mm(mm, charged))
+				return -ENOMEM;
+			newflags |= VM_ACCOUNT;
+		}
+	}
+
+	/*
+	 * First try to merge with previous and/or next vma.
+	 */
+	pgoff = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);
+	*pprev = vma_merge(mm, *pprev, start, end, newflags,
+			   vma->anon_vma, vma->vm_file, pgoff, vma_policy(vma),
+			   vma->vm_userfaultfd_ctx);
+	if (*pprev) {
+		vma = *pprev;
+		VM_WARN_ON((vma->vm_flags ^ newflags) & ~VM_SOFTDIRTY);
+		goto success;
+	}
+
+	*pprev = vma;
+
+	if (start != vma->vm_start) {
+		error = split_vma(mm, vma, start, 1);
+		if (error)
+			goto fail;
+	}
+
+	if (end != vma->vm_end) {
+		error = split_vma(mm, vma, end, 0);
+		if (error)
+			goto fail;
+	}
+
+success:
+	/*
+	 * vm_flags and vm_page_prot are protected by the mmap_lock
+	 * held in write mode.
+	 */
+	vma->vm_flags = newflags;
+	dirty_accountable = vma_wants_writenotify(vma, vma->vm_page_prot);
+	vma_set_page_prot(vma);
+
+	change_protection(vma, start, end, vma->vm_page_prot,
+			  dirty_accountable ? MM_CP_DIRTY_ACCT : 0);
+
+	/*
+	 * Private VM_LOCKED VMA becoming writable: trigger COW to avoid major
+	 * fault on access.
+	 */
+	if ((oldflags & (VM_WRITE | VM_SHARED | VM_LOCKED)) == VM_LOCKED &&
+			(newflags & VM_WRITE)) {
+		populate_vma_page_range(vma, start, end, NULL);
+	}
+
+	vm_stat_account(mm, oldflags, -nrpages);
+	vm_stat_account(mm, newflags, nrpages);
+	perf_event_mmap(vma);
+	return 0;
+
+fail:
+	vm_unacct_memory(charged);
+	return error;
+}
+
+/*
+ * pkey==-1 when doing a legacy mprotect()
+ */
+static int do_mprotect_pkey(unsigned long start, size_t len,
+		unsigned long prot, int pkey)
+{
+	unsigned long nstart, end, tmp, reqprot;
+	struct vm_area_struct *vma, *prev;
+	int error = -EINVAL;
+	const int grows = prot & (PROT_GROWSDOWN|PROT_GROWSUP);
+	const bool rier = (current->personality & READ_IMPLIES_EXEC) &&
+				(prot & PROT_READ);
+
+	start = untagged_addr(start);
+
+	prot &= ~(PROT_GROWSDOWN|PROT_GROWSUP);
+	if (grows == (PROT_GROWSDOWN|PROT_GROWSUP)) /* can't be both */
+		return -EINVAL;
+
+	if (start & ~PAGE_MASK)
+		return -EINVAL;
+	if (!len)
+		return 0;
+	len = PAGE_ALIGN(len);
+	end = start + len;
+	if (end <= start)
+		return -ENOMEM;
+	if (!arch_validate_prot(prot, start))
+		return -EINVAL;
+
+	reqprot = prot;
+
+	if (mmap_write_lock_killable(current->mm))
+		return -EINTR;
+
+	/*
+	 * If userspace did not allocate the pkey, do not let
+	 * them use it here.
+	 */
+	error = -EINVAL;
+	if ((pkey != -1) && !mm_pkey_is_allocated(current->mm, pkey))
+		goto out;
+
+	vma = find_vma(current->mm, start);
+	error = -ENOMEM;
+	if (!vma)
+		goto out;
+	prev = vma->vm_prev;
+	if (unlikely(grows & PROT_GROWSDOWN)) {
+		if (vma->vm_start >= end)
+			goto out;
+		start = vma->vm_start;
+		error = -EINVAL;
+		if (!(vma->vm_flags & VM_GROWSDOWN))
+			goto out;
+	} else {
+		if (vma->vm_start > start)
+			goto out;
+		if (unlikely(grows & PROT_GROWSUP)) {
+			end = vma->vm_end;
+			error = -EINVAL;
+			if (!(vma->vm_flags & VM_GROWSUP))
+				goto out;
+		}
+	}
+	if (start > vma->vm_start)
+		prev = vma;
+
+	for (nstart = start ; ; ) {
+		unsigned long mask_off_old_flags;
+		unsigned long newflags;
+		int new_vma_pkey;
+
+		/* Here we know that vma->vm_start <= nstart < vma->vm_end. */
+
+		/* Does the application expect PROT_READ to imply PROT_EXEC */
+		if (rier && (vma->vm_flags & VM_MAYEXEC))
+			prot |= PROT_EXEC;
+
+		/*
+		 * Each mprotect() call explicitly passes r/w/x permissions.
+		 * If a permission is not passed to mprotect(), it must be
+		 * cleared from the VMA.
+		 */
+		mask_off_old_flags = VM_READ | VM_WRITE | VM_EXEC |
+					VM_FLAGS_CLEAR;
+
+		new_vma_pkey = arch_override_mprotect_pkey(vma, prot, pkey);
+		newflags = calc_vm_prot_bits(prot, new_vma_pkey);
+		newflags |= (vma->vm_flags & ~mask_off_old_flags);
+
+		/* newflags >> 4 shift VM_MAY% in place of VM_% */
+		if ((newflags & ~(newflags >> 4)) & VM_ACCESS_FLAGS) {
+			error = -EACCES;
+			goto out;
+		}
+
+		/* Allow architectures to sanity-check the new flags */
+		if (!arch_validate_flags(newflags)) {
+			error = -EINVAL;
+			goto out;
+		}
+
+		error = security_file_mprotect(vma, reqprot, prot);
+		if (error)
+			goto out;
+
+		tmp = vma->vm_end;
+		if (tmp > end)
+			tmp = end;
+		error = mprotect_fixup(vma, &prev, nstart, tmp, newflags);
+		if (error)
+			goto out;
+		nstart = tmp;
+
+		if (nstart < prev->vm_end)
+			nstart = prev->vm_end;
+		if (nstart >= end)
+			goto out;
+
+		vma = prev->vm_next;
+		if (!vma || vma->vm_start != nstart) {
+			error = -ENOMEM;
+			goto out;
+		}
+		prot = reqprot;
+	}
+out:
+	mmap_write_unlock(current->mm);
+	return error;
+}
+
+SYSCALL_DEFINE3(mprotect, unsigned long, start, size_t, len,
+		unsigned long, prot)
+{
+	return do_mprotect_pkey(start, len, prot, -1);
+}
+
+int vsgx_mprotect(unsigned long start, size_t len, unsigned long prot) {
+	return do_mprotect_pkey(start, len, prot, -1);
+}
+
+#ifdef CONFIG_ARCH_HAS_PKEYS
+
+SYSCALL_DEFINE4(pkey_mprotect, unsigned long, start, size_t, len,
+		unsigned long, prot, int, pkey)
+{
+	return do_mprotect_pkey(start, len, prot, pkey);
+}
+
+SYSCALL_DEFINE2(pkey_alloc, unsigned long, flags, unsigned long, init_val)
+{
+	int pkey;
+	int ret;
+
+	/* No flags supported yet. */
+	if (flags)
+		return -EINVAL;
+	/* check for unsupported init values */
+	if (init_val & ~PKEY_ACCESS_MASK)
+		return -EINVAL;
+
+	mmap_write_lock(current->mm);
+	pkey = mm_pkey_alloc(current->mm);
+
+	ret = -ENOSPC;
+	if (pkey == -1)
+		goto out;
+
+	ret = arch_set_user_pkey_access(current, pkey, init_val);
+	if (ret) {
+		mm_pkey_free(current->mm, pkey);
+		goto out;
+	}
+	ret = pkey;
+out:
+	mmap_write_unlock(current->mm);
+	return ret;
+}
+
+SYSCALL_DEFINE1(pkey_free, int, pkey)
+{
+	int ret;
+
+	mmap_write_lock(current->mm);
+	ret = mm_pkey_free(current->mm, pkey);
+	mmap_write_unlock(current->mm);
+
+	/*
+	 * We could provie warnings or errors if any VMA still
+	 * has the pkey set here.
+	 */
+	return ret;
+}
+
+#endif /* CONFIG_ARCH_HAS_PKEYS */
diff --git a/kernel/emusgx/sender.c b/kernel/emusgx/sender.c
index 3b8ced2a1..7e74ab555 100644
--- a/kernel/emusgx/sender.c
+++ b/kernel/emusgx/sender.c
@@ -4,7 +4,7 @@
 #include <linux/slab.h>
 #include <linux/highmem.h>
 #include <linux/string.h>
-#include <linux/spinlock.h>
+#include <linux/mutex.h>
 
 #include <asm/atomic.h>
 #include <asm/tlbflush.h>
@@ -18,7 +18,7 @@
 #include <emusgx/emusgx_cpuid.h>
 
 static uint64_t emusgx_session = 0;
-static DEFINE_SPINLOCK(emusgx_session_number_lock);
+static DEFINE_MUTEX(emusgx_session_number_lock);
 
 // Should be generated randomly
 uint64_t emusgx_enclave_vm_id = 0;
@@ -166,13 +166,16 @@ void emusgx_init_shared_page(void) {
 
 	// The very first session number
 	// Can-sleep environment
-	spin_lock(&emusgx_session_number_lock);
+	if (mutex_lock_killable(&emusgx_session_number_lock)) {
+		pr_info("vSGX: Init shared page killed\n");
+		return;
+	}
 	current_session = emusgx_session++;
 	if (emusgx_session >= EMUSGX_MAX_SESSION_NUMBER) {
 		// Reset session number
 		emusgx_session = 0;
 	}
-	spin_unlock(&emusgx_session_number_lock);
+	mutex_unlock(&emusgx_session_number_lock);
 
 	// Capsulate the data
 	plain_package->session_number = current_session;
@@ -308,13 +311,16 @@ int emusgx_send_data(void *addr, uint64_t size) {
 
 	// First we get our session number
 	// Can-sleep environment
-	spin_lock(&emusgx_session_number_lock);
+	if (mutex_lock_killable(&emusgx_session_number_lock)) {
+		pr_info("vSGX: Send data killed\n");
+		return -1;
+	}
 	current_session = emusgx_session++;
 	if (emusgx_session >= EMUSGX_MAX_SESSION_NUMBER) {
 		// Reset session number
 		emusgx_session = 0;
 	}
-	spin_unlock(&emusgx_session_number_lock);
+	mutex_unlock(&emusgx_session_number_lock);
 
 	// Next pack data into pages and send them
 	turns = size / EMUSGX_PAYLOAD_SIZE;
diff --git a/kernel/emusgx/switchless_sync.c b/kernel/emusgx/switchless_sync.c
index da09e78f4..f344ff9ac 100644
--- a/kernel/emusgx/switchless_sync.c
+++ b/kernel/emusgx/switchless_sync.c
@@ -1,8 +1,9 @@
 #include <linux/kernel.h>
-#include <linux/spinlock.h>
+#include <linux/mutex.h>
 #include <linux/uaccess.h>
 #include <linux/semaphore.h>
 #include <linux/mm.h>
+#include <linux/mman.h>
 #include <linux/slab.h>
 #include <linux/sched.h>
 #include <linux/delay.h>
@@ -12,36 +13,48 @@
 #include <emusgx/emusgx_sender.h>
 #include <emusgx/emusgx_debug.h>
 
+extern int vsgx_mprotect(unsigned long start, size_t len, unsigned long prot);
+
 #define EMUSGX_SWITCHLESS_SLOT_FREE	0
 #define EMUSGX_SWITCHLESS_SLOT_INUSE	1
 
 struct emusgx_switchless_page_slot {
 	uint8_t status;
-	spinlock_t lock;
+	struct mutex lock;
 	void *addr;
 	uint8_t *original_content;
 };
 
 static uint64_t emusgx_switchless_sync_index = 0;
-static DEFINE_SPINLOCK(emusgx_switchless_index_lock);
+static DEFINE_MUTEX(emusgx_switchless_index_lock);
 
-struct emusgx_switchless_page_slot emusgx_switchless_pages[EMUSGX_SWITCHLESS_SLOT_COUNT] = { [0 ... EMUSGX_SWITCHLESS_SLOT_COUNT - 1] = { .status = EMUSGX_SWITCHLESS_SLOT_FREE, .lock = __SPIN_LOCK_UNLOCKED(emusgx_switchless_lock), .addr = NULL, .original_content = NULL } };
+struct emusgx_switchless_page_slot emusgx_switchless_pages[EMUSGX_SWITCHLESS_SLOT_COUNT] = { [0 ... EMUSGX_SWITCHLESS_SLOT_COUNT - 1] = { .status = EMUSGX_SWITCHLESS_SLOT_FREE, .addr = NULL, .original_content = NULL } };
+
+void vsgx_switchless_init_locks() {
+	int i;
+	for (i = 0; i < EMUSGX_SWITCHLESS_SLOT_COUNT; i++) {
+		emusgx_switchless_pages[i].lock = (struct mutex)__MUTEX_INITIALIZER(emusgx_switchless_pages[i].lock);
+	}
+}
 
 void emusgx_switchless_write_page(struct emusgx_page_package *package) {
 	// Make sure the page still exists
 	int i, group, bit;
 	for (i = 0; i < EMUSGX_SWITCHLESS_SLOT_COUNT; i++) {
 		// First grab the lock. Can sleep
-		spin_lock(&emusgx_switchless_pages[i].lock);
+		if (mutex_lock_killable(&emusgx_switchless_pages[i].lock)) {
+			pr_info("vSGX: Switchless write page killed\n");
+			return;
+		}
 
 		if (emusgx_switchless_pages[i].status != EMUSGX_SWITCHLESS_SLOT_INUSE) {
-			spin_unlock(&emusgx_switchless_pages[i].lock);
+			mutex_unlock(&emusgx_switchless_pages[i].lock);
 			// Go ahead
 			continue;
 		}
 
 		if ((uint64_t)(emusgx_switchless_pages[i].addr) != package->addr) {
-			spin_unlock(&emusgx_switchless_pages[i].lock);
+			mutex_unlock(&emusgx_switchless_pages[i].lock);
 			// Go ahead
 			continue;
 		}
@@ -76,7 +89,7 @@ void emusgx_switchless_write_page(struct emusgx_page_package *package) {
 	__uaccess_end();
 
 	// Release the lock
-	spin_unlock(&emusgx_switchless_pages[i].lock);
+	mutex_unlock(&emusgx_switchless_pages[i].lock);
 
 	// We do not clear the dirty bit since there's no way to tell if ant
 	// threads changed our data during the writing process
@@ -101,40 +114,40 @@ int _enclave_local emusgx_sync_on_dirty(int slot) {
 
 	pgd = pgd_offset(mm, addr);
 	if (pgd_none(*pgd) || pgd_bad(*pgd)) {
-		pr_info("EmuSGX: Page walk failed on PGD\n");
+	//	pr_info("EmuSGX: Page walk failed on PGD\n");
 		goto out;
 	}
 
 	p4d = p4d_offset(pgd, addr);
 	if (p4d_none(*p4d) || p4d_bad(*p4d)) {
-		pr_info("EmuSGX: Page walk failed on  P4D");
+	//	pr_info("EmuSGX: Page walk failed on  P4D");
     		goto out;
 	}
 
 	pud = pud_offset(p4d, addr);
 	if (pud_none(*pud) || pud_bad(*pud)) {
-		pr_info("EmuSGX: Page walk failed on PUD at 0x%016llX when syncing on dirty\n", (uint64_t)addr);
+	//	pr_info("EmuSGX: Page walk failed on PUD at 0x%016llX when syncing on dirty\n", (uint64_t)addr);
 		goto out;
 	}
 
 	pmd = pmd_offset(pud, addr);
 	if (pmd_none(*pmd)) {
-		pr_info("EmuSGX: Page walk failed on PMD - none");
+	//	pr_info("EmuSGX: Page walk failed on PMD - none");
 		goto out;
 	}
 	if (pmd_bad(*pmd)) {
-		pr_info("EmuSGX: Page walk failed on PMD - bad");
+	//	pr_info("EmuSGX: Page walk failed on PMD - bad");
 		goto out;
 	}
 
 	pte = pte_offset_map(pmd, addr);
 	if (!pte) {
-		pr_info("EmuSGX: Page walk failed on PTE");
+	//	pr_info("EmuSGX: Page walk failed on PTE");
 		goto out;
 	}
 
 	if (!pte_present(*pte)) {
-		pr_info("EmuSGX: PTE not presenting\n");
+	//	pr_info("EmuSGX: PTE not presenting\n");
 		goto out;
 	}
 
@@ -206,40 +219,40 @@ void emusgx_clear_dirty(void *address) {
 
 	pgd = pgd_offset(mm, addr);
 	if (pgd_none(*pgd) || pgd_bad(*pgd)) {
-		pr_info("EmuSGX: Page walk failed on PGD\n");
+	//	pr_info("EmuSGX: Page walk failed on PGD\n");
 		goto out;
 	}
 
 	p4d = p4d_offset(pgd, addr);
 	if (p4d_none(*p4d) || p4d_bad(*p4d)) {
-		pr_info("EmuSGX: Page walk failed on  P4D");
+	//	pr_info("EmuSGX: Page walk failed on  P4D");
     		goto out;
 	}
 
 	pud = pud_offset(p4d, addr);
 	if (pud_none(*pud) || pud_bad(*pud)) {
-		pr_info("EmuSGX: Page walk failed on PUD at 0x%016llX when making clean\n", (uint64_t)addr);
+	//	pr_info("EmuSGX: Page walk failed on PUD at 0x%016llX when making clean\n", (uint64_t)addr);
 		goto out;
 	}
 
 	pmd = pmd_offset(pud, addr);
 	if (pmd_none(*pmd)) {
-		pr_info("EmuSGX: Page walk failed on PMD - none");
+	//	pr_info("EmuSGX: Page walk failed on PMD - none");
 		goto out;
 	}
 	if (pmd_bad(*pmd)) {
-		pr_info("EmuSGX: Page walk failed on PMD - bad");
+	//	pr_info("EmuSGX: Page walk failed on PMD - bad");
 		goto out;
 	}
 
 	pte = pte_offset_map(pmd, addr);
 	if (!pte) {
-		pr_info("EmuSGX: Page walk failed on PTE");
+	//	pr_info("EmuSGX: Page walk failed on PTE");
 		goto out;
 	}
 
 	if (!pte_present(*pte)) {
-		pr_info("EmuSGX: PTE not presenting\n");
+	//	pr_info("EmuSGX: PTE not presenting\n");
 		goto out;
 	}
 
@@ -266,7 +279,10 @@ int _enclave_local emusgx_switchless_new_slot(void *addr, void *page_data) {
 	int munmap_error;
 	uint64_t index;
 
-	spin_lock(&emusgx_switchless_index_lock);
+	if (mutex_lock_killable(&emusgx_switchless_index_lock)) {
+		pr_info("vSGX: Switchless new slot killed when getting index lock\n");
+		return -1;
+	}
 
 	index = emusgx_switchless_sync_index;
 	emusgx_switchless_sync_index += 1;
@@ -274,19 +290,28 @@ int _enclave_local emusgx_switchless_new_slot(void *addr, void *page_data) {
 		emusgx_switchless_sync_index = 0;
 	}
 
-	spin_unlock(&emusgx_switchless_index_lock);
+	mutex_unlock(&emusgx_switchless_index_lock);
 
 	// First, get the slot
 	// Can-sleep
-	spin_lock(&emusgx_switchless_pages[index].lock);
+	if (mutex_lock_killable(&emusgx_switchless_pages[index].lock)) {
+		pr_info("vSGX: Switchless new slot killed\n");
+		return -1;
+	}
 
 	if (emusgx_switchless_pages[index].status != EMUSGX_SWITCHLESS_SLOT_FREE) {
+		// Stop any writing to the page
+		//if (vsgx_mprotect((unsigned long)emusgx_switchless_pages[index].addr, 4096, PROT_READ)) {
+		//	pr_info("vSGX: Failed to set memory read only before flushing for the last time\n");
+		//}
 		// Sync for the last time
+		// It is quite important to know that this time on the guest VM side
+		// the page is already flushed from the switchless syncing list
 		emusgx_sync_on_dirty(index);
 		// Unmap the original addr from the memory
 		if (mmap_write_lock_killable(current->mm)) {
 			pr_info("EmuSGX: mmap failed to get lock\n");
-			spin_unlock(&emusgx_switchless_pages[index].lock);
+			mutex_unlock(&emusgx_switchless_pages[index].lock);
 			return -1;
 		}
 		munmap_error = do_munmap(current->mm, (uint64_t)emusgx_switchless_pages[index].addr, 4096, NULL);
@@ -315,7 +340,7 @@ int _enclave_local emusgx_switchless_new_slot(void *addr, void *page_data) {
 	// at this moment because this IS the process
 	memcpy(emusgx_switchless_pages[index].original_content, page_data, 4096);
 
-	spin_unlock(&emusgx_switchless_pages[index].lock);
+	mutex_unlock(&emusgx_switchless_pages[index].lock);
 
 	// The slot is ready
 
@@ -326,16 +351,19 @@ int emusgx_switchless_get_slot(void *addr) {
 	// Make sure the page still exists
 	int i;
 	for (i = 0; i < EMUSGX_SWITCHLESS_SLOT_COUNT; i++) {
-		spin_lock(&emusgx_switchless_pages[i].lock);
+		if (mutex_lock_killable(&emusgx_switchless_pages[i].lock)) {
+			pr_info("vSGX: Switchless get slot killed\n");
+			return -1;
+		}
 
 		if (emusgx_switchless_pages[i].status == EMUSGX_SWITCHLESS_SLOT_INUSE) {
 			if (emusgx_switchless_pages[i].addr == addr) {
-				spin_unlock(&emusgx_switchless_pages[i].lock);
+				mutex_unlock(&emusgx_switchless_pages[i].lock);
 				return i;
 			}
 		}
 
-		spin_unlock(&emusgx_switchless_pages[i].lock);
+		mutex_unlock(&emusgx_switchless_pages[i].lock);
 			
 	}
 	return -1;
@@ -345,16 +373,19 @@ int emusgx_switchless_get_and_hold_slot(void *addr) {
 	// Make sure the page still exists
 	int i;
 	for (i = 0; i < EMUSGX_SWITCHLESS_SLOT_COUNT; i++) {
-		spin_lock(&emusgx_switchless_pages[i].lock);
+		if (mutex_lock_killable(&emusgx_switchless_pages[i].lock)) {
+			pr_info("vSGX: Switchless get slot and hold killed\n");
+			return -1;
+		}
 
 		if (emusgx_switchless_pages[i].status == EMUSGX_SWITCHLESS_SLOT_INUSE) {
 			if (emusgx_switchless_pages[i].addr == addr) {
-				// spin_unlock(&emusgx_switchless_pages[i].lock);
+				// mutex_unlock(&emusgx_switchless_pages[i].lock);
 				return i;
 			}
 		}
 
-		spin_unlock(&emusgx_switchless_pages[i].lock);
+		mutex_unlock(&emusgx_switchless_pages[i].lock);
 			
 	}
 	return -1;
@@ -363,10 +394,14 @@ int emusgx_switchless_get_and_hold_slot(void *addr) {
 void _enclave_local emusgx_sync_all_pages(void) {
 	int i;
 	for (i = 0; i < EMUSGX_SWITCHLESS_SLOT_COUNT; i++) {
-		spin_lock(&emusgx_switchless_pages[i].lock);
+		if (mutex_lock_killable(&emusgx_switchless_pages[i].lock)) {
+			pr_info("vSGX: Switchless sync all pages killed\n");
+			return;
+		}
+
 		if (emusgx_switchless_pages[i].status == EMUSGX_SWITCHLESS_SLOT_INUSE)
 			emusgx_sync_on_dirty(i);
-		spin_unlock(&emusgx_switchless_pages[i].lock);
+		mutex_unlock(&emusgx_switchless_pages[i].lock);
 		// We don't care successful or not since we are doing lazy syncing
 	}
 }
diff --git a/kernel/sched/Makefile b/kernel/sched/Makefile
index 5fc9c9b70..15d819e56 100644
--- a/kernel/sched/Makefile
+++ b/kernel/sched/Makefile
@@ -22,7 +22,7 @@ ifneq ($(CONFIG_SCHED_OMIT_FRAME_POINTER),y)
 CFLAGS_core.o := $(PROFILING) -fno-omit-frame-pointer
 endif
 
-obj-y += core.o loadavg.o clock.o cputime.o
+obj-y += core.o loadavg.o clock.o cputime.o enclave_context.o
 obj-y += idle.o fair.o rt.o deadline.o
 obj-y += wait.o wait_bit.o swait.o completion.o
 
diff --git a/kernel/sched/enclave_context.c b/kernel/sched/enclave_context.c
new file mode 100644
index 000000000..22858e8eb
--- /dev/null
+++ b/kernel/sched/enclave_context.c
@@ -0,0 +1,83 @@
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/mm.h>
+#include <linux/sched/mm.h>
+#include <linux/slab.h>
+#include <linux/highmem.h>
+#include <linux/string.h>
+#include <linux/spinlock.h>
+#include <linux/vmacache.h>
+
+#include <asm/atomic.h>
+#include <asm/tlbflush.h>
+#include <asm/cacheflush.h>
+#include <asm/io.h>
+#include <asm/mmu_context.h>
+
+#include <emusgx/emusgx.h>
+#include <emusgx/emusgx_internal.h>
+#include <emusgx/emusgx_sender.h>
+#include <emusgx/emusgx_mm.h>
+#include <emusgx/emusgx_cpuid.h>
+
+#include "sched.h"
+
+static struct mm_struct *enclave_mm;
+static struct mm_struct *manager_mm;
+
+int vsgx_init_enclave_context(void) {
+	// Create a new empty mm_struct
+	enclave_mm = mm_alloc();
+	if (!enclave_mm) {
+		return -1;
+	}
+	manager_mm = current->mm;
+	smp_mb();
+	atomic_set(&enclave_mm->membarrier_state, 0);
+	enclave_mm->vmacache_seqnum = 0;
+	
+	// Setup MMAP
+	arch_pick_mmap_layout(enclave_mm, &(current->signal->rlim[RLIMIT_STACK]));
+	return 0;
+}
+
+static void vsgx_swap_memory_context(struct mm_struct *mm) {
+	struct mm_struct *active_mm;
+
+	task_lock(current);
+	smp_mb();
+	/*
+	 * Keep the runqueue membarrier_state in sync with this mm
+	 * membarrier_state.
+	 */
+	this_cpu_write(runqueues.membarrier_state,
+		       atomic_read(&mm->membarrier_state));
+
+	local_irq_disable();
+	active_mm = current->active_mm;
+	current->active_mm = mm;
+	current->mm = mm;
+	/*
+	 * This prevents preemption while active_mm is being loaded and
+	 * it and mm are being updated, which could cause problems for
+	 * lazy tlb mm refcounting when these are updated by context
+	 * switches. Not all architectures can handle irqs off over
+	 * activate_mm yet.
+	 */
+	if (!IS_ENABLED(CONFIG_ARCH_WANT_IRQS_OFF_ACTIVATE_MM))
+		local_irq_enable();
+	activate_mm(active_mm, mm);
+	if (IS_ENABLED(CONFIG_ARCH_WANT_IRQS_OFF_ACTIVATE_MM))
+		local_irq_enable();
+	vmacache_flush(current);
+	task_unlock(current);
+}
+
+void vsgx_swap_to_enclave_context(void) {
+	// Like an execve
+	vsgx_swap_memory_context(enclave_mm);
+}
+
+void vsgx_swap_to_manager_context(void) {
+	vsgx_swap_memory_context(manager_mm);
+}
diff --git a/mm/mmap.c b/mm/mmap.c
index 5c8b44858..4940b9f02 100644
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@ -1588,6 +1588,75 @@ unsigned long do_mmap(struct file *file, unsigned long addr,
 	return addr;
 }
 
+// MUST BE CALLED WITH current->mm LOCKED!
+// The page must be filled already
+unsigned long vsgx_map_untrusted_page_to_enclave(unsigned long addr, struct page *page) {
+	struct vm_area_struct *vma, *prev;
+	struct rb_node **rb_link, *rb_parent;
+	vm_flags_t vm_flags;
+	int pkey;
+	unsigned long pgoff;
+	int err;
+
+	addr = get_unmapped_area(NULL, addr, 4096, 0, MAP_FIXED | MAP_FIXED_NOREPLACE | MAP_ANONYMOUS| MAP_PRIVATE);
+	if (IS_ERR_VALUE(addr)) {
+		return addr;
+	}
+
+	vma = find_vma(current->mm, addr);
+	if (vma && vma->vm_start < addr + 4096) {
+		pr_err("vSGX: Mapping untrusted page to an existing mapping!\n");
+		return -EEXIST;
+	}
+
+	pkey = execute_only_pkey(mm);
+	if (pkey < 0)
+		pkey = 0;
+
+	vm_flags = calc_vm_prot_bits(PROT_READ | PROT_WRITE | PROT_EXEC, pkey) | calc_vm_flag_bits(MAP_FIXED | MAP_FIXED_NOREPLACE | MAP_ANONYMOUS| MAP_PRIVATE) |
+			mm->def_flags | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC | VM_MIXEDMAP;
+	
+	while (find_vma_links(current->mm, addr, start + 4096, &prev, &rb_link, &rb_parent));
+
+	vma = vm_area_alloc(current->mm);
+	if (!vma) {
+		pr_err("vSGX: Mapping untrusted page to an existing mapping!\n");
+		return -ENOMEM;
+	}
+
+	pgoff = addr >> PAGE_SHIFT;
+
+	vma_set_anonymous(vma);
+	vma->vm_start = addr;
+	vma->vm_end = addr + 4096;
+	vma->vm_flags = vm_flags;
+	vma->vm_page_prot = vm_get_page_prot(vm_flags);
+	vma->vm_pgoff = pgoff;
+
+	vma->vm_flags |= VM_SOFTDIRTY;
+
+	vma_set_page_prot(vma);
+
+	// We insert the page into the VMA before the VMA is inserted
+	// because once the VMA is inserted and someone tries to access the page,
+	// it might cause the anonymous action to simply map an empty page to it
+	err = vm_insert_page(vma, addr, page);
+	if (err) {
+		pr_err("vSGX: Failed to insert the page into VMA at addr\n");
+		vm_area_free(vma);
+		return (long)err;
+	}
+
+	err = insert_vm_struct(current->mm, vma);
+	if (err) {
+		pr_err("vSGX: Failed to insert VM struct into current process\n");
+		vm_area_free(vma);
+		return (long)err;
+	}
+
+	return addr;
+}
+
 unsigned long ksys_mmap_pgoff(unsigned long addr, unsigned long len,
 			      unsigned long prot, unsigned long flags,
 			      unsigned long fd, unsigned long pgoff)
diff --git a/mm/mprotect.c b/mm/mprotect.c
index 56c02beb6..dd70c70ff 100644
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@ -644,6 +644,10 @@ SYSCALL_DEFINE3(mprotect, unsigned long, start, size_t, len,
 	return do_mprotect_pkey(start, len, prot, -1);
 }
 
+int vsgx_mprotect(unsigned long start, size_t len, unsigned long prot) {
+	return do_mprotect_pkey(start, len, prot, -1);
+}
+
 #ifdef CONFIG_ARCH_HAS_PKEYS
 
 SYSCALL_DEFINE4(pkey_mprotect, unsigned long, start, size_t, len,
-- 
2.25.1


From 6eff28239cc56c1eb67c83441a4cbd0c79505fc9 Mon Sep 17 00:00:00 2001
From: NSKernel <zhao.3289@osu.edu>
Date: Wed, 19 May 2021 10:17:41 -0400
Subject: [PATCH 7/9] Bug fixed

---
 arch/x86/mm/fault.c              | 41 +++++++++++++---
 include/emusgx/emusgx_internal.h |  4 +-
 kernel/emusgx/aex.c              |  2 +-
 kernel/emusgx/enclu.c            |  2 +-
 kernel/emusgx/switchless_sync.c  | 81 +++++++++++++++++++++++---------
 mm/mmap.c                        |  6 +--
 6 files changed, 100 insertions(+), 36 deletions(-)

diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c
index a6aa3cd0e..bb68377cf 100644
--- a/arch/x86/mm/fault.c
+++ b/arch/x86/mm/fault.c
@@ -33,6 +33,7 @@
 #include <asm/cpu_entry_area.h>		/* exception stack		*/
 #include <asm/pgtable_areas.h>		/* VMALLOC_START, ...		*/
 #include <asm/kvm_para.h>		/* kvm_handle_async_pf		*/
+#include <asm/cacheflush.h>
 
 #define CREATE_TRACE_POINTS
 #include <asm/trace/exceptions.h>
@@ -1222,13 +1223,16 @@ do_kern_addr_fault(struct pt_regs *regs, unsigned long hw_error_code,
 }
 NOKPROBE_SYMBOL(do_kern_addr_fault);
 
+extern unsigned long vsgx_map_untrusted_page_to_enclave(unsigned long addr, struct page *page);
+
 static int emusgx_handle_fault(struct mm_struct *mm, unsigned long address, struct pt_regs *regs) {
 	struct vm_area_struct *vma;
 	uint64_t page_addr = (address >> PAGE_SHIFT) << PAGE_SHIFT;
 	long mmap_error;
 	void *page_data;
+	struct page *data_page;
 	uint8_t get_page_ret_val;
-	unsigned long mmap_populate;
+	//unsigned long mmap_populate;
 	int i;
 	uint64_t waiting_nr;
 	struct semaphore *other_waiting_semaphore;
@@ -1249,6 +1253,16 @@ static int emusgx_handle_fault(struct mm_struct *mm, unsigned long address, stru
 		return 0;
 	}
 	if (page_data != NULL) {
+		// Allocate user space page
+		data_page = alloc_page(GFP_KERNEL & ~__GFP_RECLAIM);
+		if (data_page == NULL) {
+			pr_info("vSGX: Failed to allocate page for user space\n");
+			return -1;
+		}
+
+		// Copy data to the newly allocated page
+		memcpy(page_address(data_page), page_data, 4096);
+		clflush_cache_range(page_address(data_page), 4096);
 		// Create mmap for such page
 		// We have mm locked so we could go all the way into do_mmap
 		mmap_read_unlock(mm);
@@ -1259,8 +1273,9 @@ static int emusgx_handle_fault(struct mm_struct *mm, unsigned long address, stru
 			return -1;
 		}
 		// pr_info("EmuSGX: Doing mmap @ 0x%016llX\n", page_addr);
-		mmap_error = do_mmap(NULL, page_addr, 4096, PROT_READ | PROT_WRITE | PROT_EXEC, 
-			MAP_FIXED_NOREPLACE | MAP_ANONYMOUS| MAP_PRIVATE, 0, &mmap_populate, NULL);
+		mmap_error = vsgx_map_untrusted_page_to_enclave(page_addr, data_page);
+		//mmap_error = do_mmap(NULL, page_addr, 4096, PROT_READ | PROT_WRITE | PROT_EXEC, 
+		//	MAP_FIXED_NOREPLACE | MAP_ANONYMOUS| MAP_PRIVATE, 0, &mmap_populate, NULL);
 		// pr_info("EmuSGX: mmap is done\n");
 		mmap_write_unlock(mm);
 		// Release the write semaphore and grab the read semaphore again
@@ -1275,16 +1290,16 @@ static int emusgx_handle_fault(struct mm_struct *mm, unsigned long address, stru
 			if (likely(vma)) {
 				// Write page
 				emusgx_clear_dirty((void *)page_addr);
-				if (copy_to_user((void *)page_addr, page_data, 4096)) {
-					pr_info("EmuSGX: Failed to copy to user\n");
-				}
+				//if (copy_to_user((void *)page_addr, page_data, 4096)) {
+				//	pr_info("EmuSGX: Failed to copy to user\n");
+				//}
 				// Clear the dirty bit
 				// page_data needs to be manually freed
 				// Create new slot for switchless syncing
 				mmap_read_unlock(mm);
 				// Here the create new slot will need to down_write mem_sem
 				// So we have to leave the semaphore open
-				if (emusgx_switchless_new_slot((void *)page_addr, page_data)) {
+				if (emusgx_switchless_new_slot((void *)page_addr, page_data, data_page)) {
 					pr_info("EmuSGX: Failed to create slot\n");
 					// This is deadly
 					// Will fall to bad area
@@ -1293,6 +1308,8 @@ static int emusgx_handle_fault(struct mm_struct *mm, unsigned long address, stru
 					mmap_read_lock(mm);
 					return -1;
 				}
+
+				// Not used anymore
 				kfree(page_data);
 
 				for (i = 0; i < waiting_nr; i++) {
@@ -1434,9 +1451,13 @@ void do_user_addr_fault(struct pt_regs *regs,
 	vma = find_vma(mm, address);
 	if (unlikely(!vma)) {
 		if (current->is_enclave_thread) {
+			// Do not allow double fault to fetch things in the kernel
+			current->is_enclave_thread = 0;
 			if (!emusgx_handle_fault(mm, address, regs)) {
+				current->is_enclave_thread = 1;
 				return;
 			}
+			current->is_enclave_thread = 1;
 			pr_err("vSGX: Failed to handle page fault at 0x%016llX\n", (uint64_t)address);
 			// AEX
 			vsgx_aex_on_current_thread(regs, 14, hw_error_code, address );
@@ -1449,9 +1470,12 @@ void do_user_addr_fault(struct pt_regs *regs,
 		goto good_area;
 	if (unlikely(!(vma->vm_flags & VM_GROWSDOWN))) {
 		if (current->is_enclave_thread) {
+			current->is_enclave_thread = 0;
 			if (!emusgx_handle_fault(mm, address, regs)) {
+				current->is_enclave_thread = 1;
 				return;
 			}
+			current->is_enclave_thread = 1;
 			pr_err("vSGX: Failed to handle page fault at 0x%016llX\n", (uint64_t)address);
 			// AEX
 			vsgx_aex_on_current_thread(regs, 14, hw_error_code, address);
@@ -1461,9 +1485,12 @@ void do_user_addr_fault(struct pt_regs *regs,
 		return;
 	}
 	if (current->is_enclave_thread) {
+		current->is_enclave_thread = 0;
 		if (!emusgx_handle_fault(mm, address, regs)) {
+			current->is_enclave_thread = 1;
 			return;
 		}
+		current->is_enclave_thread = 1;
 		pr_err("vSGX: Failed to handle page fault at 0x%016llX\n", (uint64_t)address);
 		// An enclave thread must not expand its stack anyway
 		// We do not allow doing that since the real payload
diff --git a/include/emusgx/emusgx_internal.h b/include/emusgx/emusgx_internal.h
index 231cec6d1..72584f668 100644
--- a/include/emusgx/emusgx_internal.h
+++ b/include/emusgx/emusgx_internal.h
@@ -152,10 +152,10 @@ int emusgx_send_data(void *addr, uint64_t size);
 
 void vsgx_switchless_init_locks(void);
 void _enclave_local emusgx_switchless_write_page(struct emusgx_page_package *package);
-int _enclave_local emusgx_switchless_new_slot(void *addr, void *page_data);
+int _enclave_local emusgx_switchless_new_slot(void *addr, void *page_data, struct page *page);
 int emusgx_switchless_get_slot(void *addr);
 int emusgx_switchless_get_and_hold_slot(void *addr);
-void _enclave_local emusgx_sync_all_pages(void);
+void _enclave_local emusgx_sync_all_pages(char print, char force_sync);
 void _enclave_local emusgx_switchless_sync_worker(void);
 
 uint64_t emusgx_enter_enclave(struct sgx_tcs *tcs, void *aep, uint64_t pid, struct emusgx_full_regs *regs, void **pf_addr);
diff --git a/kernel/emusgx/aex.c b/kernel/emusgx/aex.c
index 5a8cd3f10..d809ccdea 100644
--- a/kernel/emusgx/aex.c
+++ b/kernel/emusgx/aex.c
@@ -36,7 +36,7 @@ void _enclave_local vsgx_aex_on_current_thread(struct pt_regs *ptrace_regs, uint
 
 	// Make sure all pages are written back to the guest VM
 	emusgx_debug_print("EmuSGX: AEX... Now syncing all pages\n");
-	emusgx_sync_all_pages();
+	emusgx_sync_all_pages(0, 1);
 	emusgx_debug_print("EmuSGX: Pages are synced\n");
 
 	package = kmalloc(sizeof(struct emusgx_eexit_package), GFP_KERNEL);
diff --git a/kernel/emusgx/enclu.c b/kernel/emusgx/enclu.c
index 273493ec9..635822920 100644
--- a/kernel/emusgx/enclu.c
+++ b/kernel/emusgx/enclu.c
@@ -941,7 +941,7 @@ void emusgx_eexit(struct emusgx_regs *reg_status, struct pt_regs *ptrace_regs) {
 
 	// Make sure all pages are written back to the guest VM
 	emusgx_debug_print("EmuSGX: Exiting... Now syncing all pages\n");
-	emusgx_sync_all_pages();
+	emusgx_sync_all_pages(0, 1);
 	emusgx_debug_print("EmuSGX: Pages are synced\n");
 
 	__uaccess_begin();
diff --git a/kernel/emusgx/switchless_sync.c b/kernel/emusgx/switchless_sync.c
index f344ff9ac..cefa26a2a 100644
--- a/kernel/emusgx/switchless_sync.c
+++ b/kernel/emusgx/switchless_sync.c
@@ -7,6 +7,9 @@
 #include <linux/slab.h>
 #include <linux/sched.h>
 #include <linux/delay.h>
+#include <linux/highmem.h>
+
+#include <asm/cacheflush.h>
 
 #include <emusgx/emusgx.h>
 #include <emusgx/emusgx_internal.h>
@@ -22,6 +25,7 @@ struct emusgx_switchless_page_slot {
 	uint8_t status;
 	struct mutex lock;
 	void *addr;
+	struct page *page;
 	uint8_t *original_content;
 };
 
@@ -86,6 +90,9 @@ void emusgx_switchless_write_page(struct emusgx_page_package *package) {
 		}
 	}
 
+	// Force a cache line flush to ensure visible
+	clflush_cache_range(emusgx_switchless_pages[i].addr, 4096);
+	clflush_cache_range(emusgx_switchless_pages[i].original_content, 4096);
 	__uaccess_end();
 
 	// Release the lock
@@ -99,7 +106,7 @@ void emusgx_switchless_write_page(struct emusgx_page_package *package) {
 
 // Must be called with the slot locked!
 // Only the owner manager can call this!
-int _enclave_local emusgx_sync_on_dirty(int slot) {
+int _enclave_local emusgx_sync_on_dirty(int slot, struct page *in_kernel_page, char force_sync) {
 	uint64_t addr = (uint64_t)emusgx_switchless_pages[slot].addr;
 	struct emusgx_page_package *package;
 	int group, bit, need_to_send;
@@ -111,7 +118,10 @@ int _enclave_local emusgx_sync_on_dirty(int slot) {
 	pmd_t *pmd;
 	pte_t *pte;
 	struct mm_struct *mm = current->mm;
+	void *kaddr = NULL;
 
+	if (force_sync)
+		goto out;
 	pgd = pgd_offset(mm, addr);
 	if (pgd_none(*pgd) || pgd_bad(*pgd)) {
 	//	pr_info("EmuSGX: Page walk failed on PGD\n");
@@ -155,6 +165,7 @@ int _enclave_local emusgx_sync_on_dirty(int slot) {
 	if (pte_dirty(*pte)) {
 		// Mark the page clear
 		set_pte(pte, pte_mkclean(*pte));
+		__flush_tlb_one_user(addr);
 		// Force a sync if huge
 out:
 		emusgx_debug_print("EmuSGX: Ever dirty?\n");
@@ -164,25 +175,32 @@ int _enclave_local emusgx_sync_on_dirty(int slot) {
 			return -1;
 		}
 
-		__uaccess_begin();
+		//clflush_cache_range(emusgx_switchless_pages[slot].addr, 4096);
 
 		// Write the mask
+		// Note that we sync data from the kernel mapped page
+		// So the sync can still happen even if we have unmapped the page from the user space
 		need_to_send = 0;
+		kaddr = kmap_atomic(in_kernel_page);
+		clflush_cache_range(kaddr, 4096);
+		//pr_info("1st byet is %d, orig %d\n", ((char *)kaddr)[1], ((char *)emusgx_switchless_pages[slot].original_content)[1]);
 		for (group = 0; group < 512; group++) {
 			package->mask[group] = 0;
 			for (bit = 0; bit < 8; bit++) {
-				package->page[group * 8 + bit] = ((uint8_t *)(emusgx_switchless_pages[slot].addr))[group * 8 + bit];
+				package->page[group * 8 + bit] = ((uint8_t *)(kaddr))[group * 8 + bit];
 				if (emusgx_switchless_pages[slot].original_content[group * 8 + bit] != package->page[group * 8 + bit]) {
 					// Set bit 1
 					package->mask[group] |= ((uint64_t)1 << bit);
 					emusgx_switchless_pages[slot].original_content[group * 8 + bit] = package->page[group * 8 + bit];
-					need_to_send = 1;
+					need_to_send += 1;
 				}
 			}
 		}
+		clflush_cache_range(emusgx_switchless_pages[slot].original_content, 4096);
+		kunmap_atomic(kaddr);
 
-		__uaccess_end();
-
+		//pr_info("vSGX: %d changes in 0x%016llX\n", need_to_send, (uint64_t)emusgx_switchless_pages[slot].addr);
+		
 		if (need_to_send) {
 			package->instr = EMUSGX_S_SWITCHLESS;
 			package->addr = (uint64_t)emusgx_switchless_pages[slot].addr;
@@ -194,6 +212,9 @@ int _enclave_local emusgx_sync_on_dirty(int slot) {
 				return -1;
 			}
 		}
+		//else {
+		//	pr_info("vSGX: Dirty but no need to be sent 0x%016llX\n", (uint64_t)emusgx_switchless_pages[slot].addr);
+		//}
 
 		kfree(package);
 
@@ -261,7 +282,7 @@ void emusgx_clear_dirty(void *address) {
 	;
 }
 
-int _enclave_local emusgx_switchless_new_slot(void *addr, void *page_data) {
+int _enclave_local emusgx_switchless_new_slot(void *addr, void *page_data, struct page *page) {
 	// The addr should have already been mapped and have data in it
 	// This should be the final step of a good page fault
 	// Caller must be the registering enclave manager
@@ -278,6 +299,7 @@ int _enclave_local emusgx_switchless_new_slot(void *addr, void *page_data) {
 
 	int munmap_error;
 	uint64_t index;
+	struct page *in_kernel_page;
 
 	if (mutex_lock_killable(&emusgx_switchless_index_lock)) {
 		pr_info("vSGX: Switchless new slot killed when getting index lock\n");
@@ -300,25 +322,31 @@ int _enclave_local emusgx_switchless_new_slot(void *addr, void *page_data) {
 	}
 
 	if (emusgx_switchless_pages[index].status != EMUSGX_SWITCHLESS_SLOT_FREE) {
-		// Stop any writing to the page
-		//if (vsgx_mprotect((unsigned long)emusgx_switchless_pages[index].addr, 4096, PROT_READ)) {
-		//	pr_info("vSGX: Failed to set memory read only before flushing for the last time\n");
-		//}
-		// Sync for the last time
 		// It is quite important to know that this time on the guest VM side
 		// the page is already flushed from the switchless syncing list
-		emusgx_sync_on_dirty(index);
-		// Unmap the original addr from the memory
+		
+		// Unmap the original addr from the memory to stop any writing to the page
+		clflush_cache_range(emusgx_switchless_pages[index].addr, 4096);
+		// For some weird reason the mapped page is not the same as what we allocated
+		// So we will be using get_user_pages to get the real backing of the page
+		get_user_pages((unsigned long)emusgx_switchless_pages[index].addr, 1, 0, &in_kernel_page, NULL);
 		if (mmap_write_lock_killable(current->mm)) {
 			pr_info("EmuSGX: mmap failed to get lock\n");
+			put_page(in_kernel_page);
 			mutex_unlock(&emusgx_switchless_pages[index].lock);
 			return -1;
 		}
 		munmap_error = do_munmap(current->mm, (uint64_t)emusgx_switchless_pages[index].addr, 4096, NULL);
 		mmap_write_unlock(current->mm);
 		if (munmap_error) {
-			pr_info("EmuSGX: Unexpected non-mapped address in an inuse sync slot\n");
+			pr_info("EmuSGX: Unexpected non-mapped address in an inuse sync slot with error %d\n", munmap_error);
 		}
+		// Sync for the final time
+		emusgx_sync_on_dirty(index, in_kernel_page, 1);
+		put_page(in_kernel_page);
+		// Free the page
+		__free_page(emusgx_switchless_pages[index].page);
+		//pr_info("vSGX: Swapped out 0x%016llX\n", (uint64_t)emusgx_switchless_pages[index].addr);
 	}
 	else {
 		emusgx_switchless_pages[index].original_content = NULL;
@@ -327,6 +355,7 @@ int _enclave_local emusgx_switchless_new_slot(void *addr, void *page_data) {
 	// Update the slot
 	emusgx_switchless_pages[index].status = EMUSGX_SWITCHLESS_SLOT_INUSE;
 	emusgx_switchless_pages[index].addr = addr;
+	emusgx_switchless_pages[index].page = page;
 	// Lazy allocate, reuse if not NULL
 	if (emusgx_switchless_pages[index].original_content == NULL) {
 		emusgx_switchless_pages[index].original_content = kmalloc(4096, GFP_KERNEL);
@@ -335,10 +364,9 @@ int _enclave_local emusgx_switchless_new_slot(void *addr, void *page_data) {
 		pr_info("EmuSGX: I cannot allocate data for the original_content\n");
 		return -1;
 	}
-	// Update the original_content field
-	// Note that the process won't be able to write to the page
-	// at this moment because this IS the process
+	// Update the original_content field with the "original content"
 	memcpy(emusgx_switchless_pages[index].original_content, page_data, 4096);
+	clflush_cache_range(emusgx_switchless_pages[index].original_content, 4096);
 
 	mutex_unlock(&emusgx_switchless_pages[index].lock);
 
@@ -391,16 +419,25 @@ int emusgx_switchless_get_and_hold_slot(void *addr) {
 	return -1;
 }
 
-void _enclave_local emusgx_sync_all_pages(void) {
+void _enclave_local emusgx_sync_all_pages(char print, char force_sync) {
 	int i;
+	struct page *in_kernel_page;
+
 	for (i = 0; i < EMUSGX_SWITCHLESS_SLOT_COUNT; i++) {
 		if (mutex_lock_killable(&emusgx_switchless_pages[i].lock)) {
 			pr_info("vSGX: Switchless sync all pages killed\n");
 			return;
 		}
 
-		if (emusgx_switchless_pages[i].status == EMUSGX_SWITCHLESS_SLOT_INUSE)
-			emusgx_sync_on_dirty(i);
+		if (emusgx_switchless_pages[i].status == EMUSGX_SWITCHLESS_SLOT_INUSE) {
+			clflush_cache_range(emusgx_switchless_pages[i].addr, 4096);
+			get_user_pages((unsigned long)emusgx_switchless_pages[i].addr, 1, 0, &in_kernel_page, NULL);
+			emusgx_sync_on_dirty(i, in_kernel_page, force_sync);
+			if (unlikely(print)) {
+				pr_info("Synced %d 0x%016llX\n", print, (uint64_t)emusgx_switchless_pages[i].addr);
+			}
+			put_page(in_kernel_page);
+		}
 		mutex_unlock(&emusgx_switchless_pages[i].lock);
 		// We don't care successful or not since we are doing lazy syncing
 	}
@@ -413,7 +450,7 @@ void _enclave_local emusgx_switchless_sync_worker(void) {
 
 	while(true) {
 		// every 100 ms we sync all 10 slots on demand
-		emusgx_sync_all_pages();
+		emusgx_sync_all_pages(0, 0);
 
 		// now wait for 100 ms
 		msleep(100);
diff --git a/mm/mmap.c b/mm/mmap.c
index 4940b9f02..9dea1f01b 100644
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@ -1609,14 +1609,14 @@ unsigned long vsgx_map_untrusted_page_to_enclave(unsigned long addr, struct page
 		return -EEXIST;
 	}
 
-	pkey = execute_only_pkey(mm);
+	pkey = execute_only_pkey(current->mm);
 	if (pkey < 0)
 		pkey = 0;
 
 	vm_flags = calc_vm_prot_bits(PROT_READ | PROT_WRITE | PROT_EXEC, pkey) | calc_vm_flag_bits(MAP_FIXED | MAP_FIXED_NOREPLACE | MAP_ANONYMOUS| MAP_PRIVATE) |
-			mm->def_flags | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC | VM_MIXEDMAP;
+			current->mm->def_flags | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC | VM_MIXEDMAP;
 	
-	while (find_vma_links(current->mm, addr, start + 4096, &prev, &rb_link, &rb_parent));
+	while (find_vma_links(current->mm, addr, addr + 4096, &prev, &rb_link, &rb_parent));
 
 	vma = vm_area_alloc(current->mm);
 	if (!vma) {
-- 
2.25.1


From ceff14992ede0936fb6f18ef172f47903226f7a7 Mon Sep 17 00:00:00 2001
From: NSKernel <zhao.3289@osu.edu>
Date: Sat, 5 Jun 2021 09:16:26 -0400
Subject: [PATCH 8/9] Performance improved by reducing syncs

---
 arch/x86/kernel/traps.c         |  2 +-
 kernel/emusgx/entrance.c        |  3 ++-
 kernel/emusgx/switchless_sync.c | 14 ++++++++++++++
 3 files changed, 17 insertions(+), 2 deletions(-)

diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 7792dcc0a..de62f3f8a 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -461,7 +461,7 @@ DEFINE_IDTENTRY_RAW(exc_invalid_op)
 				//savesegment(gs, current->backup_gs);
 				rdmsrl(MSR_FS_BASE, current->backup_fsbase);
 				rdmsrl(MSR_KERNEL_GS_BASE, current->backup_gsbase);
-				
+
 				//loadsegment(fs, 0x0B);
 				wrmsrl(MSR_FS_BASE, handle_buffer->fsbase);
 				//load_gs_index(0x0B);
diff --git a/kernel/emusgx/entrance.c b/kernel/emusgx/entrance.c
index 1c1e22c65..a36ac4b0b 100644
--- a/kernel/emusgx/entrance.c
+++ b/kernel/emusgx/entrance.c
@@ -292,6 +292,7 @@ uint64_t _enclave_local emusgx_enter_enclave(struct sgx_tcs __user *tcs, void *a
 	// Up the handling semaphore
 	up(&(tmp_manager_entry->action_semaphore));
 
+
 	emusgx_debug_print("EmuSGX: action_semaphore@0x%016llX\n", (uint64_t)&(tmp_manager_entry->action_semaphore));
 
 	// Now there should be a process running
@@ -488,4 +489,4 @@ uint8_t vsgx_resume_enclave(struct sgx_tcs __user *tcs, uint64_t tcs_pa, uint64_
 	__uaccess_end();
 
 	return 0;
-}
\ No newline at end of file
+}
diff --git a/kernel/emusgx/switchless_sync.c b/kernel/emusgx/switchless_sync.c
index cefa26a2a..9e426758b 100644
--- a/kernel/emusgx/switchless_sync.c
+++ b/kernel/emusgx/switchless_sync.c
@@ -419,11 +419,22 @@ int emusgx_switchless_get_and_hold_slot(void *addr) {
 	return -1;
 }
 
+atomic_t vsgx_force_sync_in_progress = (atomic_t)ATOMIC_INIT(0);
+
 void _enclave_local emusgx_sync_all_pages(char print, char force_sync) {
 	int i;
 	struct page *in_kernel_page;
 
+	if (force_sync) {
+		 atomic_inc(&(vsgx_force_sync_in_progress));
+	}
+
 	for (i = 0; i < EMUSGX_SWITCHLESS_SLOT_COUNT; i++) {
+		if (atomic_read(&(vsgx_force_sync_in_progress)) && !force_sync) {
+			// Someone's forcing a sync
+			// We can leave now
+			break;
+		}
 		if (mutex_lock_killable(&emusgx_switchless_pages[i].lock)) {
 			pr_info("vSGX: Switchless sync all pages killed\n");
 			return;
@@ -441,6 +452,9 @@ void _enclave_local emusgx_sync_all_pages(char print, char force_sync) {
 		mutex_unlock(&emusgx_switchless_pages[i].lock);
 		// We don't care successful or not since we are doing lazy syncing
 	}
+	if (force_sync) {
+		atomic_dec(&(vsgx_force_sync_in_progress));
+	}
 }
 
 void _enclave_local emusgx_switchless_sync_worker(void) {
-- 
2.25.1


From 9ba0c53569955c17c79dabff08700ec369d286ce Mon Sep 17 00:00:00 2001
From: NSKernel <zhao.3289@osu.edu>
Date: Sat, 14 Aug 2021 03:23:01 -0400
Subject: [PATCH 9/9] Performance improvment and bug fixes

---
 kernel/emusgx/aex.c            |  2 +
 kernel/emusgx/dispatcher.c     | 75 ++++++++++++++++++++++------------
 kernel/emusgx/encls_cross_vm.c | 19 +++++----
 kernel/emusgx/enclu.c          | 21 +++++++++-
 kernel/emusgx/entrance.c       |  4 ++
 kernel/emusgx/fault.c          |  2 +-
 kernel/emusgx/irq.c            |  9 ++--
 7 files changed, 92 insertions(+), 40 deletions(-)

diff --git a/kernel/emusgx/aex.c b/kernel/emusgx/aex.c
index d809ccdea..89d3806a1 100644
--- a/kernel/emusgx/aex.c
+++ b/kernel/emusgx/aex.c
@@ -34,6 +34,8 @@ void _enclave_local vsgx_aex_on_current_thread(struct pt_regs *ptrace_regs, uint
 	struct emusgx_aex_package *package;
 	struct sgx_exinfo *exinfo;
 
+	pr_info("vSGX: AEX is triggered for exception %d. Error code is %d\n", exception_code, error_code);
+
 	// Make sure all pages are written back to the guest VM
 	emusgx_debug_print("EmuSGX: AEX... Now syncing all pages\n");
 	emusgx_sync_all_pages(0, 1);
diff --git a/kernel/emusgx/dispatcher.c b/kernel/emusgx/dispatcher.c
index 646e461d9..8c3dbaf6f 100644
--- a/kernel/emusgx/dispatcher.c
+++ b/kernel/emusgx/dispatcher.c
@@ -32,9 +32,11 @@ struct emusgx_dispatch_slot emusgx_instruction_emulation_slot = {.status = EMUSG
 static uint64_t emusgx_used_session_number[EMUSGX_MAX_SESSION_NUMBER_D64] = { [0 ... EMUSGX_MAX_SESSION_NUMBER_D64 - 1] = (uint64_t)0 }; // Maximum session number = 4095, session number ranged from [0, 4095]
 
 static uint64_t stamps_waked[20];
-uint64_t stamps_dispatched[20];
+static uint64_t stamps_waked_dispatched[20];
+static uint64_t stamps_dispatched[20];
+static uint64_t stamps_handled[20];
 static uint32_t stamp_i = 0;
-
+static uint32_t stamp_j = 0;
 /*
 void emusgx_print_page(void *page) {
 	uint64_t *ptr = page;
@@ -475,8 +477,19 @@ int _enclave_local emusgx_dispatcher(void) {
 			// Return
 			return -1;
 		}
-		if (stamp_i < 20)
+		if (stamp_i < 20) {
 			stamps_waked[stamp_i] = ktime_get_real_ns();
+			stamp_i += 1;
+		}
+		if (stamp_i == 20) {
+			for (stamp_i = 0; stamp_i < 20; stamp_i++) {
+				pr_info("vSGX: Time 7 End: %lld\n", stamps_waked[stamp_i]);
+			}
+			stamp_i = 100;
+		}
+		if (stamp_j < 20) {
+			stamps_waked_dispatched[stamp_j] = ktime_get_real_ns();
+		}
 
 		// Get data
 		// The lock is held with IRQ save
@@ -789,9 +802,9 @@ int _enclave_local emusgx_dispatcher(void) {
 
 		// Finalization: dispatch signal to the handler thread or just do it on our own
 		if (slot->current_order == slot->total_pages - 1) {
-			if (stamp_i < 20)
-				stamps_dispatched[stamp_i] = ktime_get_real_ns();
-			stamp_i+= 1;
+			if (stamp_j < 20) {
+				stamps_dispatched[stamp_j] = ktime_get_real_ns();
+			}
 			// Check the instr and dispatch
 			instr = *((uint8_t *)(slot->data));
 			if (instr == EMUSGX_S_SWITCHLESS) {
@@ -810,14 +823,15 @@ int _enclave_local emusgx_dispatcher(void) {
 					// Should not happen
 					pr_info("EmuSGX: Unexpected page fault node not found\n");
 				}
+				else {
+					// We then need to remove this node from the queue
+					emusgx_remove_pf_node(pf_node);
 
-				// We then need to remove this node from the queue
-				emusgx_remove_pf_node(pf_node);
-
-				// We will just wake up once
-				// The thread who firstly registered the request will wakeup others
-				// when the page is ready
-				up(pf_node->node_semaphore);
+					// We will just wake up once
+					// The thread who firstly registered the request will wakeup others
+					// when the page is ready
+					up(pf_node->node_semaphore);
+				}
 			}
 			else if (instr == EMUSGX_S_ERESUME) {
 				// EEXIT
@@ -827,12 +841,13 @@ int _enclave_local emusgx_dispatcher(void) {
 					// Should not happen
 					pr_info("EmuSGX: Unexpected AEX node not found\n");
 				}
-
-				// Wake up the semaphore
-				up(&aex_node->semaphore);
-
-				// The node will be freed by the woken-up thread
-				// in emusgx_wait_for_eexit_request
+				else {
+					// Wake up the semaphore
+					up(&aex_node->semaphore);
+	
+					// The node will be freed by the woken-up thread
+					// in emusgx_wait_for_eexit_request
+				}
 			}
 			// encls and enclu
 			// Have to be differentiated
@@ -881,7 +896,7 @@ int _enclave_local emusgx_dispatcher(void) {
 						// Free the slot and continue
 						kfree(slot->data);
 						slot->status = EMUSGX_DISPATCH_SLOT_FREE;
-						continue;
+						goto continue_point;
 					}
 
 					emusgx_debug_print("vSGX: New manager occupied\n");
@@ -901,15 +916,21 @@ int _enclave_local emusgx_dispatcher(void) {
 				kfree(slot->data);
 				slot->status = EMUSGX_DISPATCH_SLOT_FREE;
 			}
-		}
-		if (stamp_i == 20) {
-			for (stamp_i = 0; stamp_i < 20; stamp_i++) {
-				pr_info("EmuSGX: Waked = %lld\n", stamps_waked[stamp_i]);
+		
+			if (stamp_j < 20) {
+				stamps_handled[stamp_j] = ktime_get_real_ns();
+				stamp_j += 1;
 			}
-			for (stamp_i = 0; stamp_i < 20; stamp_i++) {
-				pr_info("EmuSGX: Dispatched = %lld\n", stamps_dispatched[stamp_i]);
+			if (stamp_j == 20) {
+				for (stamp_j = 0; stamp_j < 20; stamp_j++) {
+					pr_info("EmuSGX: Time 8 = %lld\n", stamps_dispatched[stamp_j] - stamps_waked_dispatched[stamp_j]);
+				}
+				for (stamp_j = 0; stamp_j < 20; stamp_j++) {
+					pr_info("EmuSGX: Time 9 = %lld\n", stamps_handled[stamp_j] - stamps_dispatched[stamp_j]);
+				}
+				stamp_j = 100;
 			}
-			stamp_i = 100;
+continue_point:;
 		}
 	
 	}
diff --git a/kernel/emusgx/encls_cross_vm.c b/kernel/emusgx/encls_cross_vm.c
index 8a962d458..dd2fc8b76 100644
--- a/kernel/emusgx/encls_cross_vm.c
+++ b/kernel/emusgx/encls_cross_vm.c
@@ -914,8 +914,11 @@ uint8_t emusgx_validate_and_do_remote_for_eldb_eldu(void *srcpage, struct sgx_se
 
 	// Copy 4KBytes SRCPGE to secure location
 	// memcpy(epc_page, srcpage, 4096); Just decrypt
-	vaslot = decrypted_va_page + (((uint64_t)vaslot_pa) & ((uint64_t)PAGE_MASK));
+	vaslot = decrypted_va_page + (((uint64_t)vaslot_pa) & ~((uint64_t)PAGE_MASK));
 	tmp_ver = vaslot[0];
+	if (tmp_ver != 0) {
+		pr_info("vSGX: WARNING: tmp_ver != 0. In current implementation according to Intel's manual (which definitely does not make any sense), all version should be 0\n");
+	}
 	// Does not make sense
 	// tmp_ver = tmp_ver << 32;
 
@@ -929,10 +932,12 @@ uint8_t emusgx_validate_and_do_remote_for_eldb_eldu(void *srcpage, struct sgx_se
 
 	decrypt_ret = emusgx_aes_128_gcm_dec(emusgx_cr_base_pk, &tmp_ver, &tmp_header, sizeof(struct emusgx_mac_header), srcpage, 4096, epc_page_buffer, pcmd->mac);
 	if (decrypt_ret == -EBADMSG) {
+		pr_info("vSGX: ELDB/ELDU failed to decrypt EPC page due to MAC mismatch\n");
+		pr_info("vSGX: pcmd->mac = %016llX %016llX\n", pcmd->mac[0], pcmd->mac[1]);
 		kfree(epc_page_buffer);
 		if (page_type == SGX_PT_SECS)
 			*free_manager = 1;
-		return 2;
+		return 1;
 	}
 	if (decrypt_ret != 0) {
 		// Shit happened
@@ -1238,6 +1243,9 @@ uint64_t emusgx_validate_and_do_remote_for_ewb(void *epc_page, void *vaslot_pa,
 	void *epc_page_buffer;
 	void *va_buffer;
 	void *decrypted_va_page;
+	
+	// Zero out TMP_HEADER
+        memset(&tmp_header, 0 , sizeof(tmp_header));
 
 	// Check within EPC
 	if (emusgx_check_within_epc(epc_page)) {
@@ -1301,9 +1309,6 @@ uint64_t emusgx_validate_and_do_remote_for_ewb(void *epc_page, void *vaslot_pa,
 		tmp_pcmd_enclaveid = 0;
 	}
 
-	// Zero out TMP_HEADER
-	memset(&tmp_header, 0 , sizeof(tmp_header));
-
 	if (epc_epcm->page_type == SGX_PT_VA || epc_epcm->page_type == SGX_PT_SECS) {
 		tmp_header.linaddr = 0;
 	}
@@ -1383,7 +1388,7 @@ uint64_t emusgx_validate_and_do_remote_for_ewb(void *epc_page, void *vaslot_pa,
 
 
 	// Check if version array slot was empty
-	vaslot = decrypted_va_page + (((uint64_t)vaslot_pa) & ((uint64_t)PAGE_MASK));
+	vaslot = decrypted_va_page + (((uint64_t)vaslot_pa) & ~((uint64_t)PAGE_MASK));
 	if (vaslot[0]) {
 		kfree(va_buffer);
 		kfree(decrypted_va_page);
@@ -1403,7 +1408,7 @@ uint64_t emusgx_validate_and_do_remote_for_ewb(void *epc_page, void *vaslot_pa,
 		kfree(decrypted_va_page);
 		return EMUSGX_GP;
 	}
-	memcpy(va_buffer, va_page_wb, 4096);
+	memcpy(va_page_wb, va_buffer, 4096);
 	kfree(va_buffer);
 	kfree(decrypted_va_page);
 
diff --git a/kernel/emusgx/enclu.c b/kernel/emusgx/enclu.c
index 635822920..85a2237dc 100644
--- a/kernel/emusgx/enclu.c
+++ b/kernel/emusgx/enclu.c
@@ -236,25 +236,30 @@ void emusgx_egetkey(struct emusgx_regs *reg_status, struct pt_regs *ptrace_regs)
 
 	// Make sure KEYREQUEST is properly aligned
 	if (reg_status->rbx % 128 != 0) {
+		pr_info("vSGX: EGETKEY: KEYREQUEST is not aligned\n");
 		emusgx_gp(0, ptrace_regs);
 	}
 
 	if (!vsgx_check_in_elrange(keyrequest)) {
+		pr_info("vSGX: EGETKEY: KEYREQUEST is not in ELRANGE\n");
 		emusgx_gp(0, ptrace_regs);
 	}
 
 	// Check within EPC
 	if (emusgx_check_within_epc(keyrequest_pa)) {
+		pr_info("vSGX: EGETKEY: KEYREQUEST is not in EPC\n");
 		emusgx_pf(keyrequest, ptrace_regs);
 	}
 
 	keyrequest_epcm = emusgx_get_epcm(keyrequest_pa);
 
 	if (keyrequest_epcm->valid == 0) {
+		pr_info("vSGX: EGETKEY: KEYREQUEST EPCM is not valid\n");
 		emusgx_pf(keyrequest, ptrace_regs);
 	}
 
 	if (keyrequest_epcm->blocked == 1) {
+		pr_info("vSGX: EGETKEY: KEYREQUEST is blocked \n");
 		emusgx_pf(keyrequest, ptrace_regs);
 	}
 
@@ -263,29 +268,35 @@ void emusgx_egetkey(struct emusgx_regs *reg_status, struct pt_regs *ptrace_regs)
 		(keyrequest_epcm->pending == 1) || (keyrequest_epcm->modified == 1) ||
 		((unsigned long)(keyrequest_epcm->enclave_address) != (reg_status->rbx - (reg_status->rbx % 0x1000))) ||
 		(keyrequest_epcm->R == 0)) {
+		pr_info("vSGX: EGETKEY: KEYREQUEST EPCM is incorrect\n");
 		emusgx_pf(keyrequest, ptrace_regs);
 	}
 
 	// Make sure OUTPUTDATA is properly aligned
 	if (reg_status->rcx % 16 != 0) {
+		pr_info("vSGX: EGETKEY: OUTPUTDATA is not aligned\n");
 		emusgx_gp(0, ptrace_regs);
 	}
 	
 	if (!vsgx_check_in_elrange(outputdata)) {
+		pr_info("vSGX: EGETKEY: OUTPUTDATA is out of ELRANGE\n");
 		emusgx_gp(0, ptrace_regs);
 	}
 
 	if (emusgx_check_within_epc(outputdata_pa)) {
+		pr_info("vSGX: EGETKEY: OUTPUTDATA is out of EPC\n");
 		emusgx_pf(outputdata, ptrace_regs);
 	}
 
 	outputdata_epcm = emusgx_get_epcm(outputdata_pa);
 
 	if (outputdata_epcm->valid == 0) {
+		pr_info("vSGX: EGETKEY: OUTPUTDATA EPCM is not valid\n");
 		emusgx_pf(outputdata, ptrace_regs);
 	}
 
 	if (outputdata_epcm->blocked == 1) {
+		pr_info("vSGX: EGETKEY: OUTPUTDATA is blocked\n");
 		emusgx_pf(outputdata, ptrace_regs);
 	}
 
@@ -294,6 +305,7 @@ void emusgx_egetkey(struct emusgx_regs *reg_status, struct pt_regs *ptrace_regs)
 		(outputdata_epcm->pending == 1) || (outputdata_epcm->modified == 1) ||
 		((unsigned long)(outputdata_epcm->enclave_address) != (reg_status->rcx - (reg_status->rcx % 0x1000))) ||
 		(outputdata_epcm->W == 0)) {
+		pr_info("vSGX: EGETKEY: OUTPUTDATA EPCM is incorrect\n");
 		emusgx_pf(outputdata, ptrace_regs);
 	}
 
@@ -305,6 +317,7 @@ void emusgx_egetkey(struct emusgx_regs *reg_status, struct pt_regs *ptrace_regs)
 
 	// Verify RESERVED spaces in KEYREQUEST are valid
 	if (keyrequest->reserved != 0 || keyrequest->policy.reserved != 0) {
+		pr_info("vSGX: EGETKEY: KEYREQUEST's reserved fields are not zero\n");
 		emusgx_pf(keyrequest, ptrace_regs);
 	}
 
@@ -618,7 +631,7 @@ void emusgx_eaccept(struct emusgx_regs *reg_status, struct pt_regs *ptrace_regs)
 	if ((secinfo_epcm->valid == 0) || (secinfo_epcm->R == 0) ||
 		(secinfo_epcm->pending != 0) || (secinfo_epcm->modified != 0) ||
 		(secinfo_epcm->blocked !=0) || (secinfo_epcm->page_type != SGX_PT_REG) ||
-		(secinfo_epcm->enclave_secs != current->secs_pa) || (secinfo_epcm->enclave_address != scratch_secinfo)) {
+		(secinfo_epcm->enclave_secs != current->secs_pa) || ((uint64_t)secinfo_epcm->enclave_address != ((uint64_t)scratch_secinfo & PAGE_MASK))) {
 		emusgx_pf(scratch_secinfo, ptrace_regs);
 	}
 	
@@ -803,7 +816,7 @@ void emusgx_emodpe(struct emusgx_regs *reg_status, struct pt_regs *ptrace_regs)
 	// TODO
 
 	// Recheck .. ?
-	if ((epc_epcm->enclave_address == epc_page_va)) {
+	if ((epc_epcm->enclave_address != epc_page_va)) {
 		emusgx_pf(epc_page_va, ptrace_regs);
 	}
 
@@ -934,6 +947,8 @@ void emusgx_eacceptcopy(struct emusgx_regs *reg_status, struct pt_regs *ptrace_r
 	reg_status->flags.SF = 0;
 }
 
+//extern uint64_t vsgx_stamp_enter;
+
 void emusgx_eexit(struct emusgx_regs *reg_status, struct pt_regs *ptrace_regs) {
 	// RBX: Target address outside the enclave
 	// RCX: Address of the current AEP
@@ -979,6 +994,8 @@ void emusgx_eexit(struct emusgx_regs *reg_status, struct pt_regs *ptrace_regs) {
 		pr_info("EmuSGX: EEXIT failed to send data\n");
 	}
 
+	//pr_info("vSGX: ECall Enclave side took %lld\n", ktime_get_real_ns() - vsgx_stamp_enter);
+
 	kfree(package);
 }
 
diff --git a/kernel/emusgx/entrance.c b/kernel/emusgx/entrance.c
index a36ac4b0b..8a2c8f19e 100644
--- a/kernel/emusgx/entrance.c
+++ b/kernel/emusgx/entrance.c
@@ -12,6 +12,8 @@
 #include <emusgx/emusgx_debug.h>
 #include <emusgx/emusgx_fault.h>
 
+//uint64_t vsgx_stamp_enter;
+
 uint64_t _enclave_local emusgx_enter_enclave(struct sgx_tcs __user *tcs, void *aep, uint64_t pid, struct emusgx_full_regs *regs, void **pf_addr) {
 	void *tcs_pa = (void *)vsgx_vaddr_to_paddr(tcs);
 	struct emusgx_epcm *tcs_epcm;
@@ -33,6 +35,8 @@ uint64_t _enclave_local emusgx_enter_enclave(struct sgx_tcs __user *tcs, void *a
 	uint64_t tmp_target;
 	struct emusgx_handle_buffer *handle_buffer;
 
+	//vsgx_stamp_enter = ktime_get_real_ns();
+
 	*pf_addr = NULL;
 
 	emusgx_debug_print("EmuSGX: Now entering EENTER\n");
diff --git a/kernel/emusgx/fault.c b/kernel/emusgx/fault.c
index 2bac42487..e6706cc25 100644
--- a/kernel/emusgx/fault.c
+++ b/kernel/emusgx/fault.c
@@ -24,7 +24,7 @@ void emusgx_gp(int code, struct pt_regs *ptrace_regs) {
 	package.val = code;
 	package.pid = current->emusgx_pid;
 	emusgx_send_data(&package, sizeof(struct emusgx_fault_package));
-	pr_info("vSGX: GP");
+	pr_info("vSGX: GP\n");
 	exc_general_protection(ptrace_regs, 0);
 }
 
diff --git a/kernel/emusgx/irq.c b/kernel/emusgx/irq.c
index 36ab587e9..2c6db1799 100644
--- a/kernel/emusgx/irq.c
+++ b/kernel/emusgx/irq.c
@@ -106,13 +106,16 @@ irqreturn_t emusgx_irq_handler(int irq, void *dev_id) {
 	}
 	if (stamp_i == 20) {
 		for (stamp_i = 0; stamp_i < 20; stamp_i++) {
-			pr_info("EmuSGX: In = %lld\n", stamps_in[stamp_i]);
+			pr_info("EmuSGX: Time 4 End = %lld\n", stamps_in[stamp_i]);
 		}
 		for (stamp_i = 0; stamp_i < 20; stamp_i++) {
-			pr_info("EmuSGX: Retrieved = %lld\n", stamps_retrieved[stamp_i]);
+			pr_info("EmuSGX: Time 5 End = %lld\n", stamps_retrieved[stamp_i]);
 		}
 		for (stamp_i = 0; stamp_i < 20; stamp_i++) {
-			pr_info("EmuSGX: Pushed = %lld\n", stamps_pushed[stamp_i]);
+			pr_info("EmuSGX: Time 6 = %lld\n", stamps_pushed[stamp_i] - stamps_retrieved[stamp_i]);
+		}
+		for (stamp_i = 0; stamp_i < 20; stamp_i++) {
+			pr_info("vSGX: Time 7 Start = %lld\n", stamps_pushed[stamp_i]);
 		}
 		stamp_i = 100;
 	}
-- 
2.25.1

